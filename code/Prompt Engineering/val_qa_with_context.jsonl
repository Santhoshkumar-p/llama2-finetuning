{"input":"How does China formulate AI governance regulations, and what are the key layers in the policy formulation process?","output":"China formulates AI governance regulations through a four-layered policy funnel involving real-world roots, Xi Jinping and CCP ideology, the \"world of ideas,\" and party and state bureaucracies, with regulations often pinballing through these layers in a non-linear fashion.","context":"How China Sets AI Governance Policy\nThis paper presents a four-layered policy funnel through which China formulates and pro-mulgates AI governance regulations. Those four layers are real-world roots; Xi Jinping and CCP ideology; the \u201cworld of ideas\u201d; and the party and state bureaucracies. These layers are porous, and regulations do not proceed through them in a purely linear fashion.\n\nInstead, they often pinball forward and backward through these layers, getting shaped and reshaped by academics, bureaucrats, public opinion, and CCP ideology. The order and relative importance of the layers also varies depending on the nature of the issue confronted.\n\nSo far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers. And despite China\u2019s drastically different political system, policymakers in the United States and elsewhere can learn from its regulations. China\u2019s regulations create new bureaucratic and technical tools: disclosure requirements, model auditing mechanisms, and technical per-formance standards. These tools can be put to different uses in different countries, ranging from authoritarian controls on speech to democratic oversight of automated decisionmaking.\nCharting the successes, failures, and technical feasibility of China\u2019s AI regulations can give policymakers elsewhere a preview of what is possible and what might be pointless when it comes to governing AI.\nSo what do China\u2019s AI regulations contain? How did its massive party and state bureaucracies formulate them? And is it possible to predict where Chinese AI governance is headed?\nThis is the first in a series of three papers that will tackle these questions using a novel approach: reverse engineering. In this series of three papers, I will attempt to reverse engineer Chinese AI governance. \nI break down the regulations into their component parts\u2014the terminology, key concepts, and specific requirements\u2014and then trace those components to their roots, revealing how 3.The \u201cPolicy Funnel\u201d of China\u2019s AI Governance Major governance initiatives tend to proceed from left to right through this funnel, though often not in a linear fashion. In AI governance, Xi is best thought of as setting the direction of travel for policy actors and as providing the ultimate backstop for decisions. Policymaking will broadly focus on the issues he prioritizes and take an approach resonant with his way of seeing the world. And no decision will be made that directly contradicts his expressed wishes. But when it comes to crafting Chinese AI regulations, most of the activity has so far occurred in the next two layers."}
{"input":"What obligations apply to high-risk AI systems under the Europe Artificial Intelligence Act?","output":"High-risk AI systems must undergo fundamental rights assessments, and citizens can launch complaints for transparent explanations regarding decisions made by such systems.","context":"Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7]"}
{"input":"What obligations apply to high-risk AI systems entering the market according to Europe Commission?","output":"Conformity assessment with risk, quality, traceability, and user information requirements.","context":"Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. critical infrastructures (e.g. transport), that could put the life and health of citizens at risk;\neducational or vocational training, that may determine the access to education and professional course of someone\u2019s life (e.g. scoring of exams);\nsafety components of products (e.g. AI application in robot-assisted surgery);\nemployment, management of workers and access to self-employment (e.g. CV-sorting software for recruitment procedures);\nessential private and public services (e.g. credit scoring denying citizens opportunity to obtain a loan);\nlaw enforcement that may interfere with people\u2019s fundamental rights (e.g. evaluation of the reliability of evidence);\nmigration, asylum and border control management (e.g. verification of authenticity of travel documents);\nadministration of justice and democratic processes (e.g. applying the law to a concrete set of facts).\nHigh-risk AI systems will be subject to strict obligations before they can be put on the market: discussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial\nIntelligence\u201d \u2013 A European Approach to Excellence and Trust. The paper\nconsiders requiring a pre-marketing conformity assessment for \u201chigh-risk\u201d AI\napplications such as facial recognition, as a core element of a potential\nregulatory framework for AI. In addition, the white paper proposes a voluntary\n\u201cquality label\u201d for AI applications considered not to be high-risk. In parallel, the\nEuropean Commission is reviewing EU product safety and liability regimes in\nlight of AI.\nRead more:\nhttps:\/\/ec.europa.eu\/digital-single-market\/en\/artificial-intelligence.\nGlobal Partnership on AI (GPAI)\nResponsible entity: Consortium of countries\nDescription: GPAI is an international and multi-stakeholder initiative that\nadvances cutting-edge research and pilot projects on AI priorities to advance"}
{"input":"Does the UK AI regulatory framework propose new legal requirements for businesses in the AI supply chain?","output":"No, the government avoids new legal requirements but may introduce a statutory duty after an implementation period, aligning with the pro-growth objective.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. Shaping Europe\u2019s digital future - AI Act\n\nThe Commission is proposing the first-ever legal framework on AI, which addresses the risks of AI and positions Europe to play a leading role globally.\nThe regulatory proposal aims to provide AI developers, deployers and users with clear requirements and obligations regarding specific uses of AI. At the same time, the proposal seeks to reduce administrative and financial burdens for business, in particular small and medium-sized enterprises (SMEs).\n\nThe proposal is part of a wider AI package, which also includes the updated Coordinated Plan on AI. Together, the Regulatory framework and Coordinated Plan will guarantee the safety and fundamental rights of people and businesses when it comes to AI. And, they will strengthen uptake, investment and innovation in AI across the EU. No intervention on liability or accountability in the AI supply chain: The government concludes that it is too soon to make a cross-sectoral decision on liability in the AI supply chain, which currently differs across legal frameworks. For example, data controllers and data processors have specific allocation of accountability under data protection law, and similarly with producers and distributors under product safety laws. The government leaves this issue to regulators who, it states, are best positioned to begin allocating liability in their sectors, adopting a context-based approach that builds on best practice. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":"What are some of the current regulations in the UK that affect the development and use of Generative AI?","output":"Currently, the UK has laws in effect that govern the development and use of Generative AI to a certain extent. Some examples of these are the Data Protection Act of 2018, Equality Act of 2010, and the Copyright, Designs, and Patents Act of 1988. These laws affect areas such as data collection, the outcomes of AI-based decision-making, and the use of intellectual property in outputs and datasets.","context":"largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground The MOST played a large role in early policies like the 2017 AI plan and followed that up by establishing committees and issuing high-level principles for AI ethics and governance. It also wrote the draft version of a broader technology ethics and governance measure that was\nlater issued by the CCP Central Committee. But the MOST has taken a back seat on the more targeted regulations, not co-signing the recommendation algorithm or deep synthesis regulations. The ministry focuses primarily on issues related to research and development, making it less suited to regulating online content or certain commercial applications of AI.\n\nBut the MOST\u2019s profile may rise again as regulatory attention turns toward the underlying technology, as in the draft generative AI regulation, which imposes requirements on model training. These extremely demanding requirements for generative AI systems have kicked off a particularly active public debate on the draft regulation. At the time of writing, Chinese scholars, companies, and policymakers are actively discussing how to maintain effective content controls without squashing China\u2019s nascent generative AI industry. The third paper in this series will dive deep into how this policy debate is playing out in public workshops, academic writing, and corporate lobbying.\nCountries and cultures may differ on the specific content of AI regulations, but they can learn from the content-agnostic structure of the regulations themselves. The above Chinese regulations share three structural similarities: the choice of algorithms as a point of entry; the building of regulatory tools and bureaucratic know-how; and the vertical and iterative approach that is laying the groundwork for a capstone AI law. UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise."}
{"input":"What steps are being taken to safeguard Americans' privacy and civil liberties with the advancement of AI?","output":"To protect privacy and civil liberties, the administration will ensure lawful and secure data practices, employing policy and technical tools like privacy-enhancing technologies to mitigate risks associated with AI's data capabilities and uphold First Amendment rights.","context":"(A)  identify areas where AI can enhance law enforcement efficiency and accuracy, consistent with protections for privacy, civil rights, and civil liberties; and\n\n               (B)  recommend best practices for law enforcement agencies, including safeguards and appropriate use limits for AI, to address the concerns set forth in section 13(e)(i) of Executive Order 14074 as well as the best practices and the guidelines set forth in section 13(e)(iii) of Executive Order 14074; and  \n\n          (iii)  supplement the report set forth in subsection 7.1(b)(i) of this section as appropriate with recommendations to the President, including with respect to requests for necessary legislation.  \n\n     (c)  To advance the presence of relevant technical experts and expertise (such as machine-learning engineers, software and infrastructure engineering, data privacy experts, data scientists, and user experience researchers) among law enforcement professionals: (f)  Americans\u2019 privacy and civil liberties must be protected as AI continues advancing.  Artificial Intelligence is making it easier to extract, re-identify, link, infer, and act on sensitive information about people\u2019s identities, locations, habits, and desires.  Artificial Intelligence\u2019s capabilities in these areas can increase the risk that personal data could be exploited and exposed.  To combat this risk, the Federal Government will ensure that the collection, use, and retention of data is lawful, is secure, and mitigates privacy and confidentiality risks.  Agencies shall use available policy and technical tools, including privacy-enhancing technologies (PETs) where appropriate, to protect privacy and to combat the broader legal and societal risks \u2014 including the chilling of First Amendment rights \u2014 that result from the improper collection and use of people\u2019s data. (e)  The interests of Americans who increasingly use, interact with, or purchase AI and AI-enabled products in their daily lives must be protected.  Use of new technologies, such as AI, does not excuse organizations from their legal obligations, and hard-won consumer protections are more important than ever in moments of technological change.  The Federal Government will enforce existing consumer protection laws and principles and enact appropriate safeguards against fraud, unintended bias, discrimination, infringements on privacy, and other harms from AI.  Such protections are especially important in critical fields like healthcare, financial services, education, housing, law, and transportation, where mistakes by or misuse of AI could harm patients, cost consumers or small businesses, or jeopardize safety or rights.  At the same time, my Administration will promote responsible uses of AI that protect consumers, raise the quality of goods and services, lower their prices, or and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies with all Federal laws and to promote robust technical evaluations, careful oversight, engagement with affected communities, and rigorous regulation.  It is necessary to hold those developing and deploying AI accountable to standards that protect against unlawful discrimination and abuse, including in the justice system and the Federal Government.  Only then can Americans trust AI to advance civil rights, civil liberties, equity, and justice for all."}
{"input":"What principles guide the Blueprint for an AI Bill of Rights to protect the public from the potential harms of automated systems?","output":"The Blueprint for an AI Bill of Rights is guided by principles aimed at protecting civil rights, ensuring equal opportunities, and providing access to critical resources. It applies to automated systems that can significantly impact the public, with protections designed to be proportionate to the potential harms.","context":"To advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by From Principles to Practice\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, or access to critical needs. Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. with existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA).\nThe Act defines different risk levels with corresponding levels of regulation. It bans certain \nhigh-risk applications, such as live facial recognition.\nIn the US, a \u2018Blueprint for an AI Bill of Rights\u2019 has been outlined.233 Currently these are \nnon-binding guidelines that aim to address discrimination, data privacy, and transparency. In \nOctober 2023, US President Joe Biden signed an Executive Order on AI mandating standards and \ndisclosures for the largest AI companies, and measures to protect workers and disadvantaged \ngroups.234\nPotential future regulations\nHuman intervention in automated decision- making\nSome civil society groups235 and academics236,237 have suggested a law to enshrine a right to human \nintervention in automated decision-making. There are two ways to implement this: the operating principles of an AI system are. Third, the principle of consistency of [sic] rights and responsibilities emphasised that, on the one hand, data should be properly recorded and oversight present but, on the other hand, that commercial entities should be able to protect their intellectual property (Ding and Triolo 2018)."}
{"input":"How is the DHS Office for Civil Rights and Civil Liberties (CRCL) involved in ensuring the responsible use of AI?","output":"The DHS CRCL Office supports the AITF by leading its Responsible Use Group (RUG), which governs AITF projects to ensure AI operates accurately, unbiasedly, and fairly, advancing equity for those served by the Department.","context":"Additionally, the DHS CRCL Office helps to ensure DHS activities that rely on AI operate in an accurate and unbiased manner, are fundamentally fair, and advance equity for those that the Department serves.\u202fCRCL supports the DHS AITF by leading its Responsible Use Group (RUG), which brings together a broad range of internal stakeholders to govern AITF projects.\u202fThe RUG provides guidance, risk assessment, mitigation strategies, and oversight for the protection of individual rights in projects championed by the AITF.\n\nDHS\u2019s work on artificial intelligence is part of a whole-of-government effort to address this emerging technology. Earlier this week, the Biden-Harris Administration announced additional commitments from companies to help advance the development of safe, secure, and trustworthy AI. The Administration will continue to pursue bipartisan legislation to help America lead the way in responsible AI development. offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these objectives, agencies shall also consider opportunities to increase coordination, communication, and engagement about AI as appropriate with community-based organizations; civil-rights and civil-liberties organizations; academic institutions; industry; State, local, Tribal, and territorial governments; and other stakeholders. 7.2.  Protecting Civil Rights Related to Government Benefits and Programs.  (a)  To advance equity and civil rights, consistent with the directives of Executive Order 14091, and in addition to complying with the guidance on Federal Government use of AI issued pursuant to section 10.1(b) of this order, agencies shall use their respective civil rights and civil liberties offices and authorities \u2014 as appropriate and consistent with applicable law \u2014 to prevent and address unlawful discrimination and other harms that result from uses of AI in Federal Government programs and benefits administration.  This directive does not apply to agencies\u2019 civil or criminal enforcement authorities.  Agencies shall consider opportunities to ensure that their respective civil rights and civil liberties offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these (ii)   direct the Assistant Attorney General in charge of the Civil Rights Division to convene, within 90 days of the date of this order, a meeting of the heads of Federal civil rights offices \u2014 for which meeting the heads of civil rights offices within independent regulatory agencies will be encouraged to join \u2014 to discuss comprehensive use of their respective authorities and offices to:  prevent and address discrimination in the use of automated systems, including algorithmic discrimination; increase coordination between the Department of Justice\u2019s Civil Rights Division and Federal civil rights offices concerning issues related to AI and algorithmic discrimination; improve external stakeholder engagement to promote public awareness of potential discriminatory uses and effects of AI; and develop, as appropriate, additional training, technical assistance, guidance, or other resources; and"}
{"input":"What are the international efforts and agreements on AI policy?","output":"The article does not provide specific details on international efforts and agreements regarding AI policy but mentions that several other countries are more advanced in developing policies that protect people from algorithmic harm. Readers are directed to EPIC\u0092s International Policy page for more information.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. 60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. AI policy design\nCountries are at different stages of the development and implementation of\nnational AI strategies and policies. Some countries, such as Canada and Finland,\ndeveloped their national AI strategies as early as 2017, closely followed by\nJapan, France, Germany and the United Kingdom in 2018. Other countries, such\nas Brazil, Egypt, Hungary, Poland and Spain, launched a national AI strategy\nmore recently. Several countries are currently in AI policy consultation and\ndevelopment processes.\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f|7\nNational AI strategies and policies are often initiated with a call to action in the\nform of a report, roadmap, or white paper that frames the high-level goals for\na strategy. The policy design and development stages often follow."}
{"input":"What companies exist that could assist with ensuring that AI is developed, deployed, and used ethically?","output":"There are many companies that exist that can help to ensure that AI is developed and deployed ethically. In the field of AI Assurance, some companies that specialize in monitoring, evaluation, and legal compliance include: Weights & Biases, Babl AI, Eticas, Research and Consulting, and Credo AI.","context":"Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors. Measures to Address Challenges\n\u2022 Issue and make fully transparent\nthe legal framework, AI policy,\nethical principles and operating\nframework.\n\u2022 Establish special AI\/Innovation Hub\nor government unit as a center of\nexcellence.\n\u2022 Develop whole-of-government,\ndata fabric AI architecture.\n\u2022 Ensure human oversight to\nmanage risks and safeguard\nagainst machine-invoked bias. \u2022\n\nOrganisations and individuals developing, deploying or operating AI\nsystems should be held accountable for their proper functioning in line with\nthe above principles\n\nConsistent with these principles, the OECD also provides five recommendations to\ngovernments:\n\u2022\n\nFacilitate public and private investment in research & development to spur\ninnovation in trustworthy AI.\n\n\u2022\n\nFoster accessible AI ecosystems with digital infrastructure and technologies\nand mechanisms to share data and knowledge.\n\n\u2022\n\nEnsure a policy environment that will open the way to the deployment of\ntrustworthy AI systems.\n\n\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI. Unlike other potentially dangerous technologies, AI lacks obvious inputs that could be regulated and controlled. Data and computing power are widely available to companies large and small, and no single entity can reliably predict from where the next revolutionary AI advance might originate. Consequently, governments should consider expanding their toolboxes beyond traditional regulatory techniques. Two creative mechanisms could be for governments to invest in establishing robust, publicly owned data sets for AI research or issue challenge grants that encourage socially beneficial uses for AI. New techniques could also include creating uniform liability rules to clarify when developers will be liable for harms involving AI, requirements for how AI should be assessed, and controls on whether certain highly capable models can be proliferated. Ultimately, governments could buy a seat at the table by providing economic incentives to companies in exchange for more influence in ensuring"}
{"input":"Why is data regulation central to AI regulation? ","output":"AI crucially depends on large volumes of high-quality data. Model accuracy and outcomes directly reflect the data it was trained on. Hence, data regulation is central to AI regulation.","context":"Data flows, data privacy, data security. AI crucially depends on large volumes of high-quality data. Model accuracy and outcomes directly reflect the data it was trained on. Hence, data regulation is central to AI regulation. AI likely magnifies data privacy concerns. The United States has no national data privacy law; the EU builds on GDPR. Data security and cross-border data flows are also connected to AI regulation. These are not new issues, and AI regulation should build on prior efforts. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA). Governments around the world are implementing foundational policies to regulateartificial intelligence (AI) and algorithmic systems more generally. While legislation isadvancing, regulators should not wait idly for legislators to act. Instead, regulatorsshould be actively learning about the algorithmic systems in their regulatory domainand evaluating those systems for compliance under existing statutory authority.\nMany regulatory agencies have started this work, including the U.S. Federal TradeCommission\u2019s (FTC)\nand Consumer Financial Protection Bureau(CFPB), new algorithmic regulators in\nand\n, and onlineplatform regulators such as the UK\u2019s\n(OFCOM) and the\n. These agencies and others havestarted to implement novel approaches and policies for AI regulation.\nOffice of Technology\nthe Netherlands\nSpain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023 largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground"}
{"input":"Which Chinese organizations had a major breakthrough in the field of AI? ","output":"Huawei, Baidu, and the Beijing Academy of Artificial Intelligence (BAAI) had a major breakthrough in the field of AI. ","context":"Akin to a Sputnik moment, three Chinese organizations\u2014Huawei, Baidu, and the Beijing Academy of Artificial Intelligence (BAAI)\u2014announce a major AI breakthrough, taking the world by surprise. In this world, AI progress in China is initially overlooked and consistently downplayed by policymakers from advanced, democratic economies. Chinese companies, research institutes, and key government labs leapfrog ahead of foreign competitors, in part because of their improved ability to absorb vast amounts of government funding. State-of-the-art AI models also have been steadily advancing, leveraging a competitive data advantage from the country's massive population. Finally, China's military-civil fusion enables major actors across industry, academia, and government to share resources across a common AI infrastructure. BAAI benefits from strategic partnerships with industry leaders and access to significant computational resources, such as the Pengcheng Laboratory's Pengcheng Cloudbrain-II. This mentioned AI as one of the six critical areas for developing the country\u2019s emerging industries (CCP 2016), and as an important factor in stimulating economic growth. When read together, these documents indicate that there has been a conscious effort to develop and use AI in China for some time, even before \u2018the Sputnik moment\u2019. However, prior to 2016, AI was presented merely as one technology among many others, which could be useful in achieving a range of policy goals. This changed with the release of the AIDP. 3 China\u2019s AI strategic focus The AIDP provides a longitudinal perspective on China\u2019s strategic situation regarding AI, including its comparative capabilities, the opportunities offered, and the potential risks. Following a technology-first approach, it may be tempting to concentrate one\u2019s attention on the stated capabilities of AI, to gain an insight into the types of technologies in which China is investing. However, this would likely offer only a short-term perspective and would soon be out of date as technological innovation advances rapidly. Furthermore, it would do little to explain why China is seeking to develop a strong AI sector in the decades to come. To this end, it is more useful to try to understand China\u2019s strategic focus from a policy-first approach, by analysing the areas where China considers that AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the"}
{"input":"When was the EOAI released? ","output":"The EOAI was released on 30th October. ","context":"globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d The EOAI also calls for the accelerated development of AI standards with international partners in standards organizations. For government use of AI, the EO requires the development of guidance for agency use of AI and a faster and more efficient process for agencies to procure AI products and services. The directive also calls for the rapid hiring of AI professionals and the training of employees at all levels. The content of a forthcoming Office of Management and Budget memo will dictate the details of how government AI use will change.\n\nEPIC has long advocated for comprehensive privacy protections, rigorous testing protocols, expanded resources for evaluation of AI systems, and a government-whole effort to fighting algorithmic discrimination.\n\nTwo days later, on November 1st, 2023, the Office of Management and Budget released a memo dictating how Government Agencies should approach using and regulating AI. There is a 30-day comment period before it\u2019s finalized. Press Release, January 20. http:\/\/www.mcit.gov.eg\/Media_Center\/Press_Room\/Press_Releases\/28084\nArabian Business. 2018. \u201cUAE Adopts New National AI Strategy Aimed at Becoming Global Leader.\u201d Arabian Business, April 22.\nhttps:\/\/www.arabianbusiness.com\/technology\/418254-uae-adopts-new-national-ai-strategy-aimed-at-becoming-global-leader\nArshadi, A. K. and others. 2020. \u201cArtificial Intelligence for Covid-19 Drug Discovery and Vaccine Development.\u201d\nFrontiers in Artificial Intelligence, 18.\nhttps:\/\/doi.org\/10.3389\/frai.2020.00065\nAstana Times. 2019. \u201cMicrosoft Kazakhstan to Assist Domestic Companies Integrate AI Technology.\u201d The Astana Times, March 18.\nhttps:\/\/astanatimes.com\/2019\/03\/microsoft-kazakhstan-to-assist-domestic-companies-integrate-ai-technology\/\nAU-EU Digital Economy Task Force (AU-EU DETF). 2019. New Africa-Europe Digital Economy Partnership: Accelerating the Achievement\nof the Sustainable Development Goals. Yamamoto, C. 2017, \u201cAssocia\u00e7\u00e3o Brasileira de Intelig\u00eancia Artificial \u2013 ABRIA.\u201d Or\u00e1culo TI, 31 May.\nhttps:\/\/oraculoti.com.br\/2017\/05\/31\/associacao-brasileira-de-inteligencia-artificial-abria\/\nYampolskiy, R. 2018. Current State of Knowledge on Failures of AI-Enabled Products. Consortium for Safer AI."}
{"input":"What is the purpose of the G7 documents? ","output":"The purpose of the G7 documents is to predict potential cooperation on AI governance among the G7 countries.","context":"The G7 statements emphasize the need to support the development of tools for trustworthy AI several times. Tools here include a wide range of regulatory and nonregulatory frameworks, technical standards and assurance techniques, risk assessment and management frameworks, auditing, and potential certification schemes. The Path to Trustworthy AI G7 Outcomes and Implications for Global AI Governance\n\nCommentary by Hiroki Habuka\n\nPublished June 6, 2023\n\nIntroduction\nThe G7 Summit, held from May 19 to 21, 2023, left a strong impression of unity among global leaders. Emerging technology such as artificial intelligence (AI), along with national security and energy, was highlighted as a key area requiring a strong alliance among G7 countries toward fundamental values such as democracy and human rights. The G7 Digital and Tech Ministers\u2019 Meeting, which took place a little earlier, also discussed responsible AI and global AI governance as one of the central topics. Conclusion\nThe G7 statements have highlighted the shared commitment to developing and implementing trustworthy AI that upholds values such as human rights and democracy. The statements also acknowledge risks including online abuse, threats to privacy, misuse of AI, and IP concerns. The G7 leaders have agreed to support the establishment of an interoperable governance framework to counteract potential gaps and fragmentation in global technology governance. They have also committed to assisting the efforts by organizations such as OECD and GPAI, and to collaborate in the development of international technical standards under SDOs. Risk-Based and Forward-Looking Approaches under Different Policy Frameworks\n\nFirst, it is important to note that the G7 statements do not attempt to harmonize the approaches and policy instruments of the G7 members, declaring that \u201cthe common vision and goal of trustworthy AI may vary across G7 members.\u201d Furthermore, the declaration emphasizes that policies and regulations should take into account not only technical and institutional characteristics, but also social and cultural impacts, including geographic, sectoral, and ethical aspects. In fact, there are policy differences among G7 members, from the European Union (France, Germany, and Italy) and Canada, which promote comprehensive and binding regulations for AI, to Japan, the United Kingdom, and the United States, which promote sector-specific guidance-based policies."}
{"input":"Why are increases in US export control enforcement necessary?","output":"Increases in US export control enforcement are necessary due to substantial evidence that sanctioned entities are devoting large amounts of resources to circumventing established export controls in order to gain access to advanced technology to meet strategic aims, including aims that influence warfare in contested parts of the world.","context":"Improved Export Controls Enforcement Technology Needed for U.S. National Security\nExecutive Summary\nAs technology has become increasingly central to strategic competition with Russia and China, export controls have moved to the forefront of U.S. foreign policy on technology issues. Most notably, restricting Russia\u2019s access to advanced technology through export controls is a key part of the U.S. response to Russia\u2019s invasion of Ukraine, as U.S. government officials have repeatedly stated.\n\nUnfortunately, nearly all the debate is focused on whether and when to apply export controls, not how to ensure that export controls are effectively administered and enforced once applied. The October 7, 2022, package of export controls against China is too new for comparable data to be available at the time of this writing, but the same caution applies: published government data does not necessarily tell the whole story, which must focus on both licit and illicit transactions. \n\nFor BIS, the scale and complexity of U.S. export control requirements have increased massively over the past three years due to new legislative and executive requirements.\n\nFor those exports that do require a license, the typical process for export controls administration and enforcement is depicted in Figure 1.\n\n\n\nIncreasingly sophisticated and well-resourced attempts by Russian and Chinese organizations to evade export controls are not the only reason that BIS\u2019s job has become significantly harder in recent years. Legislation and executive orders have posed new challenges to BIS in administering and enforcing export controls in at least three ways. The Huawei and ZTE export controls were a potent reminder to the U.S. foreign policy community of the power of export controls. U.S. technology export controls were able to do significant damage to the financial prospects of leading Chinese technology giants. The United States did this entirely with non-violent means and at a limited cost to the U.S. economy.\n\nHowever, major weaknesses in U.S. export control enforcement capacity are evident, as demonstrated by Russia\u2019s success in evading controls for weapons used in Ukraine. The foundation of the export controls administration and enforcement process is the lists managed by BIS, both the parties of concern lists (e.g., the Entity List) and the Commerce Control List of items. It is the responsibility of BIS to ensure that the lists of parties of concern accurately capture the parties that U.S. law and policy state should be restricted from transactions with U.S. industry. As an industry interviewee said, \u201cif the U.S. government does not want us to do business with an entity, they have to tell us that. They can\u2019t just ask us to know that.\u201d The same is true for sales of certain types of items. It is the responsibility of the U.S. government to tell industry what is and is not allowed."}
{"input":"Is the AI Policy retroactive, and how does it apply to historical information?","output":"\nYes, the AI Policy applies retroactively to all information held by the Bank, regardless of when it was created, ensuring broad access to historical documents and data.","context":"Go Top\n\nIs the AI Policy retroactive?\n\nYes. The Policy applies to all information in the Bank's possession, regardless of its age.\n\n                                                                                                                    Go Top\n\nWas the AI Policy discussed with member countries?\n\nPublic consultations on the AI Policy proposal were held in 2009 with various stakeholder groups in 33 countries as\nwell as on the Bank's website. The final AI Policy reflects the input of member countries, civil society organizations,\nacademics, parliamentarians, media, the private sector, other international organizations and donor agencies\n\n                                                                                                                    Go Top\n\fWhat can be disclosed under the AI Policy? Policy implications of artificial intelligence (AI)\n27                                  POSTnote 708 - 9 January 2024\nContributors\nPOST is grateful to Ansh Bhatnagar for researching this briefing, to STFC for funding their \nparliamentary fellowship, and to all contributors and reviewers. For further information on this \nsubject, please contact the co-author, Devyani Gajjar.\nMembers of the POST Board*\nDr Elena Abrusci, Brunel University London* Dr Mhairi Aitken, Alan Turing Institute* Emmanuelle \nAndrews, Liberty\nDr Hayleigh Bosher, Brunel University London* Matt Davies, Ada Lovelace Institute\nMaximilian Gahntz, Mozilla Foundation Conor Griffin, Google DeepMind\nProfessor Oliver Hauser, University of Exeter Harry Law, Google*\nMia Leslie, Public Law Project*\nMavis Machirori, Ada Lovelace Institute Professor Gina Neff, University of Cambridge\nSam Nutt, London Office of Technology and Innovation Lucy Purdon, Mozilla Foundation*\nAdam Smith, British Computer Society* AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. AI FAQs\n\nAI POLICY HIGHLIGHTS\n\n    \uf0b7    What is the main difference between the World Bank's Access to Information Policy (AI Policy) and the\n         previous Disclosure policy?\n    \uf0b7    Is the AI Policy retroactive?\n    \uf0b7    Was the AI Policy discussed with member countries?\n    \uf0b7    What can be disclosed under the AI Policy?\n    \uf0b7    What are the exceptions to disclosing information?\n    \uf0b7    Why does the AI Policy restrict some types of information from disclosure?\n\nPROACTIVE DISCLOSURE\n\n    \uf0b7    Where do I find World Bank information?\n    \uf0b7    When do project documents become available?\n    \uf0b7    When do Board papers and Board records become available?\n\nREQUESTING INFORMATION TO THE WORLD BANK\n\n    \uf0b7    Can I contact a Bank staff directly for Bank\u2019s information?\n    \uf0b7    What does it cost to get information from the Bank?\n    \uf0b7    What if the Bank restricts something it is supposed to release when requested?\n    \uf0b7    What is the process to file an appeal?\n\n\n\nAI POLICY HIGHLIGHTS"}
{"input":"What institutional functions are deemed necessary for effective AI governance?","output":"Recommended institutional functions include regular assessments of AI's future directions, developing and harmonizing standards, facilitating AI's development for economic and societal benefit, promoting international collaboration on talent and infrastructure, monitoring risks, and ensuring compliance and accountability through binding norms.","context":"Effective implementation of national AI initiatives hinges on coordination\nCountries pursue different national governance models to co-ordinate the\nimplementation of their national AI policies across government, offering\nregulatory and ethical oversight (Figure 1). Models include:\n\u2022\n\nAssigning oversight of the development and implementation strategies\nto an existing ministry, department or body. Among existing ministries\nor agencies tasked with developing or implementing an AI strategy, the\nfollowing tend to drive the creation of AI strategies most often: 1)\ninformation technology and communications ministries; 2) economics or\nfinance ministries; or 3) education, science (and technology) and\ninnovation ministries.\n\n\u2022\n\nCreating a new governmental or independent AI co-ordination entity.\n\n\u2022 AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI Accountability and governance: Businesses should put in place governance measures that ensure effective oversight of the supply and use of AI systems, with clear lines of accountability established across the AI lifecycle. Regulators will likely implement this principle through regulatory guidance and assurance techniques.\nContestability and redress: Users, impacted third parties, and actors in the AI lifecycle should be able to contest an AI decision or outcome that is harmful or creates material risk of harm. Regulators will be expected to clarify existing routes to contestability and redress and implement proportionate measures to ensure that the outcomes of AI use are contestable where appropriate. The government\u2019s initial non-statutory approach will not create new rights or new routes to redress at this stage."}
{"input":"What are the main exceptions to information disclosure under the AI Policy?","output":"The policy outlines exceptions for certain types of information, including personal data, communications from executive directors, privileged legal information, and financial data, aimed at protecting sensitive information and the deliberative process.","context":"Go Top\n\nWhy does the AI Policy restrict some types of information from disclosure?\n\nThe AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the\npublic and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff and other\nparties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure\ncould cause harm to well defined interests.\n\n                                                                                                                    Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information? Go Top\n\fWhat can be disclosed under the AI Policy?\n\nThe Bank can disclose any information in its possession, provided that the information does not fall under one or more\nof the exceptions set out in the AI Policy or the Bank has decided, on an exceptional basis, to restrict access to the\ninformation.\n\n                                                                                                                    Go Top\n\nWhat are the exceptions to disclosing information?\n\nThe Policy explicitly sets out 10 categories of information that are considered to be exceptions and not eligible for\ndeclassification: AI POLICY HIGHLIGHTS\n\nWhat is the main difference between the Bank's Access to Information Policy (AI Policy) and the previous\nDisclosure policy?\n\nThe AI Policy, launched in 2010, moved the Bank from an approach that spelled out what information is disclosed to\none that allows the Bank to disclose any information in its possession that is not on a list of exceptions set out in the AI\nPolicy.\n\n    \uf0b7    Under the AI Policy, significantly more information on Bank operations and Board proceeding is available.\n    \uf0b7    The new Policy permits public release of some documents prior to discussion by the World Bank's Board of\n         Executive Directors.\n    \uf0b7    Certain restricted information is eligible for declassification after 5, 10, or 20 years.\n    \uf0b7    It also establishes an appeal mechanism that provides public recourse when the Bank denies access to\n         information. an individual right to \u201cmeaningful information aboutthe logic\u201d of algorithmic systems. This has led companies, such as\n, to offer responses\u2014albeit limited ones\u2014to requests for information aboutalgorithmic decisions. Although not yet passed into law, the forthcoming EU AI Act willalso create\nsubstantial new transparency requirements\n(https:\/\/www.brookings.edu\/articles\/the-eu-and-us-diverge-on-ai-regulation-atransatlantic-\ncomparison-and-steps-to-alignment\/#anchor5)\n, likely including directdisclosure of chatbots and public reporting about high-risk AI systems.\nTransparency requirements require little expertise and capacity from governmentagencies, making them an appealing early step in AI regulation. However, regulators doneed to be careful in specifying transparency requirements\u2014vaguely or poorly wordedrequirements can permit too much flexibility in algorithmic transparency, allowing forcompanies to cherry pick self-serving statistics."}
{"input":"What ethical principles should guide the use of AI in the public sector?","output":"Ethical use involves ensuring AI does not harm humans but enhances human well-being, with policies in place for governance, oversight, and managing bias in AI solutions.","context":"6\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nMost advanced digital governments have issued governance frameworks,\nincluding ethical principles for the use of AI. Governance models incorporate three aspects: ethical principles, the role of a central agency, and\noperational framework.\nNational governments, including Australia, Canada, China, Japan, Singapore,\nUnited Arab Emirates, and the United States as well as international organizations including the European Commission, the Institute of Electrical and Electronics Engineers, International Organization for Standardization, United Nations, and World Economic Forum, are actively proposing governance models\nfor AI that emphasize common principles:\n\nEthical Principles\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\nPrivacy and data protection\nAccountability\nSafety and security\nTransparency and explainability\nFairness\nHuman control of technology\nProfessional responsibility\nPromotion of human values\n\n\u2022 and implemented. As an initial step there needs to be a greater investment in digital skills, so that the government can formulate\nthe appropriate policies and risk management strategies which are context specific and not just adopted from other country\ncontexts. The shift in the public sector employment needs to more high-skilled workers will take place gradually over the long\nterm, but it is a key consideration today because building digital skills in the public sector and overcoming skills shortages more\ngenerally, takes a long time.\nHaving in place a policy for the ethical use of AI is fundamental to managing the adverse consequences of AI use in public policy.\nThe ethical use of AI means that these systems should not harm humans. Rather, they are used to enhance overall human\nwellbeing. Since the software decision-making may result in bias, it is critical that there are policies in place for dealing with the\ngovernance and oversight of the use of AI solutions. 8\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nThe typology of AI use cases in the public sector includes:\n\u2022\n\nCITIZEN ENGAGEMENT. The introduction of AI tools such as chatbots that\nanswer citizen queries. For example, how can I apply for social welfare\nbenefits? Additionally, aggregation and pattern determination can be used\nto collect feedback from millions of citizens, for example on a draft policy or\nlegislation published online for public consultations.\n\n\u2022\n\nCITIZEN ENGAGEMENT: AI was able to analyze citizen sentiments from 21\nmillion comments received on the US Government\u2019s policy on NetNeutrality shared online;\n\n\u2022\n\nCOMPLIANCE AND RISK MANAGEMENT: In Armenia, AI helped the revenue agency increase its revenue by 6 times. The World Bank supported\nthe underlying technologies for this project. For more details, please watch\nthe World Bank hosted event.\n\n\u2022 of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions that emerge from using AI in the industry are currently lacking. Thus, AI can help foster increased productivity and high levels of growth, but its use is likely to intensify the inequalities present within society and even decrease support for the government and its policies. The AIDP also promotes AI as a way to help deal with some of the major social problems, ranging from pollution to standards of living. However, positive impact in this area seem to come with increased control over individuals\u2019 behaviour, with governance extending into the realm of moral behaviour and further erosion of privacy."}
{"input":"Is there any recourse if the World Bank denies a request for information?","output":"If the World Bank denies a request for information, the denial can be appealed to the World Bank's Access to Information Committee and may or may not refer the particular request to the relevant bank Managing Director for a recommendation regarding the decision.","context":"Bank to locate the information within a reasonable period of time. If a particular\ndocument is required, it should be identified precisely, preferably by date and title.\nElectronic submissions should be directed through the World Bank website:\nwww.worldbank.org\/wbaccess. Requests may also be made in person at the InfoShop in\nWashington, DC, or the Public Information Centers located in member countries.\n25. Timelines for Responding to Requests. The Bank acknowledges receipt of\nwritten requests for information within 5 working days, and endeavors to provide a more\ncomprehensive response within 20 working days. Additional time may be needed in\nspecial circumstances, including, for example, those involving complex or voluminous\nrequests, or requests requiring review by or consultations with internal Bank units,\nexternal parties, the Access to Information Committee, or the Board.\n26. Unreasonable or Unsupported Requests. The Bank reserves the right to refuse (d) Board papers distributed to the Executive Directors for information are posted\nupon distribution.\n24.\nInformation on Request. Information that is disclosable under this policy and is\nnot on the Bank\u2019s external website is available on request (subject to paragraph 26). Such\nrequests may be submitted in writing by electronic means, mail, or fax. Requests should\nindicate, with reasonable specificity, the information that is being sought, to enable the\nBank to locate the information within a reasonable period of time. If a particular\ndocument is required, it should be identified precisely, preferably by date and title.\nElectronic submissions should be directed through the World Bank website:\nwww.worldbank.org\/wbaccess. Requests may also be made in person at the InfoShop in\nWashington, DC, or the Public Information Centers located in member countries.\n25.\nTimelines for Responding to Requests. The Bank acknowledges receipt of The Bank also makes available to the public the final documents that it prepares jointly with partners after all parties\nhave agreed to disclose them. Board papers and Board records are available in this link after disclosure.\n\n                                                                                                                 Go Top\n\nREQUESTING INFORMATION TO THE WORLD BANK\n\n\n\nCan I contact a Bank staff directly for Bank's information?\n\nNo. Publicly available information can be accessed through the World Bank's website at\nwww.worldbank.org\/documents, www.worldbank.org\/projects and www.worldbank.org\/topics. For information that is\nnot readily available in the World Bank's website, you can submit your request through the online portal at\nwww.worldbank.org\/wbaccess.\n\n                                                                                                                 Go Top\n\nWhat does it cost to get information from the Bank? mail to Access to Information Secretariat, World Bank, 1818 H Street, NW, Washington,\nDC, 20433. Appeals should be addressed to either the AI Committee (first stage of\nappeal) or the Appeals Board (second stage of appeal for those appeals alleging that the\nBank violated the Policy by improperly or unreasonably restricting access). All appeals\nmust contain the following:\n(a) the original case number, provided in the Bank\u2019s response to the request for\ninformation;\n(b) a description of the information originally requested; and\n(c) an explanatory statement that sets out the facts and the grounds which support the\nrequester\u2019s claim that (i) the Bank violated this Policy by improperly or\ninappropriately restricting access to the information requested; or (ii) for public\ninterest reasons, an override of the Policy\u2019s exceptions is justified.\n40. Notification of Decision. Requesters are notified if the appeal is dismissed:"}
{"input":"Who were the featured speakers at the 2023 NAII International Summit for AI in Healthcare?","output":"The Honorable Denis McDonough, Secretary of Veterans Affairs, announced the release of VA\u0092s new Trustworthy AI Framework, which integrates the White House\u0092s AI Bill of Rights, as well as other AI guiding principles. Under VA\u0092s Trustworthy AI Framework, AI must be purposeful, effective, and safe, secure and private, fair and equitable, transparent and explainable, and accountable and monitored.\nVA Under Secretary for Health, Dr. Shereef Elnahal, announced the next AI Tech Sprint, which will address the administrative workload that places a heavy burden on clinicians at VA and across the U.S. healthcare system. AI Tech Sprint participants will be challenged to propose AI solutions to mitigate the time-consuming reporting and routine tasks clinicians are responsible for, so they can spend more time directly with patients.\nAssistant Under Secretary for Health for the Office of Discovery, Education and Affiliate Networks, Dr. Carolyn Clancy, shared the VA NAII\u0092s data use agreement with LinkedIn, which holds great potential to advance VA\u0092s data scientist recruiting.\nVA Chief AI Officer and NAII Director, Dr. Gil Alterovitz, presented VA\u0092s new AI website. The new website is designed to serve as a hub for information and resources related to AI initiatives within VA, showcasing their commitment to leveraging innovative solutions for the benefit of those who have served the nation.\nLeaders from the Netherlands Ministry of Health and Defense, Colonel Henk Van der Wal and Monica Schagen, discussed the Netherlands\u0092 leading AI work in a plenary session on the Current State, Collaboration and Future Advancement of AI in Health Care.\nCEO and Founder of insitro, Dr. Daphne Koller, shared her company\u0092s groundbreaking work on multimodal AI, which can process and generate outputs from more than one type of data to provide improved capabilities and understanding of specific health problems.\nThe summit also featured remarks from, VA Chief Technology Officer, Charles Worthington, Founder and CEO of FOUR, Antonija Burcul, Pulitzer Prize-winner and Professor of Medicine at Columbia University, Dr. Siddhartha Mukherjee, and Chief AI Officer at the Department of Health and Human Services, Greg Singleton","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f22 | AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 17 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f16 |"}
{"input":"What sort of problems could result from underdeveloped investment in artificial intelligence?","output":"The primary issue with not investing heavily enough in the development and responsible control of Artificial Intelligence is the risk that a competitor or bad actor develops a better, faster, or more damaging version of AI that could pose a threat to national security.","context":"3                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nBenefits and risks\nJob creation and displacement\nIn 2022, a report commissioned for the Department for Science, Innovation & Technology stated that \ninvestment in the UK AI sector grew five-fold between 2019 and 2021, and that it brought in \u00a310.6 \nbillion in revenue and employed over 50,000 people.28\nOECD analysis published in July 2023 found that AI was changing the nature of work by assisting \nworkers and reducing the time spent on mundane tasks, rather than causing job losses.29\nHowever, some emerging academic research indicates that developments in generative AI may be linked \nto a loss in the quantity and earnings of white-collar jobs.30,31\nThere is potential for new jobs across all sectors to be created with improved productivity and \nglobal economic growth (PB 57).32 However, some reports state that certain jobs, such as clerical https:\/\/www.ifc.org\/wps\/wcm\/connect\/publications_ext_content\/ifc_external_publication_site\/publications_listing_page\/artificial+intelligence+in+emerging+markets\nILO (International Labour Organization). 2018. The Economics of artificial intelligence: Implications for the future of work. Geneva: ILO.\nhttps:\/\/www.ilo.org\/wcmsp5\/groups\/public\/---dgreports\/---cabinet\/documents\/publication\/wcms_647306.pdf\nINSEAD.2020. The Global Talent in the Age of Artificial Intelligence: The Global Talent Competitiveness Index 2020. Paris: INSEAD.\nhttps:\/\/gtcistudy.com\/wp-content\/uploads\/2020\/01\/GTCI-2020-Report.pdf\nITU (International Telecommunication Union). 2018. Accessing the Economic Impact of Artificial Intelligence.\nhttps:\/\/www.itu.int\/dms_pub\/itu-s\/opb\/gen\/S-GEN-ISSUEPAPER-2018-1-PDF-E.pdf\nJackson, T. 2017. \u201cSA\u2019s hearX Group Crowdfunding Campaign Oversubscribed.\u201d Disrupt Africa, August 9.\nhttp:\/\/disrupt-africa.com\/2017\/08\/sas-hearx-group-crowdfunding-campaign-oversubscribed\/ HM Government. 2018. Industrial Strategy: Artificial Intelligence Sector Deal.\nhttps:\/\/assets.publishing.service.gov.uk\/government\/uploads\/system\/uploads\/attachment_data\/file\/702810\/180425_BEIS_AI_Sector_\nDeal__4_.pdf\nHolon IQ. 2020. The Global AI Strategy Landscape.\nhttps:\/\/www.holoniq.com\/notes\/the-global-ai-strategy-landscape\/\nIDC (International Data Corporation). 2019. \u201cSpending on Artificial Intelligence Systems in the Middle East & Africa to Top $374 Million in\n2020.\u201d IDC Media Center, September 30.\nhttps:\/\/www.idc.com\/getdoc.jsp?containerId=prMETA45546719\nIFC (International Finance Corporation). 2020. Artificial Intelligence in Emerging Markets: Opportunities, Trends, and Emerging Business\nModels. Washington, D.C.: International Finance Corporation.\nhttps:\/\/www.ifc.org\/wps\/wcm\/connect\/publications_ext_content\/ifc_external_publication_site\/publications_listing_page\/artificial+intelligence+in+emerging+markets Unlike other potentially dangerous technologies, AI lacks obvious inputs that could be regulated and controlled. Data and computing power are widely available to companies large and small, and no single entity can reliably predict from where the next revolutionary AI advance might originate. Consequently, governments should consider expanding their toolboxes beyond traditional regulatory techniques. Two creative mechanisms could be for governments to invest in establishing robust, publicly owned data sets for AI research or issue challenge grants that encourage socially beneficial uses for AI. New techniques could also include creating uniform liability rules to clarify when developers will be liable for harms involving AI, requirements for how AI should be assessed, and controls on whether certain highly capable models can be proliferated. Ultimately, governments could buy a seat at the table by providing economic incentives to companies in exchange for more influence in ensuring"}
{"input":"What are the 15 Actions that Singapore will undertake across these systems and enablers, to support our ambitions over the next 3-5 years?","output":"(1) Anchor new AI Centres of Excellence (CoEs) across companies and explore establishing Sectoral AI CoEs to drive sophisticated AI value creation and usage in key sectors.\n(2) Strengthen our AI start-up ecosystem, including attracting AI-focused accelerator programmes to spur rapid AI experimentation.\n(3) Improve Public Service productivity, with new value propositions for our citizens.\n(4) Update national AI R&D plans to sustain leadership in select research areas.\n(5) Attract world\u2019s top AI Creators to work from and with Singapore. \n(6) Boost AI Practitioner pool to 15,000. \n(7) Intensify enterprise AI adoption for industry transformation. \n(8) Upskill workforce through sector-specific AI training programmes.\n(9) Establish an iconic AI site to co-locate AI creators and practitioners, and nurture the AI community in Singapore.\n(10) Significantly increase high-performance compute available in Singapore.\n(11) Build capabilities in data services and PrivacyEnhancing Technologies.\n(12) Unlock Government data for use cases that serve the Public Good.\n(13) Ensure fit-for-purpose regulatory environment for AI.\n(14) Raise security and resilience baseline for AI.\n(15)  Establish Singapore as an ambitious and pragmatic international partner on AI innovation and governance.\n","context":"The Plan will be guided by a new AI Strategy Advisory Committee, established in November 2017, and will be coordinated by the Ministry of Science and Technology (MIST), alongside the AI Plan Promotion Office, and other relevant bodies (\u201cAI Policy-China\u201d n.d.).Footnote3 Although these bodies will provide central guidance, the Plan is not meant to act as a centrally enacted initiative. The AIDP instead functions as a stamp of approval for de-risking and actively incentivising local projects that make use of AI. Recognising this point is important: the AIDP is an ambitious strategy set by the central government, but the actual innovation and transformation is expected to be driven by the private sector and local governments. In other words, it is more appropriate to view the AIDP as a highly incentivised \u2018wish list\u2019, to nudge, and coordinate other relevant stakeholders, rather than a central directive (Sheehan 2018). This is why the 3-year plan promoting the AIDP (2018\u20132020) emphasises MMC Ventures. 2019. The State of AI 2019 Divergence.\nhttps:\/\/www.mmcventures.com\/The-State-of-AI-2019-Divergence.pdf\nMulgan, G. 2019. \u201cWhy Regulation Needs an Overhaul.\u201d Nesta (Blog), March 12.\nhttps:\/\/www.nesta.org.uk\/blog\/regulation-needs-overhaul\/\nMurison, Malek. 2018. \u201cBrazil\u2019s National IoT Strategy Gets the Green Light.\u201d Internet of Business, March 28.\nhttps:\/\/internetofbusiness.com\/brazil-national-iot-strategy\/\nMzekandaba, Simnikiwe. 2019. \u201cANC Promises to Bolster Tech Innovation.\u201d ITWeb, January 14.\nhttps:\/\/www.itweb.co.za\/content\/GxwQDM1A9BrMlPVo\nNational Communication Authority. 2018. Ghana\u2019s Digital Agenda.\nhttps:\/\/www.nca.org.gh\/assets\/Uploads\/Key-NCA-Projects-2018.pdf These incentive structures contain a subtle distinction. A national team has been selected to lead the research and development in a handful of designated strategic areas. Beyond these selected companies, there are few specific guidelines provided to industry and local state agents as to which items to pursue on the AIDP\u2019s \u2018wish list\u2019. This enables companies to cherry-pick the technologies they want to develop and provides local governments with a choice of private sector partners for integrating AI into city infrastructure or governance (Sheehan 2018). Subsequent documentation has emphasised the importance of strengthening organisation and implementation,Footnote4 including between provinces and ministries, yet it is unclear how this coordination would function in practice. Thus, the AIDP may work as a \u2018wish list\u2019, but the exact guidance, incentivisation and risk differ depending on the type of stakeholder. innovation (STI) with a holistic approach to\ndevelopment.53 South Africa is leading the continent\nin AI adoption with a strong start-up ecosystem\npredominantly driven by the private sector (Gadzala\n2018).\nInitiatives for AI adoption led by various stakeholders\nin the four countries exhibit a combination of both\ntop-down and bottom-up strategies. The top-down\napproach is particularly observed in governments\u2019\naspirations to invest in hubs and education\nprograms. In Egypt for example, the government\n\n52 Egypt\u2019s national AI strategy is available at https:\/\/stip.oecd.org\/stip\/policy-initiatives\/2019%2Fdata%2FpolicyInitiatives%2F26476\n53 The seven pillars address the coordination of all sectors involving STI, the private-public partnerships (PPPs), the R&D funding, the STEM education,\n\nrequired legislation as well as the implementation of AI across key sectors of the economy. See Government of Ghana. n.d. \u201cGov\u2019t using science, technology\nfor national development.\u201d"}
{"input":"Aiming at the realistic requirements of promoting the healthy and rapid development of AI in China, it is necessary to deal with the possible challenges of AI, what are China\u2019s guarantee measures?","output":"(1) Develop laws, regulations, and ethical norms that promote the development of AI\nStrengthen research on legal, ethical, and social issues related to AI, and establish laws, regulations and ethical frameworks to ensure the healthy development of AI. Conduct research on legal issues such as civil and criminal responsibility confirmation, protection of privacy and property, and information security utilization related to AI applications. Establish a traceability and accountability system and clarify the main body of AI and related rights, obligations, and responsibilities. Focus on autonomous driving, service robots, and other application subsectors with a comparatively good usage foundation and speed up the study and development of relevant safety management laws and regulations, to lay a legal foundation for the rapid application of new technology. Launch research on AI behavior science and ethics and other issues, establish an ethical and moral multi-level judgment structure and human-computer collaboration ethical framework. Develop an ethical code of conduct and R&D design for AI products, strengthen the assessment of the potential hazards and benefits of AI, and build solutions for emergencies in complex AI scenarios. China will actively participate in global governance of AI, strengthen the study of major international common problems such as robot alienation and safety supervision, deepen international cooperation on AI laws and regulations, international rules and so on, and jointly cope with global challenges.\n(2) Improve key policies for the support of AI development\nImplement tax incentives for small and mid-sized enterprise and startup AI development, and, using high-tech enterprises, tax incentives, R&D cost deductions, and other policies, support the development of AI enterprises. Improve the implementation of open data and protection-related policies, launch open public data reform pilots to support the public and enterprises in fully tapping the commercial value of public data, and promote the application of AI innovation. China will study the policy system of education, medical care, insurance, and social assistance to adapt to AI, and effectively deal with the social problems brought by AI.\n(3) Establish an AI technology standards and intellectual property system\nConduct research on strengthening the AI standards framework system. Adhere to the principles of security, availability, interoperability, and traceability; and gradually establish and improve the basic basis of AI, interoperability, industry applications, network security, privacy protection, and other technical standards. Speed up the promotion of autonomous driving, service robot, and other application sector industry associations in developing relevant standards. Encourage AI enterprises to participate in or lead the development of international standards, and a technical standards \u201cgoing out\u201d approach to promote AI products and services in overseas applications. Strengthen the protection of intellectual property in the field of AI, improve the field of AI technology innovation, patent protection, and standardization of interactive support mechanisms to promote the innovation of AI intellectual property rights. Establish AI public patent pools to promote the use of AI and the spread of new technologies.\n(4) Establish an AI security supervision and evaluation system\nStrengthen research and evaluation of the influence of AI on national security and secrecy protection; improve the security protection system of human, technology, material, and management support; and construct an early warning mechanism of AI security monitoring. Strengthen the development of AI technology prediction, research, and follow-up research, adhere to a problem-oriented, accurate grasping of technology and industry trends. Enhance the awareness of risk, pay attention to risk assessment and prevention and control, and strengthen prospective prevention and restraint guidance. In the near-term focus on the impact on employment, with a long-term focus on the impact on social ethics, to ensure that the development of AI falls with the sphere of secure and controllable. Establish and improve an open and transparent AI supervision system, the implementation of design accountability, and application of the supervision of a two-tiered regulatory structure, to achieve management of the whole process of AI algorithm design, product development and results application. Promote AI industry and enterprise self-discipline, and earnestly strengthen management, increase disciplinary efforts aimed at the abuse of data, violations of personal privacy, and actions contrary to moral ethics. Strengthen AI cybersecurity technology research and development, strengthen AI products and systems cybersecurity protection. Develop dynamic AI research and development evaluation mechanisms, focus on AI design, product and system complexity, risk, uncertainty, interpretability, potential economic impact, and other issues. Develop a systematic testing methods and indicators system. Construct a cross-domain AI test platform to promote AI security certification, and assessment of AI products and systems key performance.\n(5) Vigorously strengthen the training of an AI labor force\nAccelerate the study of the employment structure brought on by AI, changes in employment methods, and the skills demand of new occupations and jobs, establish a lifelong learning and employment training system to meet the needs of the intelligent economy and intelligent society, and support institutions of higher learning, vocational schools, and socialization training Institutions to carry out AI skills training. Substantially increase the professional skills of workers to meet the development requirements of China\u2019s AI to bring high-quality jobs. Encourage enterprises and organizations to provide AI skills training for employees. Strengthen the re-employment training and guidance of workers to ensure the smooth transfer of simple and repetitive workers due to AI.\n(6) Carry out a wide range of AI scientific activities\nSupport the development of a variety of AI scientific activities, encourage the broad masses of scientific and technological workers to join the promotion of AI popular science, and comprehensively improve the level of the whole society on the application of AI. Implement a universal intelligence education project. In the primary and secondary schools, set up AI-related courses, and gradually promote programming education to encourage social forces to participate in the promotion and development of educational programming software and games. Construct and improve the AI science infrastructure, give full play to all kinds of AI innovation base platforms and other popular science roles, encourage AI enterprises, and research institutions to build open-source platforms for public open AI research and development, plus production facilities or exhibition halls. Support the development of AI competitions, encourage the formation of a variety of AI science creational work efforts. Encourage scientists to participate in AI science.\n","context":"AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the perceived risks present in each of these areas. The AIDP highlights three areas where AI can make a substantial difference within China: international competition, economic development, and social governance. They are strictly interrelated but, for the sake of clarity, we shall analyse them separately, and contextualise each of them by discussing the relevant literature surrounding the broader political backdrop and contemporary policy debates. The AIDP provides a longitudinal perspective on China\u2019s strategic situation regarding AI, including its comparative capabilities, the opportunities offered, and the potential risks. Following a technology-first approach, it may be tempting to concentrate one\u2019s attention on the stated capabilities of AI, to gain an insight into the types of technologies in which China is investing. However, this would likely offer only a short-term perspective and would soon be out of date as technological innovation advances rapidly. Furthermore, it would do little to explain why China is seeking to develop a strong AI sector in the decades to come. To this end, it is more useful to try to understand China\u2019s strategic focus from a policy-first approach, by analysing the areas where China considers that AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory of, China\u2019s AI strategy. It emphasises that the Chinese government is aware of the potential benefits, practical risks, and the ethical challenges that AI presents, and that the direction of China\u2019s AI strategy will largely be determined by the interplay of these factors and by the extent to which government\u2019s interests may outweigh ethical concerns. Section 5 concludes the paper by summarising the key findings of our analysis. A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article."}
{"input":"What Are the UK\u2019s Five Principles for Regulating AI?","output":"In its white paper, the UK government focuses on five principles the government believes should govern AI to foster responsible development and use of the technology.  1. Safety, Security, and Robustness 2.Appropriate Transparency and Explainability, 3. Fairness. 4. Accountability and Governance, 5.Contestability and Redress.","context":"largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents."}
{"input":"What are the three exceptions(AI systems) that are not coverd in the AI act?","output":"AI systems exclusively developed or used for military purposes, and possibly defense and national security purposes more broadly, pending negotiations;  AI developed and used for scientific research; and, Free and open source AI systems and components (a term not yet clearly defined), with the exception of foundation models which are discussed below.","context":"This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. \u2022\n\nCOMPLIANCE AND RISK MANAGEMENT. AI systems are used to crossreference and reconcile terabytes of data from multiple sources to create alerts for noncompliance. For example, tax authorities can use AI to\ntrack tax filers who use duplicate profiles to avoid taxation, and social safety program may use AI to detect whether beneficiaries comply with eligibility criteria.\n\n\u2022\n\nFRAUD DETECTION, PREVENTION, AND INVESTIGATION.\n\n9\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR Why do we need rules on AI?\nThe proposed AI regulation ensures that Europeans can trust what AI has to offer. While most AI systems pose limited to no risk and can contribute to solving many societal challenges, certain AI systems create risks that we must address to avoid undesirable outcomes.\n\nFor example, it is often not possible to find out why an AI system has made a decision or prediction and taken a particular action. So, it may become difficult to assess whether someone has been unfairly disadvantaged, such as in a hiring decision or in an application for a public benefit scheme.\n\nAlthough existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring. Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach"}
{"input":"How did the research team differentiate their interview approach between the federal government and industry\/academia experts during the AI assessment","output":"The research team staggered non-DoD interviews relative to DoD interviews to better target them once they had a clearer understanding of DoD's posture and activities. The rules of engagement for interviews were consistent across government, industry, and academia, focusing on anonymity and organization listing","context":"(g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is (iv)   convening a cross-agency forum for ongoing collaboration between AI professionals to share best practices and improve retention.\n\n     (c)  Within 45 days of the date of this order, to advance existing Federal technology talent programs, the United States Digital Service, Presidential Innovation Fellowship, United States Digital Corps, OPM, and technology talent programs at agencies, with support from the AI and Technology Talent Task Force described in subsection 10.2(b) of this section, as appropriate and permitted by law, shall develop and begin to implement plans to support the rapid recruitment of individuals as part of a Federal Government-wide AI talent surge to accelerate the placement of key AI and AI-enabling talent in high-priority areas and to advance agencies\u2019 data and technology strategies. (b)  Within 45 days of the date of this order, to coordinate rapid advances in the capacity of the Federal AI workforce, the Assistant to the President and Deputy Chief of Staff for Policy, in coordination with the Director of OSTP and the Director of OMB, and in consultation with the National Cyber Director, shall convene an AI and Technology Talent Task Force, which shall include the Director of OPM, the Director of the General Services Administration\u2019s Technology Transformation Services, a representative from the Chief Human Capital Officers Council, the Assistant to the President for Presidential Personnel, members of appropriate agency technology talent programs, a representative of the Chief Data Officer Council, and a representative of the interagency council convened under subsection 10.1(a) of this section.  The Task Force\u2019s purpose shall be to accelerate and track the hiring of AI and AI-enabling talent across the Federal Government, including through the following (i)    within 180 days of the date of this order, tracking and reporting progress to the President on increasing AI capacity across the Federal Government, including submitting to the President a report and recommendations for further increasing capacity; \n\n          (ii)   identifying and circulating best practices for agencies to attract, hire, retain, train, and empower AI talent, including diversity, inclusion, and accessibility best practices, as well as to plan and budget adequately for AI workforce needs;\n\n          (iii)  coordinating, in consultation with the Director of OPM, the use of fellowship programs and agency technology-talent programs and human-capital teams to build hiring capabilities, execute hires, and place AI talent to fill staffing gaps; and\n\n          (iv)   convening a cross-agency forum for ongoing collaboration between AI professionals to share best practices and improve retention."}
{"input":"In what ways does the Brookings analysis suggest that congressional action differs from self-regulation in ensuring the effectiveness of the proposed AI Bill of Rights? ","output":"The analysis highlights that congressional action is crucial for establishing clear guidelines, enforceable legislation, and new data privacy rules to support the implementation of a rights-based AI governance framework, unlike self-regulation which may lack effective enforcement mechanisms","context":"U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. (d)  Artificial Intelligence policies must be consistent with my Administration\u2019s dedication to advancing equity and civil rights.  My Administration cannot \u2014 and will not \u2014 tolerate the use of AI to disadvantage those who are already too often denied equal opportunity and justice.  From hiring to housing to healthcare, we have seen what happens when AI use deepens discrimination and bias, rather than improving quality of life.  Artificial Intelligence systems deployed irresponsibly have reproduced and intensified existing inequities, caused new types of harmful discrimination, and exacerbated online and physical harms.  My Administration will build on the important steps that have already been taken \u2014 such as issuing the Blueprint for an AI Bill of Rights, the AI Risk Management Framework, and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies A broad-based national AI law like the EU Act is unlikely over the next few years. The administration likely builds on its AI \u201cbill of rights\u201d that spans different sectors, voluntary commitments, and executive orders. Voluntary rules are seen by many as a stop-gap measure, but a divided Congress is unlikely to pass a major law with new mandatory rules.  \n\nActions addressing sensitive areas such as health care, financial services (lending, insurance, housing), work force practices (discrimination), and child safety are more likely. Executive branch agencies are likely to move forward with existing authorities in the absence of new laws. \n\nPrivate tech companies advance their own responsible AI initiatives to serve their customers and avoid tougher government action. These are largely voluntary and seen as insufficient."}
{"input":"What challenges does Japan recognize in regulating AI, and how does it propose to address them?","output":"Japan acknowledges the difficulty of balancing AI risks with the acceleration of beneficial innovation and adoption due to the compliance burden and regulatory ambiguity potentially stifling innovation. It proposes addressing these challenges through a focus on maximizing AI's positive societal impact with an emphasis on a risk-based, agile, and multistakeholder approach rather than stringent prohibitions.","context":"These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. FUTURE RESEARCH\nAs policy and regulatory pathways for harnessing AI\nare still in their infancy, further time and research\nis needed to assess the outputs of these strategies,\nand their effectiveness in order to identify best\npractices for developing countries. Rather than\nconcluding with best practices, this article provides\nan initial review of policy and regulatory practices\ncurrently deployed in AI policymaking, illustrating\na wide range of tools and approaches that\ngovernments have taken in their attempt to harness\nAI technologies while mitigating anticipated risks.\nWhile many of the policies observed have been The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable."}
{"input":"What concerns do critics have regarding the EU's AI Act's potential impact on innovation and competitiveness?","output":"Critics, including EU businesses, have expressed concerns that the EU's AI Act could jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges faced. They argue that the act's extensive top-down prescriptive rules may stifle innovation","context":"Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition."}
{"input":"What are some examples of AI applications reported by federal agencies?","output":"Examples include analyzing data from cameras and radar for border activities, analyzing drone-collected photographs, and targeting scientific specimens for planetary rovers, specifically mentioned by NASA and the Department of Commerce.","context":"offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these objectives, agencies shall also consider opportunities to increase coordination, communication, and engagement about AI as appropriate with community-based organizations; civil-rights and civil-liberties organizations; academic institutions; industry; State, local, Tribal, and territorial governments; and other stakeholders. (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is \u2022\n\nCOMPLIANCE AND RISK MANAGEMENT. AI systems are used to crossreference and reconcile terabytes of data from multiple sources to create alerts for noncompliance. For example, tax authorities can use AI to\ntrack tax filers who use duplicate profiles to avoid taxation, and social safety program may use AI to detect whether beneficiaries comply with eligibility criteria.\n\n\u2022\n\nFRAUD DETECTION, PREVENTION, AND INVESTIGATION.\n\n9\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is adopted, deployed, and used."}
{"input":"How do AI regulatory sandboxes function?","output":"AI regulatory sandboxes facilitate systematic communication between regulators and AI developers, often on a voluntary basis, to improve regulatory compliance and legal certainty for companies while enhancing the regulator's understanding of AI system design, development, and deployment.","context":"There is no specific technical definition of an AI sandbox; the term can refer to a rangeof approaches from a simple ongoing exchange of documentation (from companies)and feedback (from regulators) all the way to a computing environment shared by acompany and regulators. This creates some uncertainty\u2014for instance, while theEuropean Parliament\u2019s version of the AI Act requires each EU member state toestablish at least one regulatory sandbox, it is not clear what precisely each countrywould implement.\nThe first such AI sandbox has been\nby a partnership between theEuropean Commission and the Spanish government, but regulatory sandboxes forother industries are not new. Over 50 countries have experimented with usingregulatory sandboxes for\n, and the\nothers in biotechnology, health, energy, and waste treatment. Some of thesesandboxes have performed assessments on AI systems, such as those from the UKICO and the U.S. , and the\nothers in biotechnology, health, energy, and waste treatment. Some of thesesandboxes have performed assessments on AI systems, such as those from the UKICO and the U.S.\n. Particularly valuable are thepublic reports published by the ICO when a financial technology application leaves theregulatory sandbox. These reports can\non how aspecific algorithmic system can comply with regulatory requirements, therebyinforming the public and other companies building similar applications.\nAI sandboxes have many distinguishing qualities relative to other AI regulatoryinterventions. First, they require ongoing collaboration between regulators andregulated companies, and may be less adversarial than an algorithmic audit.Sandboxes may require more work for companies (such as sharing updated data orensuring an algorithmic system works in a government computing environment).\nDevelop Regulatory AI Sandboxes 3.\nrecently launched\ndigital financial services\nOECD has documented AI regulatory sandboxes can also demand more from regulators, especially if theyentail developing a computing environment. Beyond the skills necessary foralgorithmic auditing, regulators would need to ensure that their computingenvironments can accommodate a broad range of algorithmic software in order toallow various AI developers to use the sandboxes. Further, regulators may have todevelop regulatory sandboxes that are capable of testing many distinct types ofalgorithmic systems, including algorithms built into phone apps, online platforms, andphysical products. Holding algorithmic systems indefinitely in government computingenvironments during development may increase risks to intellectual property,increasing the stakes of strong cybersecurity. Due to the significant workload requiredfor AI sandboxes, they may be more appropriate for relatively high-stakes algorithmicsystems. scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms."}
{"input":"How does the order plan to address the security risks associated with AI systems?","output":"It mandates the creation of standardized evaluations for AI systems, development of effective labeling for AI-generated content, and implementation of safeguards to mitigate security risks.","context":"(ii)   Within 150 days of the date of this order, the Secretary of the Treasury shall issue a public report on best practices for financial institutions to manage AI-specific cybersecurity risks.\n\n          (iii)  Within 180 days of the date of this order, the Secretary of Homeland Security, in coordination with the Secretary of Commerce and with SRMAs and other regulators as determined by the Secretary of Homeland Security, shall incorporate as appropriate the AI Risk Management Framework, NIST AI 100-1, as well as other appropriate security guidance, into relevant safety and security guidelines for use by critical infrastructure owners and operators. Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach Department of Defense, the Department of State, other relevant agencies, and the Intelligence Community to address the national security risks and potential benefits posed by AI.  In particular, the memorandum shall: Notably, the order requires the developers of the most powerful AI systems to share their safety test results with the government, promises federal support for development and agencies use of privacy-preserving techniques, requires an evaluation of how agencies collect and use commercially available data (including from data brokers), and requires increased training on how to investigate and prosecute civil rights violations related to AI.\n\nThe order tasks agencies with a number of responsibilities that will lead to standards from the National Institute of Standards and Technology to layout responsible AI testing frameworks and guidance for content authentication and watermarking."}
{"input":"What actions has the Biden-Harris Administration taken to regulate high-risk AI?","output":"The administration issued an Executive Order focusing on the safe, secure, and trustworthy development and use of AI, linking privacy to AI and requiring developers of powerful AI systems to share safety test results with the government.","context":"U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. (d)  Artificial Intelligence policies must be consistent with my Administration\u2019s dedication to advancing equity and civil rights.  My Administration cannot \u2014 and will not \u2014 tolerate the use of AI to disadvantage those who are already too often denied equal opportunity and justice.  From hiring to housing to healthcare, we have seen what happens when AI use deepens discrimination and bias, rather than improving quality of life.  Artificial Intelligence systems deployed irresponsibly have reproduced and intensified existing inequities, caused new types of harmful discrimination, and exacerbated online and physical harms.  My Administration will build on the important steps that have already been taken \u2014 such as issuing the Blueprint for an AI Bill of Rights, the AI Risk Management Framework, and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with"}
{"input":"How is international cooperation on AI currently being pursued?","output":"Governments are regulating AI, and international cooperation is taking place through trade agreements, digital economy agreements (DEAs), and forums like the G7 and the U.S.-EU Trade and Technology Council (TTC). These efforts include commitments to cross-border data flows, AI governance, and developing international AI standards.","context":"on Digital Cooperation.\nhttps:\/\/www.un.org\/en\/pdfs\/DigitalCooperation-report-for%20web.pdf\nUSAID.2018. Reflecting the Past, Shaping the Future: Making AI Work for International Development. Washington, DC.\nhttps:\/\/www.usaid.gov\/sites\/default\/files\/documents\/15396\/AI-ML-in-Development.pdf\nVillani, C. and others. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy.\nParis: Conseil national du num\u00e9rique.\nVinuesa, R., Azizpour, H., Leite, I. et al. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals.\nNature Communications, 11, 233.\nhttps:\/\/doi.org\/10.1038\/s41467-019-14108-y\nWhittaker, M. and others. 2018. AI Now Report 2018. New York: AI Now Institute, New York University.\nhttps:\/\/ainowinstitute.org\/AI_Now_2018_Report.pdf\nWhite House. 2020. American Artificial Intelligence Initiative: Year One Annual Report. Office of Science and Technology Policy. What have we learned?\nCountries and companies will clash in new ways, and AI could become an actor, not just a factor Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 International co-operation on AI\nInternational co-operation to promote the beneficial use of AI and address its\nchallenges is a priority for many countries. Co-operation for the development\nand adoption of AI and AI governance is being conducted at the bilateral,\nplurilateral, regional and international level. Moreover, many intergovernmental organisations with complementary mandates are engaged in AI\ninitiatives and projects. International co-operation on AI is taking place in fora\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021"}
{"input":"What are the three core provisions of the AIA that could have global considerations?","output":"The three core provisions are: AI systems in regulated products, high-risk AI systems for human services, and transparency requirements for AI that interacts with humans.","context":"Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC. globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d The EOAI also calls for the accelerated development of AI standards with international partners in standards organizations. 60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed (ii)  The Secretary of State and the Administrator of the United States Agency for International Development, in collaboration with the Secretary of Energy and the Director of NSF, shall develop a Global AI Research Agenda to guide the objectives and implementation of AI-related research in contexts beyond United States borders.  The Agenda shall:\n\n               (A)  include principles, guidelines, priorities, and best practices aimed at ensuring the safe, responsible, beneficial, and sustainable global development and adoption of AI; and\n\n               (B)  address AI\u2019s labor-market implications across international contexts, including by recommending risk mitigations."}
{"input":"How can international cooperation help align AI policies and regulations?","output":"Timely and focused international cooperation can help align AI policies and regulations by reducing regulatory burdens, incentivizing AI development, and increasing market competition at the global level. This is especially important as AI policy development is in the early stages in most countries","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. INTERNATIONAL LAWS\nSeveral other counties are more advanced than the U.S. in terms of AI policy development that works to protect people from algorithmic harm. For more information on AI laws and norms internationally, please see EPIC\u2019s International Policy page. The UN and the International Telecommunication\nUnion (ITU) have also taken steps towards\ninternational coordination by articulating an\napproach for the UN system to support AI adoption\nin developing countries and by hosting the annual\nAI for Good conference. UN agencies also apply\na baseline of standards in countries that may lack\nregulatory regimes, which can ultimately help guide\nand shape the formulation of national AI strategies."}
{"input":"How has China become embedded in international AI R&D networks?","output":"China has become embedded through multiple pathways such as enrollment at universities, conferences, joint publications, and work in research labs. This collaboration has helped China become a global leader in AI","context":"These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. In this article, we analysed the nature of AI policy within China and the context within which it has emerged, by mapping the major national-level policy initiatives that express the intention to utilise AI. We identified three areas of particular relevance: international competitiveness, economic growth, and social governance (construction). The development and deployment of AI in each of these areas have implications for China and for the international community. For example, although the \u2018trump-card\u2019 policy to gain a military advantage may not be something new, its application to AI technologies risks igniting an arms race and undermining international stability (Taddeo and Floridi 2018). Efforts to counteract this trend seem largely hollow. Our analysis indicates that China has some of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible 3.\nBy 2030, China seeks to become the world\u2019s innovation centre for AI. By then, growth in the core AI industry is expected to more than double again and be valued at 1 trillion yuan (ca 147 billion dollars), and further upgrades in the laws and standards are also to be expected, to deal with newly emerging challenges."}
{"input":"How does the framework propose to implement ethical AI within the UN?","output":"The framework proposes the establishment of architectural standards, assessment methods, and tools to ensure AI technologies adhere to ethical principles. It also recommends developing educational programs for UN staff to foster understanding and adherence to these ethical guidelines.","context":"Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors. emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to establish ethical principles but did form an AI ethics committee in mid-2018 with this purpose in mind (\u201cAI association to draft ethics guidelines\u201d 2019). \u201cPrincipled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI\u201d\nBerkman Klein Center Research Publication No. 2020-1\nhttps:\/\/ssrn.com\/abstract=3518482 or http:\/\/dx.doi.org\/10.2139\/ssrn.3518482\nFleming, Sean. 2018. \u201cIn Rwanda, High-Speed Drones are Delivering Blood to Remote Communities.\u201d World Economic Forum.\nhttps:\/\/www.weforum.org\/agenda\/2018\/12\/in-rwanda-drones-are-delivering-blood-to-remote-communities\/\nFloridi, Luciano, and others. 2018. \u201cAI4People\u2014An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.\u201d Minds & Machines 28 (4): 689\u2013707.\nhttps:\/\/doi.org\/10.1007\/s11023-018-9482-5 software & knowledge; regulation, testbeds and documentation; skills and\nlabour markets; and international co-operation.\nRead more: https:\/\/oecd.ai.\nUNESCO\u2019s Ad Hoc Expert Group on AI (AHEG)\nResponsible entity: UNESCO\nDescription: UNESCO has organised events to exchange knowledge about AI,\nfocusing on the dimensions of ethics, policy and capacity building. In March\n2020, UNESCO appointed 24 leading experts to an Ad Hoc Expert Group (AHEG)\non the ethics of AI. UNESCO\u2019s November 2019 General Conference tasked the\nAHEG with elaborating a recommendation on the ethics of artificial intelligence.\nRead more: https:\/\/en.unesco.org\/artificial-intelligence."}
{"input":"How can countries address the challenges of AI adoption in the public sector?","output":"Solutions involve developing AI policies with clear ethical guidelines, establishing AI innovation hubs, and creating whole-of-government AI frameworks. Emphasizing human oversight to manage AI risks and biases is also recommended.","context":"What are the main challenges?\nIn most of the World Bank\u2019s client countries, a lack of awareness of the potential of\nAI, inadequate foundational digital technologies, low availability or quality of data,\nand low access to digital skills, are the major barriers to AI adoption see Figure 1.\nAs of today, the appropriate policy and legal environment for AI do not yet exist.\nPublic administrations that lack digital processes, data collection capabilities,\ntechnical skills in the civil service and digital infrastructure are unlikely to be able\nto benefit from the application of AI in the short to medium term, and the policy\nand legal frameworks need to first be more developed.\nThat said, despite the digital divide that exists across countries in terms of\nfulfilling the prerequisites for AI adoption, several use cases are now emerging\nin our client countries and several governments have started piloting AI to\naddress their development challenges. 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. The Role of Government\nThe role of government in conceiving and developing\nan AI strategy varies from a hands-off approach,\nto providing a suitable enabling environment, to\nmore active direction of AI initiatives. While some\ncountries have taken a laissez-faire approach, this\nis not prevalent among countries leading in AI\nadoption. An examination of the early adopters of\nnational AI strategies shows that the government\u2019s\nrole typically varies from active driving of initiatives\non AI development and providing a supportive\nenvironment for stakeholders, to playing a more\npassive facilitating role, as noted in Box 1. While\nvarious stakeholders participate in implementing\nAI strategies, governments play a fundamental\nrole in setting AI policy direction and accelerating\ndevelopment and adoption. They can also lead AI\nadoption in public services to improve outcomes\nin health care, education, transportation, and\nadministrative efficiency. \u2022\n\nIdentify and manage risks rather than avoiding them. Good algorithm impact assessment framework models exist, which can\nbe tailored to suit a country\u2019s context.\n\nGovernments and world leaders are instrumental in guiding the transition to automation and AI. They can provide leadership to influence the trajectory of AI adoption among citizens at national and international levels. This will help avoid adverse consequences\nand reap productivity gains. National governments could choose global guiding principles that will inevitably shape the acceptance\nor rejection of AI. Since AI will have a profound influence on service delivery, citizen engagement, and core operations, it is imperative to formulate a cohesive governance model that supports the process of ethical implementation.\n12\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fSupported by the GovTech Global Partnership\n\nRepublic of Korea"}
{"input":"What are the types of information classified under the policy?","output":"Information is classified into categories such as \"Public,\" \"Official Use Only,\" \"Confidential,\" and \"Strictly Confidential,\" based on its sensitivity and the need to protect certain types of information.","context":"information is classified accordingly and restricted from disclosure pursuant to paragraph 14 of this\npolicy statement. from disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member\ncountries, clients, donors, recipients, or vendors, including consultants) and the financial\nand deliberative information that contains or relates to such information, is not\ndeclassified or made publicly available.\n26 See footnote 11.\n13\n33. Information Eligible for Declassification. The following documents are\ndeclassified and made publicly available 5, 10, or 20 years after the date on the IV.\nA.\n\nImplementation Aspects of the Policy\n\nAccessing Information\n\n22.\nProactive Disclosure. The Bank routinely discloses a wide range of documents,\nthrough its external website \u2013 www.worldbank.org \u2013 as soon as the documents are\nfinalized after key process milestones. Under this Policy, the Bank significantly\nincreases the amount of information it makes available to the public, particularly\ninformation related to projects under implementation and to the actions of the Board of\nExecutive Directors.\n\n19\n\n20 Go Top\n\nWhy does the AI Policy restrict some types of information from disclosure?\n\nThe AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the\npublic and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff and other\nparties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure\ncould cause harm to well defined interests.\n\n                                                                                                                    Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information?"}
{"input":"What was the focus of President Biden's Executive Order on Artificial Intelligence?","output":"President Biden's Executive Order emphasized the safe, secure, and trustworthy development and use of Artificial Intelligence, focusing on national security concerns and assigning new responsibilities to various federal agencies. Also, President Biden highlighted initiatives like developing the Blueprint for an AI Bill of Rights, securing commitments from tech companies for thorough testing of systems, and a $140 million grant to the National Science Foundation for AI research institutes.","context":"President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated. At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society."}
{"input":"What are some challenges regulatory agencies face in governing Algorithmic Decision-Making Systems (ADSs)?","output":"Regulatory agencies face challenges such as limited mandates to govern certain ADSs, lack of administrative subpoena authority for systematic review, and ambiguity in the application of existing regulatory authority to ADSs. ","context":"Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm. Agencies should use the tools they have to understandand regulate AI\nRegulators should actively consider what steps are necessary and valuable in theirdomains to ensure their regulatory mission is preserved. This includes cataloging andobserving emerging uses of algorithmic systems in their field, exploring what theirexisting statutory authority allows for, and hiring staff with expertise in algorithmicsystems. Regulators may benefit from a gap analysis\u2014identifying where current\ndocumented\nthousands ofpages\nled to congressional hearings\noffer cash rewards for whistleblowers\nauthorities and capacities are lacking so that they can inform legislators, who are farless likely to understand the nuances of every regulatory subfield.\nWhile regulators may often lack the most appropriate and best suited tools forinformation gathering about algorithmic systems, many will have some authority toperform information gathering. Beyond the interventions explored here, regulators canalso learn from Governments around the world are implementing foundational policies to regulateartificial intelligence (AI) and algorithmic systems more generally. While legislation isadvancing, regulators should not wait idly for legislators to act. Instead, regulatorsshould be actively learning about the algorithmic systems in their regulatory domainand evaluating those systems for compliance under existing statutory authority.\nMany regulatory agencies have started this work, including the U.S. Federal TradeCommission\u2019s (FTC)\nand Consumer Financial Protection Bureau(CFPB), new algorithmic regulators in\nand\n, and onlineplatform regulators such as the UK\u2019s\n(OFCOM) and the\n. These agencies and others havestarted to implement novel approaches and policies for AI regulation.\nOffice of Technology\nthe Netherlands\nSpain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023 \u2022  safeguards and mechanisms to ensure compliance with regulations\nAlgorithmic impact assessments exist in the Canadian public sector and require government agencies \nto complete a questionnaire before deploying automated decision-making.269\nThere is a voluntary Algorithmic Transparency Recording Standard that UK public bodies can use to \ndisclose information about their use of AI.270\nThe Institute for the Future of Work has stated that audits of AI systems could ensure that these \nsystems are compliant with their impact assessments and the law, and that the Digital Regulation \nCooperation Forum could play a role in helping regulators cooperate on these audits.264\nThe Information Commissioners Office currently carries out consensual and compulsory data \nprotection audits* of AI systems271. However, issues relating to AI are wider than data protection \nand fall under the remit of not just the Information Commissioners Office but multiple regulators"}
{"input":"How has the United States and the Biden administration escalated its focus on governing AI?","output":"The Biden administration created the AI Bill of Rights in 2022, and followed that up by announcing a pledge from fifteen leading technology companies to adopt shared standards for AI safety. The administration issued an executive order in October 2023 to create a unified framework for safe AI use in the executive branch, and a bipartisan group of senators proposed legislation to govern the technology.","context":"My Administration places the highest urgency on governing the development and use of AI safely and responsibly, and is therefore advancing a coordinated, Federal Government-wide approach to doing so.  The rapid speed at which AI capabilities are advancing compels the United States to lead in this moment for the sake of our security, economy, and society.\n\n     In the end, AI reflects the principles of the people who build it, the people who use it, and the data upon which it is built.  I firmly believe that the power of our ideals; the foundations of our society; and the creativity, diversity, and decency of our people are the reasons that America thrived in past eras of rapid change.  They are the reasons we will succeed again in this moment.  We are more than capable of harnessing AI for justice, security, and opportunity for all. The United States is likely to take a decentralized bottom-up approach that is messy and may have gaps and inconsistencies. This will likely disappoint proponents of strong AI regulation. Yet, over time this may produce a patchwork quilt of AI rules that if implemented well is grounded in the expertise of specific agencies and more tailored to innovation. The tremendous risks and opportunities of AI have made it a presidential-level issue. The White House is coordinating executive agencies, as each moves ahead with actions in its own domain. The lack of meaningful regulation of social media is widely seen as a failure in Washington, and bolsters efforts to be more proactive with AI. Yet the decentralized structure of the U.S. government, political differences, and the complexities of AI forestall big, quick actions. There is also a real but less-likely probability that a big AI-related failure sparks a strong national government action. The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023"}
{"input":"What is the regulation around training powerful AI models in Europe?","output":"Regulations in Europe mandate that developers must include drafting technical documentation, adhere to EU copyright laws, and provide detailed summaries of the content used for training. Moreover, for high-impact general purpose AI models that pose systemic risks, additional obligations apply. These obligations entail conducting model evaluations, assessing and mitigating systemic risks, performing adversarial testing, reporting serious incidents to the Commission, ensuring cybersecurity measures, and reporting on energy efficiency.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. Why do we need rules on AI?\nThe proposed AI regulation ensures that Europeans can trust what AI has to offer. While most AI systems pose limited to no risk and can contribute to solving many societal challenges, certain AI systems create risks that we must address to avoid undesirable outcomes.\n\nFor example, it is often not possible to find out why an AI system has made a decision or prediction and taken a particular action. So, it may become difficult to assess whether someone has been unfairly disadvantaged, such as in a hiring decision or in an application for a public benefit scheme.\n\nAlthough existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring. INTERNATIONAL LAWS\nSeveral other counties are more advanced than the U.S. in terms of AI policy development that works to protect people from algorithmic harm. For more information on AI laws and norms internationally, please see EPIC\u2019s International Policy page."}
{"input":"What is one potential harm of new AI systems?","output":"One potential harm of new AI systems is the creation and dissemination of deepfake content, particularly non-consensual pornographic material, leading to harassment and distress for individuals. The risk of non-consensual deepfake material has prompted legislative action, such as criminalization under the UK's Online Safety Act 2023. Additionally, generative AI poses security risks by enabling the creation of fake personas or impersonation of real people, potentially facilitating the release of confidential information to malicious actors and increasing the effectiveness of phishing and scam attempts.","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d We are entering an era of both enlightenment and chaos\nThe borderless nature of AI makes it hard to control or regulate. As computing power expands, models are optimized, and open-source frameworks mature, the ability to create highly impactful AI applications will become increasingly diffuse. In such a world, well-intentioned researchers and engineers will use this power to do wonderful things, ill-intentioned individuals will use it to do terrible things, and AIs could do both wonderful and terrible things. The net result is neither an unblemished era of enlightenment nor an unmitigated disaster, but a mix of both. Humanity will learn to muddle through and live with this game-changing technology, just as we have with so many other transformative technologies in the past. environmental toll. The Guardian.\n202. Hsu, J. (2023). Shifting where data is processed for AI can reduce environmental harm. New \nScientist.\n203. M\u00fcller, V. C. et al. (2016). Future Progress in Artificial Intelligence: A Survey of Expert \nOpinion. in Fundamental Issues of Artificial Intelligence. (ed. M\u00fcller, V. C.) 555\u2013572. Springer \nInternational Publishing.\n204. Bubeck, S. et al. (2023). Sparks of Artificial General Intelligence: Early experiments with \nGPT-4. arXiv.\n205. Metz, C. (2023). Microsoft Says New AI Shows Signs of Human Reasoning. The New York Times.\n206. Nathan, A. et al. (2023). Generative AI: hype, or truly transformative? Goldman Sachs.\n23                                  POSTnote 708 - 9 January 2024 Notes\n[1] Tai Ming Cheung and Thomas G. Mahnken, The Decisive Decade: United States-China Competition in Defense Innovation and Defense Industrial Policy in and Beyond the 2020s, Center for Strategic and Budgetary Assessments, May 22, 2023.\n[2] Yoshua Bengio, \"AI and Catastrophic Risk,\" Journal of Democracy, Vol. 34, No. 4, October 2023.\n[3] Christopher Mouton, Caleb Lucas, and Ella Guest, The Operational Risks of AI in Large Scale Biological Attacks: A Red-Team Approach, RAND Corporation, RR-A2977-1, 2023.\n[4] Will Knight, \"OpenAI's CEO Says the Age of Giant AI Models Is Already Over,\" Wired, April 17, 2023."}
{"input":"What was a major point of contention during the negotiations, and how was it addressed?","output":"AI-powered face recognition surveillance systems; a compromise was found after intensive bargaining.","context":"Secretary Blinken is correct. The current era of strategic geopolitical competition is centered around leadership in technology. BIS plays a vital role in ensuring that U.S. technology is not improperly used by foreign governments in ways that are counter to U.S. national security interests. The new export controls policy demonstrates that the Biden administration understands the critical role that BIS must play in this new era.\n\nHowever, the same cannot be said for the FY 2023 President\u2019s Budget request or FY 2022 congressional appropriation. Neither provides adequate resources for BIS to succeed in its vital mission of strategic technology competition. As described above, the debates occur within a constrained political and intellectual environment. \nFew of these policy actors will swim against the ideological stream, and policy solutions that contravene Xi\u2019s expressed wishes will not be entertained. How much latitude these actors have depends on the political salience of the issue at stake. For highly sensitive political issues, such as the status of Taiwan, the bounds of public discussion are extremely narrow. And what counts as political and sensitive has continuously expanded under Xi. At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups. The primary barriers to improving BIS digital and data infrastructure and the efficacy of export controls are a lack of funding and a lack of staff with the required skill sets.\n\nAs mentioned earlier in this paper, BIS funding levels adjusted for inflation have been flat for more than a decade. This reflects the BIS funding and staffing levels that Congress assessed as prudent when the primary focus of national security was terrorism and U.S. military operations in Iraq and Afghanistan. This is no longer the global security environment in which BIS is operating. On October 17, 2022, 10 days after the Commerce Department announced its new export controls policy toward China, Secretary of State Antony Blinken gave a major speech at Stanford University, stating: \u201cWe are at an inflection point. The post-Cold War world has come to an end, and there is an intense competition underway to shape what comes next. And at the heart of that competition is technology.\u201d"}
{"input":"How does the Act intend to handle the rapid expansion and capabilities of AI systems?","output":"By setting obligations based on potential risks and impacts, including transparency for GPAI systems and stringent obligations for high-impact models to ensure they do not present systemic risks.","context":"limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is adopted, deployed, and used. My Administration places the highest urgency on governing the development and use of AI safely and responsibly, and is therefore advancing a coordinated, Federal Government-wide approach to doing so.  The rapid speed at which AI capabilities are advancing compels the United States to lead in this moment for the sake of our security, economy, and society.\n\n     In the end, AI reflects the principles of the people who build it, the people who use it, and the data upon which it is built.  I firmly believe that the power of our ideals; the foundations of our society; and the creativity, diversity, and decency of our people are the reasons that America thrived in past eras of rapid change.  They are the reasons we will succeed again in this moment.  We are more than capable of harnessing AI for justice, security, and opportunity for all. Policy implications of artificial intelligence (AI)\n27                                  POSTnote 708 - 9 January 2024\nContributors\nPOST is grateful to Ansh Bhatnagar for researching this briefing, to STFC for funding their \nparliamentary fellowship, and to all contributors and reviewers. For further information on this \nsubject, please contact the co-author, Devyani Gajjar.\nMembers of the POST Board*\nDr Elena Abrusci, Brunel University London* Dr Mhairi Aitken, Alan Turing Institute* Emmanuelle \nAndrews, Liberty\nDr Hayleigh Bosher, Brunel University London* Matt Davies, Ada Lovelace Institute\nMaximilian Gahntz, Mozilla Foundation Conor Griffin, Google DeepMind\nProfessor Oliver Hauser, University of Exeter Harry Law, Google*\nMia Leslie, Public Law Project*\nMavis Machirori, Ada Lovelace Institute Professor Gina Neff, University of Cambridge\nSam Nutt, London Office of Technology and Innovation Lucy Purdon, Mozilla Foundation*\nAdam Smith, British Computer Society* The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with"}
{"input":"What innovative approach does China use in building its AI governance framework?","output":"Targeted regulations to build bureaucratic know-how and a series of more targeted AI regulations.","context":"In this series of three papers, I will attempt to reverse engineer Chinese AI governance. \nI break down the regulations into their component parts\u2014the terminology, key concepts, and specific requirements\u2014and then trace those components to their roots, revealing how 3.The \u201cPolicy Funnel\u201d of China\u2019s AI Governance Major governance initiatives tend to proceed from left to right through this funnel, though often not in a linear fashion. AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. How China Sets AI Governance Policy\nThis paper presents a four-layered policy funnel through which China formulates and pro-mulgates AI governance regulations. Those four layers are real-world roots; Xi Jinping and CCP ideology; the \u201cworld of ideas\u201d; and the party and state bureaucracies. These layers are porous, and regulations do not proceed through them in a purely linear fashion.\n\nInstead, they often pinball forward and backward through these layers, getting shaped and reshaped by academics, bureaucrats, public opinion, and CCP ideology. The order and relative importance of the layers also varies depending on the nature of the issue confronted.\n\nSo far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers. AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the perceived risks present in each of these areas. The AIDP highlights three areas where AI can make a substantial difference within China: international competition, economic development, and social governance. They are strictly interrelated but, for the sake of clarity, we shall analyse them separately, and contextualise each of them by discussing the relevant literature surrounding the broader political backdrop and contemporary policy debates."}
{"input":"What is the UK Government's approach to AI regulation as of March 2023?","output":"The UK adopted a 'pro-innovation' approach, largely regulating AI through existing laws and principles for safety, security, and transparency among others.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach."}
{"input":"What are several challenges and ways to mitigate them?","output":"Broadly defined harms, resource constraints, congressional action, guideline adoption, and industry and private sector engagement.","context":"Mitigating risk and addressing malicious actors. The existence of risk may not justify full regulation if those risks can be mitigated, as in other domains (e.g., mitigating environmental impacts or mitigating financial risk through lending standards and insurance). Rules also need to reflect the difference between legitimate businesses seeking to comply with rules and malicious actors with bad intent. Malicious actions (cybersecurity, cyber-crime, terrorism) pursuing dangerous goals (theft, physical safety, child safety) need stronger rules and more punitive deterrents. administration charged with delivering them; but several risks also arise. These\ninclude managing inherent biases in data and algorithms that exacerbate or\ncreate problems. or jeopardize safety or rights.  At the same time, my Administration will promote responsible uses of AI that protect consumers, raise the quality of goods and services, lower their prices, or expand selection and availability. lawsuits to punish lax algorithmic practices. Allthese benefits can arise even without the regulators themselves using publicinformation, although transparency also helps inform better policymaking and otherinterventions from the AI regulatory toolbox."}
{"input":"Who should be a member of the White House AI Council?","output":"(i) the Secretary of State;\n (ii) the Secretary of the Treasury;\n (iii) the Secretary of Defense;\n (iv) the Attorney General;\n (v) the Secretary of Agriculture;\n (vi) the Secretary of Commerce;\n (vii) the Secretary of Labor;\n (viii) the Secretary of HHS;\n (ix) the Secretary of Housing and Urban Development;\n (x) the Secretary of Transportation;\n (xi) the Secretary of Energy;\n (xii) the Secretary of Education;\n (xiii) the Secretary of Veterans Affairs;\n (xiv) the Secretary of Homeland Security;\n (xv) the Administrator of the Small Business Administration;\n (xvi) the Administrator of the United States Agency for International Development;\n (xvii) the Director of National Intelligence;\n (xviii) the Director of NSF;\n (xix) the Director of OMB;\n (xx) the Director of OSTP;\n (xxi) the Assistant to the President for National Security Affairs;\n (xxii) the Assistant to the President for Economic Policy;\n (xxiii) the Assistant to the President and Domestic Policy Advisor;\n (xxiv) the Assistant to the President and Chief of Staff to the Vice President;\n (xxv) the Assistant to the President and Director of the Gender Policy Council;\n (xxvi) the Chairman of the Council of Economic Advisers;\n (xxvii) the National Cyber Director;\n (xxviii) the Chairman of the Joint Chiefs of Staff; and\n (xxix) the heads of such other agencies, independent regulatory agencies, and executive offices as the Chair may from time to time designate or invite to participate.","context":"Sec. 12.  Implementation.  (a)  There is established, within the Executive Office of the President, the White House Artificial Intelligence Council (White House AI Council).  The function of the White House AI Council is to coordinate the activities of agencies across the Federal Government to ensure the effective formulation, development, communication, industry engagement related to, and timely implementation of AI-related policies, including policies set forth in this order.\n\n     (b)  The Assistant to the President and Deputy Chief of Staff for Policy shall serve as Chair of the White House AI Council.\n\n     (c)  In addition to the Chair, the White House AI Council shall consist of the following members, or their designees:\n\n          (i)       the Secretary of State;\n\n          (ii)      the Secretary of the Treasury;\n\n          (iii)     the Secretary of Defense;\n\n          (iv)      the Attorney General;\n\n          (v)       the Secretary of Agriculture; 10.1.  Providing Guidance for AI Management.  (a)  To coordinate the use of AI across the Federal Government, within 60 days of the date of this order and on an ongoing basis as necessary, the Director of OMB shall convene and chair an interagency council to coordinate the development and use of AI in agencies\u2019 programs and operations, other than the use of AI in national security systems.  The Director of OSTP shall serve as Vice Chair for the interagency council.  The interagency council\u2019s membership shall include, at minimum, the heads of the agencies identified in 31 U.S.C. 901(b), the Director of National Intelligence, and other agencies as identified by the Chair.  Until agencies designate their permanent Chief AI Officers consistent with the guidance described in subsection 10.1(b) of this section, they shall be represented on the interagency council by an appropriate official at the Assistant Secretary level or equivalent, as determined by the head of each agency. (xxix)    the heads of such other agencies, independent regulatory agencies, and executive offices as the Chair may from time to time designate or invite to participate.\n\n     (d)  The Chair may create and coordinate subgroups consisting of White House AI Council members or their designees, as appropriate.\n\n     Sec. 13.  General Provisions.  (a)  Nothing in this order shall be construed to impair or otherwise affect:\n\n          (i)   the authority granted by law to an executive department or agency, or the head thereof; or\n\n          (ii)  the functions of the Director of the Office of Management and Budget relating to budgetary, administrative, or legislative proposals.\n\n     (b)  This order shall be implemented consistent with applicable law and subject to the availability of appropriations. In March 2023, Secretary Mayorkas tasked the Homeland Security Advisory Council to\u202fexamine and provide recommendations on the development of the Department\u2019s AI Strategy. The Council has formed two subcommittees, one to focus on how DHS can leverage AI to advance critical missions, and the second on how DHS should be building defenses to nefarious uses of AI by adversaries. The Council provides recommendations to the Secretary on ways the Department can better meet the challenges of the evolving threat landscape and seize opportunities to better serve the American people."}
{"input":"Does the AI training process infringe on copyright in other works?","output":"AI companies may argue that their training processes constitute fair use and are therefore noninfringing. Whether or not copying constitutes fair use depends on four statutory factors under 17 U.S.C. \u00a7 107: 1. the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes; 2. the nature of the copyrighted work; 3. the amount and substantiality of the portion used in relation to the copyrighted work as a whole; and 4. the effect of the use upon the potential market for or value of the copyrighted work.","context":"(iii)  within 270 days of the date of this order or 180 days after the United States Copyright Office of the Library of Congress publishes its forthcoming AI study that will address copyright issues raised by AI, whichever comes later, consult with the Director of the United States Copyright Office and issue recommendations to the President on potential executive actions relating to copyright and AI.  The recommendations shall address any copyright and related issues discussed in the United States Copyright Office\u2019s study, including the scope of protection for works produced using AI and the treatment of copyrighted works in AI training. Generative AI tools are trained using datasets, which may or may not be open to the public (PB 57). \nGenerative AI tools can output written, visual, aural, or audio-visual works that can mimic the \nstyle of specific human creators if their works are present in the datasets.186 This raises \nimplications for intellectual property rights, regardless of whether companies are transparent with \ndatasets or not.186\u2013191\nIn ongoing court cases, some authors and rightsholders have sued model developers in the UK and US \nalleging copyright infringement based on outputs that may imply that the datasets contain their \nwork.192,193\nThere are also differing views on whether copyright should lie with users of AI tools, developers, \nthose whose works appear in the dataset, or with nobody at all.186,194,195\nResource requirements\nThe Competition and Markets Authority,196 US Federal Trade Commission,197 and researchers,198,199 Review.\n189. Smits, J. et al. (2022). Generative AI and Intellectual Property Rights. in Law and Artificial \nIntelligence: Regulating AI and Applying AI in Legal Practice. (eds. Custers, B. et al.) 323\u2013344.\nT.M.C. Asser Press.\n190. Chesterman, S. (2023). Good Models Borrow, Great Models Steal: Intellectual Property Rights \nand Generative AI. National University of Singapore.\n191. Baker, C. (2023). Intellectual property and generative AI. Deloitte.\n192. Italie, H. (2023). \u2018Game of Thrones\u2019 creator and other authors sue ChatGPT-maker OpenAI for \ncopyright infringement. AP News.\n193. Getty Images (2023). Getty Images Statement. Getty Images.\n194. Bosher, H. (2023). Policy Brief: Copyright, generative AI and data mining. Brunel University \nLondon.\n195. Brittain, B. (2023). AI-generated art cannot receive copyrights, US court says. Reuters.\n196. Competition and Markets Authority (2023). AI Foundation Models: Initial report. GOV.UK. websites. That also poses a challenge for the draft\u2019s requirement that training data not violate\nintellectual property rights. The regulation mandates that generative AI not be discriminato-ry on the basis of race or sex and that generated content be \u201ctrue and accurate,\u201d an unsolved technical problem for LLMs that are prone to \u201challucinating\u201d inaccurate or baseless claims in their outputs."}
{"input":"What is the Critical Algorithmic System Classification(CASC)?","output":"A legal designation that can be applied to an ADS category through the federal rulemaking process, leading to legally binding and enforceable rules for that ADS category.","context":"You should not face discrimination by algorithms and systems should be used and designed in an equitable way. Algorithmic discrimination occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate legal protections. Designers, developers, and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination and to use and design systems in an equitable way. This protection should include proactive equity assessments as part of the system design, use of representative data and protection on how their algorithms are trained and deployed, including which datasets the algorithm is trained on. They are also required to complete an \u201calgorithm security self-assessment report\u201d. Once an algorithm is successfully registered, a limited version of the filing is made public. Subsequent regulations on deep synthesis and generative AI also required developers to register their algorithms. The second paper in this series will dig into the key goals and mechanisms of the algorithm registry. Algorithmic Discrimination Protections (C)  identify instances in which reliance on automated or algorithmic systems would require notification by the State, local, Tribal, or territorial government to the Secretary;\n\n               (D)  identify instances when applicants and participants can appeal benefit determinations to a human reviewer for reconsideration and can receive other customer support from a human being;\n\n               (E)  enable auditing and, if necessary, remediation of the logic used to arrive at an individual decision or determination to facilitate the evaluation of appeals; and\n\n               (F)  enable the analysis of whether algorithmic systems in use by benefit programs achieve equitable outcomes."}
{"input":"How proliferation of big technology companies, combined with AI advancement impacts geopolitical power dynamics?","output":"Big technology companies, operating across national boundaries, exert influence at local, national, and international levels, vastly impacting geopolitical power dynamics by shaping consumer behavior and gathering vast amounts of data. This phenomenon has led to polarization within populations and even regime changes in some countries.","context":"Countries and corporations have long competed for power. Big technology companies challenge governments in many of the old ways while adding new approaches to the mix. Similar to traditional multinational corporations, such companies reach across national boundaries; however, big tech companies also influence local communities in much more comprehensive and invasive ways because they touch consumers' lives and gather data on their locations, activities, and habits. Big tech companies also affect national economies, domestic policy, and local politics in new ways because they influence the spread of information (and disinformation) and create new communities and subcultures. This has contributed to polarized populations in several countries and regime change in others. AI companies specifically enjoy access to vast investment funds and massive computing power, giving them additional advantages. AI and Geopolitics\nHow Might AI Affect the Rise and Fall of Nations?\n\nPublished Nov 3, 2023\n\nby Barry Pavel, Ivana Ke, Michael Spirtas, James Ryseff, Lea Sabbag, Gregory Smith, Keller Scholl, Domenique Lumpkin\n\nNations across the globe could see their power rise or fall depending on how they harness and manage the development of artificial intelligence (AI). Regardless of whether AI poses an existential risk to humanity, governments will need to develop new regulatory frameworks to identify, evaluate, and respond to the variety of AI-enabled challenges to come. Although technology has often influenced geopolitics, the prospect of AI means that the technology itself could become a geopolitical actor. AI could have motives and objectives that differ considerably from those of governments and private companies. Humans' inability to comprehend how AI \"thinks\" and our limited understanding of the second- and third-order effects of our commands or requests of AI are also very troubling. Humans have enough trouble interacting with one another. It remains to be seen how we will manage our relationships with one or more AIs. to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response."}
{"input":"What are some challenges associated with achieving interoperability among AI governance frameworks at the international level?","output":"Some key challenges in achieving interoperability:\n - Varying approaches and policy instruments among countries\n - Differences in defining and assessing risks associated with AI\n - Need for ongoing discussions to align regulatory frameworks and international technical standards","context":"The Road Ahead: Achieving Interoperability in AI Governance Frameworks\nTo maximize the shared values while mitigating the aforementioned risks, the G7 statements highlighted the necessity of interoperability among AI governance frameworks to foster trustworthy AI. This list outlines what exactly such a framework entails:\n\nRisk-Based and Forward-Looking Approaches under Different Policy Frameworks The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI governance that can otherwise stifle innovation and reduce opportunities for uptake and use of AI; and finding ways to broaden access globally to the computing power and data that is essential for building and training AI models. Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC."}
{"input":"What is the contrast between US and EU approaches to AI risk management?","output":"The contrast between the U.S. and EU approaches to AI risk management lies in the level of regulatory coverage, central coordination, and enforcement mechanisms. While both adopt risk-based approaches and advocate for trustworthy AI principles, the EU's approach is more centrally coordinated and comprehensive. The EU has implemented legislation like the GDPR, DSA, and DMA, and is currently developing the AI Act, which includes detailed regulatory requirements for high-risk AI systems. In contrast, the U.S. approach relies on sectorally specific regulations and non-binding guidance, with regulatory plans developed slowly across federal agencies. Enforcement in the EU is backed by investigatory powers and significant fines for non-compliance, whereas U.S. agencies may need to pursue novel litigation without explicit legal authority to regulate algorithms. Despite some overlap in principles, the U.S. and EU approaches exhibit significant differences in regulatory scope, transparency, and enforcement mechanisms.","context":"Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach"}
{"input":"Why is it important for law enforcement to be fully part of the national blueprint guidance?","output":"It's crucial to include law enforcement in the national blueprint guidance to prevent discrimination, ensure accountability, and protect civil liberties. Excluding law enforcement could perpetuate biases and hinder oversight, while incorporating them ensures that AI technologies are deployed ethically and transparently, fostering public trust and confidence.","context":"(A)  identify areas where AI can enhance law enforcement efficiency and accuracy, consistent with protections for privacy, civil rights, and civil liberties; and\n\n               (B)  recommend best practices for law enforcement agencies, including safeguards and appropriate use limits for AI, to address the concerns set forth in section 13(e)(i) of Executive Order 14074 as well as the best practices and the guidelines set forth in section 13(e)(iii) of Executive Order 14074; and  \n\n          (iii)  supplement the report set forth in subsection 7.1(b)(i) of this section as appropriate with recommendations to the President, including with respect to requests for necessary legislation.  \n\n     (c)  To advance the presence of relevant technical experts and expertise (such as machine-learning engineers, software and infrastructure engineering, data privacy experts, data scientists, and user experience researchers) among law enforcement professionals: The Blueprint notes that individuals must be protected from abusive data practices and calls for data minimization rules, stating \u201c[y]ou should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected.\u201d\n\nIn the days following its release, EPIC Deputy Director Caitirona Fitzgerald and Senior Counsel Ben Winters published an op-ed in Protocol, urging the Biden Administration to take action to bring the Blueprint into practice. The Fall 2023 Executive Order and Office of Management and Budget Guidance does exactly that. In addition to the $25 million for technology modernization and analyst staff, Congress should provide $18.4 million in annual funding for additional enforcement agents.\n\nNotably, many individuals said that while new technology could radically improve the productivity of the export administration and enforcement analytic communities, additional resources are likely needed for other parts of the BIS export enforcement enterprise. This is because a more productive enforcement analysis community will identify more entities as likely shell companies engaging in illicit transactions. This will in turn increase the need for enforcement agents to conduct site inspections or criminal investigations of these identified entities. Despite the severe current technological limitations on the efficacy of the analytic community, its work is already identifying enough candidate entities for inspection to more than fully consume the capacity of the current staff. Developing guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination\nSome top takeaways from the Executive Order, according to the MIT Technology Review's Tate Ryan-Mosley and Melissa Heikkil\u00e4, include requiring the Department of Commerce to develop guidance for labeling AI-generated content, such as watermarking tools; compelling developers to share safety test results for new AI models that may pose a national security risk; and forcing AI companies to be more transparent about how their new models work, among other \u201csafety and security\u201d priorities."}
{"input":"What are some sector specific regulations that have been implemented in Japan?","output":"Some sector-specific regulations implemented in Japan include the Digital Platform Transparency Act and the Financial Instruments and Exchange Act. The Digital Platform Transparency Act imposes requirements on large online malls, app stores, and digital advertising businesses to ensure transparency and fairness in transactions with business users, while the Financial Instruments and Exchange Act regulates businesses engaging in algorithmic high-speed trading, requiring registration with the government and the establishment of risk management systems.","context":"Another point to consider is that the G7 statements do not explicitly address the link between regulations and standards. Even if international standards are established, unless they are aligned with each country's regulatory content and civil liability systems, businesses could still face challenges due to regulatory inconsistencies. This suggests a need for ongoing discussions among nations, not just to support the development of standards, but also to understand the interplay between regulations and standards.\nFurther Collaboration in DFFT and Emerging Technologies So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. Third, the government has become increasingly creative in creating complex new rules that are often more difficult to administer and enforce. For example, the U.S. approach to export controls has changed significantly via the Foreign Direct Product (FDP) Rule, which consists of an extraterritorial application of U.S. export controls on items manufactured with U.S. equipment, inputs, or design. The application of the FDP rule imposes new license requirements on exports to Russia and China and institutes a presumption of denial policy for license applications for exports or re-exports to Russia and China. The FDP rule is intended to restrict Russia and China\u2019s abilities to acquire critical goods such as microelectronics, telecommunications items, and other components with military applications, including from third countries. Five cross-sectoral principles: When implementing a context-specific approach, regulators must have regard to five cross-sectoral principles, as explained further below."}
{"input":"What opportunity does Vice President Kamala Harris have regarding international AI governance?","output":"Vice President Kamala Harris, as head of the U.S. delegation to the U.K. AI Safety Summit, has the opportunity to lead the further development of international AI governance.","context":"The EOAI along with the other domestic AI policies outlined will have significant international impacts through various channels.\n\nFirst and most immediately, these suites of domestic policy developments on AI governance give Vice President Kamala Harris as head of the U.S. delegation to the U.K. AI Safety Summit a real opportunity to lead the further development of international AI governance. The U.S. position on AI stands in stark contrast to the lack of strong U.S. leadership on privacy regulation, where the absence of federal privacy legislation created a vacuum that the EU\u2019s General Data Protection Regulation (GDPR) filled, allowing GDPR to become a leading model for privacy regulation worldwide. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 On day one of the Summit, Vice President Harris announced new US actions meant to build on Monday\u2019s Executive Order, with the stated goal of strengthening international rules and norms with different allies and partners. This includes launching the US AI Safety Institute (AISI) inside the National Institute of Standards and Technology (NIST), and via the Department of Commerce, that will operationalize its AI Risk Management Framework. The Office of Management and Budget (OMB) has developed first-ever draft policy guidance for public review, according to the Vice President, that will build on the Blueprint for an AI Bill of Rights and NIST\u2019s Framework. Additionally, the US is joining 31 other nations, including countries like France, Germany, and the UK, in endorsing the Political Declaration on Responsible Military Use of Artificial Intelligence and Autonomy that responsibly and lawfully harnesses AI capabilities, like autonomous functioning systems. And finally, the White House Finally, there remains one auxiliary goal: making China a leader in the governance and regulation of AI. U.S. and Chinese leaders frequently point out that China has laid out some of the world\u2019s first binding regulations on AI, with the latter using it as a point of pride and the former as an impetus to action. But the rhetorical emphasis on global leadership often leads to a mistaken impression that this is a major driver of Chinese actions. An examination of the regulations and conversations with Chinese policy actors indicates otherwise. For China, Matt Sheehan | 17\nbeing a global leader or model for AI governance is a \u201cnice-to-have\u201d\u2014a small bonus for its businesses and national soft power, but not a significant driver of these AI regulations.\n\nChina\u2019s choice of first targets for regulation\u2014recommendation algorithms and deep synthesis\u2014indicates that global leadership is not a core motivation for its AI governance."}
{"input":"Why is the pace of adding Chinese companies to the Entity List a source of frustration for Congress and the White House?","output":"The slow pace of adding Chinese companies to the Entity List frustrates Congress and the White House because it delays the implementation of policies aimed at restricting Chinese access to critical technologies, such as AI chip design and semiconductor manufacturing equipment.","context":"But international discourse on Chinese AI governance often fails to take these regulations seriously, to engage with either their content or the policymaking process. \nInternational commentary often falls into one of two traps: dismissing China\u2019s regulations as irrelevant or using them as a political prop. Analysts and policymakers in other countries often treat them as meaningless pieces of paper. \nPresident Xi Jinping and the Chinese Communist Party (CCP) have unchecked power to disregard their own rules, the argument goes, and therefore the regulations are unimportant. \nOther U.S. policy actors use the specter of Chinese AI governance to advance their agendas. When Senate Majority Leader Chuck Schumer announced his plans to begin regulating AI earlier this year, he described China\u2019s efforts as a \u201cwake up call to the nation, \u201d warning that the United States could not afford to let its geopolitical adversary \u201cwrite the rules of the road\u201d for AI. The October 7, 2022, package of export controls against China is too new for comparable data to be available at the time of this writing, but the same caution applies: published government data does not necessarily tell the whole story, which must focus on both licit and illicit transactions. \n\nFor BIS, the scale and complexity of U.S. export control requirements have increased massively over the past three years due to new legislative and executive requirements.\n\nFor those exports that do require a license, the typical process for export controls administration and enforcement is depicted in Figure 1.\n\n\n\nIncreasingly sophisticated and well-resourced attempts by Russian and Chinese organizations to evade export controls are not the only reason that BIS\u2019s job has become significantly harder in recent years. Legislation and executive orders have posed new challenges to BIS in administering and enforcing export controls in at least three ways. Party and State Bureaucracies\nIdeas and proposals are molded into regulations in the final layer of the policy funnel, consisting of the party and state bureaucracies. When it comes to setting AI regulation, organizations across the party and state bureaucracies are deeply interwoven. But that prox-imity should not be mistaken for harmonious relations. China\u2019s ministries and agencies are a notoriously \u201cfractious and highly competitive group,\u201d always angling for their policies to be adopted at higher levels. Examining the regulations issued so far illuminates some initial conclusions about which members of this \u201cfractious\u201d group are prevailing in the competition for influence. and analyses of their impact just days after their release. This project aims to continue moving the focus of analysis up the supply chain by seeking out the early signals of what policies are likely to come. It identifies actors from across Chinese academia, media, policy think tanks, corporations, and the party and state bureaucracies that signal and shape forthcoming AI governance. Ultimately, this approach aims to both deeply understand China\u2019s existing AI regulations and to help predict what new measures may be coming around the bend.\n\u201cAI\u201d and \u201cgovernance\u201d are slippery concepts. Attempting to dissect all government policies that impact this basket of technologies would further muddy China\u2019s already-murky policymaking process. This paper thus focuses on a specific subset of Chinese measures: national-level policy documents that explicitly and primarily target AI or algorithms for regulation or governance."}
{"input":"Other than misuse, what is another challenge of AI?","output":"Missed uses - failing to take advantage of and share the benefits of AI technologies out of an excess of caution.","context":"Policy implications of artificial intelligence (AI)\n27                                  POSTnote 708 - 9 January 2024\nContributors\nPOST is grateful to Ansh Bhatnagar for researching this briefing, to STFC for funding their \nparliamentary fellowship, and to all contributors and reviewers. For further information on this \nsubject, please contact the co-author, Devyani Gajjar.\nMembers of the POST Board*\nDr Elena Abrusci, Brunel University London* Dr Mhairi Aitken, Alan Turing Institute* Emmanuelle \nAndrews, Liberty\nDr Hayleigh Bosher, Brunel University London* Matt Davies, Ada Lovelace Institute\nMaximilian Gahntz, Mozilla Foundation Conor Griffin, Google DeepMind\nProfessor Oliver Hauser, University of Exeter Harry Law, Google*\nMia Leslie, Public Law Project*\nMavis Machirori, Ada Lovelace Institute Professor Gina Neff, University of Cambridge\nSam Nutt, London Office of Technology and Innovation Lucy Purdon, Mozilla Foundation*\nAdam Smith, British Computer Society* limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is adopted, deployed, and used. Defining AI: There will be no legal definition of AI. Instead, \u201cAI\u201d is defined by reference to the combination of two characteristics: (1) adaptivity\u2014i.e., being \u201ctrained\u201d and operating by inferring patterns and connections in data which are not easily discernible to humans; and (2) autonomy\u2014i.e., making decisions without the express intent or ongoing control of a human. Defining AI with reference to functional capabilities is intended to future-proof the framework against unanticipated new technologies that are autonomous and adaptive.\nContext-specific \u2013 Regulating use, not technology: The framework will regulate outcomes that AI is likely to generate. This approach could even extend to failure to use AI; the government highlighted feedback received from regulators that failure to exploit AI capabilities may in fact risk harm, such as not utilizing AI in safety-critical processes. \u2022\n\nCOMPLIANCE AND RISK MANAGEMENT. AI systems are used to crossreference and reconcile terabytes of data from multiple sources to create alerts for noncompliance. For example, tax authorities can use AI to\ntrack tax filers who use duplicate profiles to avoid taxation, and social safety program may use AI to detect whether beneficiaries comply with eligibility criteria.\n\n\u2022\n\nFRAUD DETECTION, PREVENTION, AND INVESTIGATION.\n\n9\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR"}
{"input":"How should AI actors ensure robustness, security, and safety?","output":"Ensure traceability, employ risk management approach to each phase of the AI system lifecycle","context":"Safety, security, and robustness: AI systems should function as intended and in a robust, secure, and safe way throughout the AI lifecycle, and risks should be continually identified, assessed, and managed. Safety-related risks will be sector specific, and regulators should take a proportionate approach to manage them. Regulators may require the corresponding AI lifecycle actors to regularly test or carry out due diligence on the functioning, resilience, and security of a system. Artificial Intelligence Safety and Stability\nNations around the world are investing in artificial intelligence (AI) to improve their military, intelligence, and other national security capabilities. Yet AI technology, at present, has significant safety and security vulnerabilities. AI systems could fail, potentially in unexpected ways, due to a variety of causes. Moreover, the interactive nature of military competition means that one nation\u2019s actions affect others, including in ways that may be detrimental to mutual stability. There is an urgent need to explore actions that can mitigate these risks, such as improved processes for AI assurance, norms and best practices for responsible AI adoption, and confidence-building measures that improve stability among all nations. Governments should focus on strengthening resilience to AI threats What should government policymakers do to protect humanity?\nThe potential dangers posed by AI are many. At the extreme, they include the threat of human extinction, which could come about by an AI-enabled catastrophe, such as a well-designed virus that spreads easily, evades detection, and destroys our civilization. Less dire, but considerably worrisome, is the threat to democratic governance if AIs gain power over people.[2] Even if AIs do not kill humans or overturn democracy, authoritarian regimes, terrorist groups, and organized crime groups could use AI to cause great harm by spreading disinformation and manipulating public opinion. Governments need to view the AI landscape as a regulatory training ground in preparation for the threats posed by even more-advanced AI capabilities, including the potential arrival of artificial general intelligence."}
{"input":"What were some strengths and weaknesses of Finland?","output":"- Strength: advanced digital economies\n - Weakness: lacks economies of scale, internationally connected companies and foreign direct investment, and exhibits slow commercialization.","context":"country\u2019s entire population \u2013 including people who are employed and the\nelderly \u2013 in basic AI, which it sees as a \u201ccivic competence\u201d. While Finland initially\ntargeted the training of 1% of its population, the course attracted more than\n100 000 participants. This represents more than 2% of the population.\nRead more: https:\/\/oecd.ai\/dashboards\/countries\/Finland. Miner, A.S., Laranjo, L. and A. Baki Kocaballi. 2020. \u201cChatbots in the fight against the COVID-19 pandemic.\u201d\nnpj Digital Medicine (Nature), 3 (65). https:\/\/doi.org\/10.1038\/s41746-020-0280-0\nMinistry of Economic Affairs and Employment. 2017. Finland\u2019s Age of Artificial Intelligence: Turning Finland into a Leading Country in the\nApplication of Artificial Intelligence.\nhttps:\/\/julkaisut.valtioneuvosto.fi\/bitstream\/handle\/10024\/160391\/TEMrap_47_2017_verkkojulkaisu.pdf\nMinistry of Economic Affairs and Employment. 2019. Leading the Way into the Era of Artificial Intelligence.\nFinal Report of Finland\u2019s Artificial Intelligence Programme 2019.\nhttp:\/\/julkaisut.valtioneuvosto.fi\/handle\/10024\/161688\nMinistry of Finance. 2018a. \u201cDigital Economy and Society Index: Finland has EU\u2019s Best Digital Public Services.\u201d News Item, May 23.\nhttps:\/\/vm.fi\/en\/article\/-\/asset_publisher\/digitaalitalouden-ja-yhteiskunnan-indeksi-suomessa-eu-n-parhaat-julkiset-digitaaliset-palvelut 19 https:\/\/julkaisut.valtioneuvosto.fi\/bitstream\/handle\/10024\/160391\/TEMrap_47_2017_verkkojulkaisu.pdf\n20 https:\/\/julkaisut.valtioneuvosto.fi\/handle\/10024\/161688\n\nworldbank.org\/digitaldevelopment\n15\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nFIGURE 1: Finland\u2019s National AI Strategy: 11 Key Actions\n\n1. INCREASING COMPETITIVENESS OF FINNISH COMPANIES THROUGH AI\n2. INCREASING USE OF DATA AND DATA AVAILABILITY\n3. SPEEDING UP THE DEPLOYMENT OF AI\n4. ATTRACTING AI TALENT AND EXPERTS\n5. INCREASING INVESTMENTS IN AI\n6. AUGMENTING PUBLIC SERVICES WITH AI\n7. CREATING NEW TYPES OF COLLABORATION\n8. RAISING FINLAND\u2019S PROFILE AND BECOMING A LEADER IN AI APPLICATIONS\n9. PREPARING FOR THE FUTURE OF WORK\n10. CREATING TRUST-BASED AND HUMAN-CENTERED AI, BEING A LEADER IN AI ETHICS\n11. ADDRESSING CYBERSECURITY AND SAFETY CHALLENGES OF AI TECHNOLOGIES view of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries.\n\nFinland\u2019s National AI Strategy\nFinland was among the first European countries\nto launch an AI strategy in October 2017,19 and its\nlatest AI strategy document was published in June\n20\n2019. Finland has one of the most advanced digital\neconomies in Europe and is well positioned to reap\nthe benefits of AI technologies (Foley and others\n2020). Their 2019 AI strategy positions Finland\nas a piloting environment spearheaded by agile,\ninnovation-friendly public administration and\nenabling legislation. However, it recognizes that the\ncountry lacks economies of scale, internationally\nconnected companies and foreign direct investment,\nand exhibits slow commercialization. Finland\u2019s\n2019 AI strategy report identifies eleven areas of\naction, as outlined in Figure 1."}
{"input":"What is the importance of the Affordable Connectivity Program?","output":"The program addresses 'digital exclusion,' impacting government service costs, literacy rates, and economic growth. It also affects access to AI technology in education and healthcare, which could widen societal gaps.","context":"Procuring technology is only part of the solution. Congress needs to appropriate funds on an annual basis to ensure that BIS staff are able to leverage the technology. Furthermore, funding extra staff will help close ongoing gaps in enforcement, particularly as enforcement needs grow in number and complexity. In other words, simply acquiring new technologies is not a panacea on its own; successful use of new technologies depends on the agency\u2019s ability to use them effectively. An appropriation of $25 million annually for the next five years will help significantly enhance immediate enforcement capabilities and also improve the speed and accuracy of export licensing. This figure takes into account BIS\u2019s opportunities to improve collaboration with other U.S. government agencies and the need to prevent unnecessary duplication of effort. This funding will go toward four primary areas: (1) procuring access to large proprietary and open-source data sets, (2) integrating those data sets into a modern data analytics platform, (3) adding additional analyst staff with needed specialist skills to use new technology (e.g., data scientists), and (4) covering staff training costs. Specifically, this funding should support the following mission objectives: worldbank.org\/digitaldevelopment\n44\n\n\fANALYTICAL INSIGHTS - NOTE 4 worldbank.org\/digitaldevelopment\n45\n\n\fANALYTICAL INSIGHTS - NOTE 4"}
{"input":"How does the Executive Order specifically address fake AI generated content?","output":"The Order involves commitments from AI firms like Anthropic, Google, OpenAI to label AI-generated content and directs the Department of Commerce to develop labeling guidance for such content.","context":"The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated. and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies with all Federal laws and to promote robust technical evaluations, careful oversight, engagement with affected communities, and rigorous regulation.  It is necessary to hold those developing and deploying AI accountable to standards that protect against unlawful discrimination and abuse, including in the justice system and the Federal Government.  Only then can Americans trust AI to advance civil rights, civil liberties, equity, and justice for all."}
{"input":"What are some examples of current applications of on-chip mechanisms?","output":"On-chip mechanisms in iPhones prevent unauthorized app installation. Google uses them to verify data center chip integrity.","context":"Improved technology would go a long way toward strengthening BIS\u2019s ability to manage its key administrative lists, rapidly assess license applications, and effectively enforce export controls.\n\nBIS enforcement agents engage in open-source intelligence analysis via mainstream search engines and use Excel spreadsheets to comb through data. As one interviewee noted, enforcement officers spend 80 percent of their time looking for data and 20 percent of their time analyzing that data. Creating a system yielding the opposite results\u201480 percent analysis and 20 percent data gathering\u2014would be far superior. In other words, the use of new technology at BIS would be a major force multiplier. A few noteworthy examples of how modern technology could improve BIS efficacy and efficiency are listed below. These are only a fraction of the total potential. Procuring technology is only part of the solution. Congress needs to appropriate funds on an annual basis to ensure that BIS staff are able to leverage the technology. Furthermore, funding extra staff will help close ongoing gaps in enforcement, particularly as enforcement needs grow in number and complexity. In other words, simply acquiring new technologies is not a panacea on its own; successful use of new technologies depends on the agency\u2019s ability to use them effectively. Lannquist, Ashley. 2017. \u201cBlockchains, Cryptocurrencies and the New Decentralized Economy: Part 2 \u2013 Blockchain-Based Apps.\u201d\nMedium: Blockchain at Berkeley, June 30.\nhttps:\/\/medium.com\/blockchain-at-berkeley\/blockchains-cryptocurrencies-the-new-decentralized-economy-part-2-blockchain-based-apps-e6ea71236ca\nLannquist, Yolanda and others. 2020. \u201cThe Intersection and Governance of Artificial Intelligence and Cybersecurity.\u201d\nThe Future Society. https:\/\/www.researchgate.net\/publication\/341412701_The_Intersection_and_Governance_of_Artificial\nLee, Kai-Fu. 2018. AI Superpowers: China, Silicon Valley and the New World Order. Boston: Houghton Mifflin Harcourt.\nLin, H. 2016. Governance of Information Technology and Cyber Weapons in Governance of Dual-Use Technologies: Theory and Practice.\nCambridge, MA: American Academy of Arts & Sciences.\nLindauer, D. and L. Pritchett. 2002. \u201cWhat\u2019s the Big Idea? The Third Generation of Policies for Economic Growth.\u201d\nEconomia 3 (1): 1-36, Fall 2002. Additional funds appropriated by Congress will allow BIS to leverage existing and externally available technology to support its licensing and enforcement needs. In addition to acquiring the rote data, a core cost will be for BIS to work with external technology providers and contractors to ensure that the data is searchable and readable, enabling BIS staff to maximize the benefits of the technology. Experts building comparable systems for the intelligence community and other federal agencies with similar missions to build, maintain, and staff this technology stack would cost roughly $25 million annually."}
{"input":"How does the EU AI Act categorize AI systems based on risk?","output":"The EU AI Act employs a risk-based approach, categorizing AI systems into four levels of risk: unacceptable, high, limited, and minimal\/none, with specific regulations focusing on unacceptable and high-risk categories.","context":"Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition. The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct."}
{"input":"What is the role of the private sector and local governments in China's AI strategy?","output":"The private sector and local governments are pivotal to China's AI innovation and implementation. The strategy includes designating certain companies as AI national champions for specific AI sectors and incentivizing local AI projects, indicating a collaborative approach between the government, private sector, and local authorities.","context":"AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the perceived risks present in each of these areas. The AIDP highlights three areas where AI can make a substantial difference within China: international competition, economic development, and social governance. They are strictly interrelated but, for the sake of clarity, we shall analyse them separately, and contextualise each of them by discussing the relevant literature surrounding the broader political backdrop and contemporary policy debates. A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. Nevertheless, in the area of AI regulation there is still a relatively large space for policy debates. This is perhaps due to the relatively technical nature of policies and to the freshness of the problems. How to effectively regulate AI remains a wide-open question globally, and the political interests at play in China are not yet entrenched. Ministries and state-owned enterprises have not spent decades fighting to gain leverage or to hang onto preferential policies they have carved out. This mix of factors has made public debates over AI governance unusually lively and open. The Policy Funnel of Chinese AI Governance\nClick on a section of the diagram above to learn more about the key actors and influences on major AI governance initiatives in China.\n\nA growing economy and healthy labor markets are core to Chinese social stability, making them major, though indirect, factors in AI regulation. Economic and labor conditions that gave rise to the \u201cCommon Prosperity\u201d agenda and anti-monopoly actions have likely contributed to provisions on recommendation algorithms covering antitrust and worker rights."}
{"input":" How does the document suggest improving the regulatory framework for AI?","output":" Recommendations include rethinking elements of the Data Protection and Digital Information Bill, reviewing existing legal protections, and potentially establishing new rights and protections for individuals affected by AI.","context":"This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. Governments should look beyond traditional regulatory techniques to influence AI developments \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes These extremely demanding requirements for generative AI systems have kicked off a particularly active public debate on the draft regulation. At the time of writing, Chinese scholars, companies, and policymakers are actively discussing how to maintain effective content controls without squashing China\u2019s nascent generative AI industry. The third paper in this series will dive deep into how this policy debate is playing out in public workshops, academic writing, and corporate lobbying.\nCountries and cultures may differ on the specific content of AI regulations, but they can learn from the content-agnostic structure of the regulations themselves. The above Chinese regulations share three structural similarities: the choice of algorithms as a point of entry; the building of regulatory tools and bureaucratic know-how; and the vertical and iterative approach that is laying the groundwork for a capstone AI law."}
{"input":" What are the main concerns regarding the use of generative AI tools like ChatGPT in academic settings?","output":" The main concerns include the potential for cheating or plagiarism, the decline in students' writing and critical thinking skills, and the broader impact on academic integrity and the quality of education.","context":"Measures for the Management of Generative Artificial Intel igence Services At first glance, China\u2019s regulatory apparatus appeared well prepared for the wave of generative AI applications that would follow ChatGPT. The deep synthesis regulation technically included most forms of generative AI, such as using the technology to create or edit images, videos, voice, and text. Term                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI\nregulation\u2019 defined AI, AI systems or AI technologies as \u201cproducts and services that are \n\u2018adaptable\u2019 and \u2018autonomous\u2019.\u201d The adaptability of AI refers to AI systems, after being trained, \noften developing the ability to perform new ways of finding patterns and\nconnections in data that are not directly envisioned by their human\nprogrammers. The autonomy of AI refers to some AI systems that can make decisions without the \nintent or ongoing control of a human (PB 57).\nGenerative AI               The Alan Turing Institute defines generative AI as an \u201cartificial\nintelligence system that generates text, images, audio, video or other media in response to user \nprompts.\u201d26 Generative AI applications include chatbots, such as OpenAI\u2019s ChatGPT, photo and video\nfilters, and virtual assistants (PB 57). Risk of Extinction. At the same time, AI academic computer scientists like Yoshua Bengio and Geoffrey Hinton, who are the Turing prize-winning \u201cgodfathers\u201d of generative AI, warn of dramatic risks that threaten humanity\u2019s very existence. Bengio, Hinton, Bill Gates, top executives from Google, Microsoft, OpenAI and many AI luminaries including from China and Russia have signed a statement stating \u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\u201d There are also proposals to pause developing more powerful AI models than ChatGPT 4. Elon Musk has said, \u201cWith AI we\u2019re summoning the demon.\u201d The precautionary principle suggests low-probability, high-impact outcomes should be taken seriously. prompts.\u201d26 Generative AI applications include chatbots, such as OpenAI\u2019s ChatGPT, photo and video\nfilters, and virtual assistants (PB 57).\nAutomated decision-making        A term that the Office for AI* uses to\nrefer to \u201cboth solely automated decisions (no human judgement\ninvolved) and automated assisted decision-making (assisting human judgement).\u201d27\nTraining datasets             The set of data used to train an AI system, which often requires \nlabelling (such as captioning pictures) to explain what the data means.\nAlgorithm                 A set of instructions used to perform tasks (such as calculations and \ndata analysis) usually using a computer or another smart device (PB 57).\n* The Office for AI is an office within the Department for Science, Innovation, and Technology.\n3                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nBenefits and risks\nJob creation and displacement"}
{"input":" How does the National Institute of Standards and Technology (NIST) AI Risk Management Framework propose to guide the responsible development and use of AI systems?","output":" The NIST AI Risk Management Framework, formally released on January 26, 2023, is a voluntary framework that includes four overarching functions: Govern (policy decisions and organizational culture), Map (contextualizing AI risks and benefits), Measure (assessing and quantifying AI risks), and Manage (mitigating risks and prioritizing trustworthy AI elements). It provides recommendations and a community Playbook to help organizations navigate these aspects for more responsible AI usage.","context":"(A)  AI nomenclature and terminology;\n\n               (B)  best practices regarding data capture, processing, protection, privacy, confidentiality, handling, and analysis;\n\n               (C)  trustworthiness, verification, and assurance of AI systems; and\n\n               (D)  AI risk management;\n\n          (ii)   within 180 days of the date the plan is established, submit a report to the President on priority actions taken pursuant to the plan; and\n\n          (iii)  ensure that such efforts are guided by principles set out in the NIST AI Risk Management Framework and United States Government National Standards Strategy for Critical and Emerging Technology.\n\n     (c)  Within 365 days of the date of this order, to promote safe, responsible, and rights-affirming development and deployment of AI abroad: 4.1.  Developing Guidelines, Standards, and Best Practices for AI Safety and Security.  (a)  Within 270 days of the date of this order, to help ensure the development of safe, secure, and trustworthy AI systems, the Secretary of Commerce, acting through the Director of the National Institute of Standards and Technology (NIST), in coordination with the Secretary of Energy, the Secretary of Homeland Security, and the heads of other relevant agencies as the Secretary of Commerce may deem appropriate, shall:\n\n          (i)   Establish guidelines and best practices, with the aim of promoting consensus industry standards, for developing and deploying safe, secure, and trustworthy AI systems, including:\n\n               (A)  developing a companion resource to the AI Risk Management Framework, NIST AI 100-1, for generative AI; National Institute of Standards and Technology AI Risk Management Framework\nFormally released on January 26, 2023, the A.I. Risk Management Framework is a four-part, voluntary framework intended to guide the responsible development and use of A.I. systems. The core of the framework are recommendations divided into four overarching functions: (1) Govern, which covers overarching policy decisions and organizational culture around A.I. development; (2) Map, which covers efforts to contextualize A.I. risks and potential benefits; (3) Measure, which covers efforts to assess and quantify A.I. risks; and (4) Manage, which covers the active steps an organization should take to mitigate risks and prioritize elements of trustworthy A.I. systems. In addition to the core Framework, NIST also hosts supplemental resources like a community Playbook to help organizations navigate the Framework. Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional."}
{"input":" What principles underlie the OECD Recommendation on Artificial Intelligence adopted by the United States?","output":" The OECD Recommendation on AI, adopted by the United States and other democracies, promotes inclusive growth, human-centered values, transparency, safety and security, and accountability. It encourages national policies and international cooperation to invest in AI research and development.","context":"is a relatively new phenomenon. To track these initiatives, the OECD AI Policy\nObservatory (OECD.AI) comprises over 620 national AI policies from over 60\ncountries and the European Union (EU). These resources provide a baseline to\nmap countries\u2019 AI policy initiatives according to the recommendations to\ngovernments contained in the OECD AI Principles (Box 1).\nBox 1. OECD AI Principles\nThe OECD Principles on Artificial Intelligence promote AI that is innovative,\ntrustworthy and respects human rights and democratic values. They were adopted\nin May 2019 by OECD member countries when they approved the OECD Council\nRecommendation on Artificial Intelligence. The OECD AI Principles identify five\ncomplementary values-based principles for the responsible stewardship of\ntrustworthy AI:\n\u2022 \u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI.\n\nTo develop practical guidance to implement the AI Principles as mandated by the\nOECD Council, the OECD convened a multi-stakeholder and multi-disciplinary OECD\nNetwork of Experts on AI in early 2020 that is developing a report on the State of\nimplementation of the OECD AI Principles: Insights from national AI policies (OECD,\n2021[1]). The report provides good practices and lessons learned on the\nimplementation of the five recommendations to policy makers contained in the\nOECD AI Principles.\nSource: oecd.ai\/ai-principles and oecd.ai\/network-of-experts. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13 International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules."}
{"input":" What are the key principles of the new AI policy statement issued by DHS?","output":" The policy statement outlines principles for DHS's AI use, including conformity with Executive Order 13960, adherence to the Constitution, applicable laws, policies, and the avoidance of decisions based on inappropriate considerations like race, gender, or disability.","context":"Policy Statement 139-06 \u201cAcquisition and Use of Artificial Intelligence and Machine Learning by DHS Components:\u201d The policy statement establishes the foundation for DHS\u2019s use of AI with a clear set of principles. These principles include that DHS systems, programs, and activities using AI will conform to the requirements of Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in\u202fthe Federal Government. In addition, DHS will only acquire and use AI in a manner that is consistent with the Constitution and all other applicable laws and policies. Also, DHS will not collect, use, or disseminate data used in AI activities, or establish AI-enabled\u202fsystems that make or support decisions, based on the inappropriate consideration of race, ethnicity, gender, national origin, religion, gender, sexual orientation, gender identity, age, nationality, medical condition, or disability. is a relatively new phenomenon. To track these initiatives, the OECD AI Policy\nObservatory (OECD.AI) comprises over 620 national AI policies from over 60\ncountries and the European Union (EU). These resources provide a baseline to\nmap countries\u2019 AI policy initiatives according to the recommendations to\ngovernments contained in the OECD AI Principles (Box 1).\nBox 1. OECD AI Principles\nThe OECD Principles on Artificial Intelligence promote AI that is innovative,\ntrustworthy and respects human rights and democratic values. They were adopted\nin May 2019 by OECD member countries when they approved the OECD Council\nRecommendation on Artificial Intelligence. The OECD AI Principles identify five\ncomplementary values-based principles for the responsible stewardship of\ntrustworthy AI:\n\u2022 Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. \u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI.\n\nTo develop practical guidance to implement the AI Principles as mandated by the\nOECD Council, the OECD convened a multi-stakeholder and multi-disciplinary OECD\nNetwork of Experts on AI in early 2020 that is developing a report on the State of\nimplementation of the OECD AI Principles: Insights from national AI policies (OECD,\n2021[1]). The report provides good practices and lessons learned on the\nimplementation of the five recommendations to policy makers contained in the\nOECD AI Principles.\nSource: oecd.ai\/ai-principles and oecd.ai\/network-of-experts."}
{"input":" What role do algorithmic audits play in AI regulation and what are their potential impacts?","output":" Algorithmic audits are evaluations of AI systems that can reveal inaccuracies, discrimination, and other flaws. They are a critical tool for regulators to assess compliance with laws and regulations, offering a direct way to analyze and identify harmful aspects of AI systems without relying on the claims of developers.","context":"scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms. Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm. \u2022  safeguards and mechanisms to ensure compliance with regulations\nAlgorithmic impact assessments exist in the Canadian public sector and require government agencies \nto complete a questionnaire before deploying automated decision-making.269\nThere is a voluntary Algorithmic Transparency Recording Standard that UK public bodies can use to \ndisclose information about their use of AI.270\nThe Institute for the Future of Work has stated that audits of AI systems could ensure that these \nsystems are compliant with their impact assessments and the law, and that the Digital Regulation \nCooperation Forum could play a role in helping regulators cooperate on these audits.264\nThe Information Commissioners Office currently carries out consensual and compulsory data \nprotection audits* of AI systems271. However, issues relating to AI are wider than data protection \nand fall under the remit of not just the Information Commissioners Office but multiple regulators far more likely (https:\/\/www.brookings.edu\/articles\/auditing-employmentalgorithms-\nfor-discrimination\/)\nto uncover flawed and harmful aspects of analgorithmic system. However, more intensive algorithmic audits are also far moretechnically complex, requiring more expertise and technical capacity from regulators.Specifically, regulators would need data scientists with expertise in evaluatingalgorithmic systems and may need to take necessary steps to develop a computingenvironment for algorithmic evaluation with appropriate privacy and cybersecuritysafeguards.\nscientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are"}
{"input":" What are the ten key parameters driving successful AI regulatory design according to the document?","output":" The document outlines ten key parameters for successful AI regulatory design, including transparency, fairness, explainability, security, trust, a risk-based approach, mitigating risk, innovation, data flows, and international harmonization.","context":"II. Top Ten Parameters Driving Regulatory Design and Outcomes   \n\nTo help policy makers and regulators, we define 10 key parameters that must be addressed in some fashion. This does not propose solutions for each domain or issue.   \n\nTransparency, fairness, explainability, security and trust are over-arching goals. Unless these goals can be achieved, AI regulation is unlikely to succeed, and AI adoption will be stunted. Many specific issues, e.g., informing users of risks, disclosing model characteristics, model-bias, and independent model safety testing, support these broader goals. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. The MOST played a large role in early policies like the 2017 AI plan and followed that up by establishing committees and issuing high-level principles for AI ethics and governance. It also wrote the draft version of a broader technology ethics and governance measure that was\nlater issued by the CCP Central Committee. But the MOST has taken a back seat on the more targeted regulations, not co-signing the recommendation algorithm or deep synthesis regulations. The ministry focuses primarily on issues related to research and development, making it less suited to regulating online content or certain commercial applications of AI.\n\nBut the MOST\u2019s profile may rise again as regulatory attention turns toward the underlying technology, as in the draft generative AI regulation, which imposes requirements on model training."}
{"input":" How does the U.S. approach to AI governance differ from the EU's focus on privacy regulation?","output":" The U.S. position on AI stands in stark contrast to the lack of strong U.S. leadership on privacy regulation, where the absence of federal privacy legislation created a vacuum that the EU\u2019s General Data Protection Regulation (GDPR) filled, allowing GDPR to become a leading model for privacy regulation worldwide.","context":"The EOAI along with the other domestic AI policies outlined will have significant international impacts through various channels.\n\nFirst and most immediately, these suites of domestic policy developments on AI governance give Vice President Kamala Harris as head of the U.S. delegation to the U.K. AI Safety Summit a real opportunity to lead the further development of international AI governance. The U.S. position on AI stands in stark contrast to the lack of strong U.S. leadership on privacy regulation, where the absence of federal privacy legislation created a vacuum that the EU\u2019s General Data Protection Regulation (GDPR) filled, allowing GDPR to become a leading model for privacy regulation worldwide. Data flows, data privacy, data security. AI crucially depends on large volumes of high-quality data. Model accuracy and outcomes directly reflect the data it was trained on. Hence, data regulation is central to AI regulation. AI likely magnifies data privacy concerns. The United States has no national data privacy law; the EU builds on GDPR. Data security and cross-border data flows are also connected to AI regulation. These are not new issues, and AI regulation should build on prior efforts. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA). This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents."}
{"input":" What is the role of the Digital Platform Transparency Act in Japan's AI regulatory landscape?","output":" The Digital Platform Transparency Act imposes requirements on large online malls, app stores, and digital advertising businesses to ensure transparency and fairness in transactions with business users, including the disclosure of key factors determining their search rankings.","context":"Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA). an individual right to \u201cmeaningful information aboutthe logic\u201d of algorithmic systems. This has led companies, such as\n, to offer responses\u2014albeit limited ones\u2014to requests for information aboutalgorithmic decisions. Although not yet passed into law, the forthcoming EU AI Act willalso create\nsubstantial new transparency requirements\n(https:\/\/www.brookings.edu\/articles\/the-eu-and-us-diverge-on-ai-regulation-atransatlantic-\ncomparison-and-steps-to-alignment\/#anchor5)\n, likely including directdisclosure of chatbots and public reporting about high-risk AI systems.\nTransparency requirements require little expertise and capacity from governmentagencies, making them an appealing early step in AI regulation. However, regulators doneed to be careful in specifying transparency requirements\u2014vaguely or poorly wordedrequirements can permit too much flexibility in algorithmic transparency, allowing forcompanies to cherry pick self-serving statistics. Governments around the world are implementing foundational policies to regulateartificial intelligence (AI) and algorithmic systems more generally. While legislation isadvancing, regulators should not wait idly for legislators to act. Instead, regulatorsshould be actively learning about the algorithmic systems in their regulatory domainand evaluating those systems for compliance under existing statutory authority.\nMany regulatory agencies have started this work, including the U.S. Federal TradeCommission\u2019s (FTC)\nand Consumer Financial Protection Bureau(CFPB), new algorithmic regulators in\nand\n, and onlineplatform regulators such as the UK\u2019s\n(OFCOM) and the\n. These agencies and others havestarted to implement novel approaches and policies for AI regulation.\nOffice of Technology\nthe Netherlands\nSpain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023 \u2022  safeguards and mechanisms to ensure compliance with regulations\nAlgorithmic impact assessments exist in the Canadian public sector and require government agencies \nto complete a questionnaire before deploying automated decision-making.269\nThere is a voluntary Algorithmic Transparency Recording Standard that UK public bodies can use to \ndisclose information about their use of AI.270\nThe Institute for the Future of Work has stated that audits of AI systems could ensure that these \nsystems are compliant with their impact assessments and the law, and that the Digital Regulation \nCooperation Forum could play a role in helping regulators cooperate on these audits.264\nThe Information Commissioners Office currently carries out consensual and compulsory data \nprotection audits* of AI systems271. However, issues relating to AI are wider than data protection \nand fall under the remit of not just the Information Commissioners Office but multiple regulators"}
{"input":" What role does the private sector play in the Blueprint's strategy for AI governance?","output":" The Blueprint looks to the private sector for self-regulatory management of AI, emphasizing a consumer rights-based approach to product and service governance.","context":"The Role of the Private Sector\nThe intertwined relationship between AI\ndevelopment and the digital economy makes the\n\nworldbank.org\/digitaldevelopment\n7\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nwell-developed innovation ecosystems can drive\ntechnology diffusion in new areas, with a growing\nnumber of start-ups developing AI applications for\nnew and different contexts. One key way the private\nsector can help shape national AI strategies is\nthrough providing comments during consultation\nperiods, where feedback on strategy proposals is\nsolicited by government planning bodies, such as\nin Brazil. 3 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. 4\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fHow can countries address these challenges?\nAI policy incorporating legislative foundations, institutional arrangements and a\ngovernance framework for the use of AI could provide the necessary enabling\nenvironment. This could be part of a broader Digital Transformation Policy or a\nstand-alone framework.\nA central innovation hub for AI in government could help pool scarce human\nand financial resources to support the initiatives of line ministries and start to\ndevelop a whole-of-government approach. In the use cases examined here,\nmost governments have already set-up the main hub for AI issues that serves\nas a central authority over AI projects being delivered by line agencies.\nThe AI hub\u2019s role could also be to connect industry expertise to the government\ndepartments and line agencies, to promote research, and build alliances with\nacademic institutions and the private sector."}
{"input":" How does the World Bank respect and protect personal information?","output":" The Bank\u2019s Principles of Staff Employment require the Bank to establish and maintain appropriate safeguards to respect the personal privacy of staff members and protect the confidentiality of personal information about them. Accordingly, the Bank does not provide access to the following information, except to the extent expressly permitted by the Staff Rules. (a) Personal information, including personal staff records, medical information, and personal communications (including e-mail) of the following individuals and their families: Executive Directors, their Alternates, and their Senior Advisers; the President of the Bank; other Bank officials; and Bank staff. (b) Information relating to staff appointment and selection processes. (c) Information relating to proceedings of the Bank\u2019s internal conflict resolution mechanisms. (d) Information relating to investigations of allegations of staff misconduct and personal conflicts of interest.","context":"Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction World Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\nI. Introduction\n1. The World Bank1 recognizes that transparency and accountability are of\nfundamental importance to the development process and to achieving its mission to end\nextreme poverty and promote shared prosperity. Transparency is essential to building\nand maintaining public dialogue and increasing public awareness about the Bank\u2019s\ndevelopment role and mission. It is also critical for enhancing good governance,\naccountability, and development effectiveness. Openness promotes engagement with\nstakeholders, which, in turn, improves the design and implementation of projects and public only after the country (or other third-party) authority concerned has\ngranted its written consent to disclose.\n21. Documents Prepared Jointly with Partners. The Bank makes available to the\npublic the final documents that it prepares jointly with partners after reaching agreement\non such disclosure with the partner concerned.20\nIV. Implementation Aspects of the Policy\nA. Accessing Information\n22. Proactive Disclosure. The Bank routinely discloses a wide range of documents,\nthrough its external website \u2013 www.worldbank.org \u2013 as soon as the documents are\nfinalized after key process milestones. Under this Policy, the Bank significantly\nincreases the amount of information it makes available to the public, particularly\ninformation related to projects under implementation and to the actions of the Board of\nExecutive Directors.\nunder IBRD Loans and IDA Credits, and Guidelines: Selection and Employment of Consultants by\nWorld Bank Borrowers). 31.\nThe Bank declassifies and discloses\u2014routinely on the Bank\u2019s external website,\nand in response to requests\u2014certain types of restricted information (including\ninformation prepared under earlier disclosure policies of the Bank) as their sensitivity\ndiminishes over time. Some restricted information is not eligible for declassification.\n32.\nInformation Not Eligible for Declassification. The information that is restricted\nfrom disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member"}
{"input":" What is the availability policy on Board papers and Board records?","output":" Board papers and Board records that are routinely available from the Bank are posted on the Bank's website at specific Board milestones. Some Board discussions may deal with issues that fall under the exceptions of the AI Policy. In such cases, the related Board records are classified as Official Use Only, Confidential or Strictly Confidential and are not disclosed unless they become eligible for declassification.","context":"The Bank routinely discloses a wide range of documents through its website as soon as key process milestones have\nbeen passed and the documents have been finalized.\n\n                                                                                                                    Go Top\n\fWhen do Board papers and Board records become available?\n\nBoard papers and Board records that are routinely available from the Bank are posted on the Bank's website at specific\nBoard milestones. Some Board discussions may deal with issues that fall under the exceptions of the AI Policy. In such\ncases, the related Board records are classified as \"Official Use Only\", \"Confidential\" or \"Strictly Confidential\" and are\nnot disclosed unless they become eligible for declassification. 9\n\n\f23.\nBoard Proceedings. Board papers and Board records that are routinely available\nfrom the Bank are posted on the Bank\u2019s external website at specific Board milestones.\nSome Board discussions may deal with issues that fall under the exceptions of the policy.\nIn such cases, the related Board records are classified as \u201cConfidential\u201d and \u201cStrictly\nConfidential\u201d and are not disclosed unless they become eligible for declassification under\nthe declassification schedule provided under paragraph 33 of this policy statement.\n(a) The following Board records prepared on or after July 1, 2010, are posted at the\nend of the Board\u2019s deliberative process:\n(i)\n(ii)\n(iii)\n(iv)\n(v)\n\n(vi) exercised only by:\n(a) The Board, with respect to Board records. In the case of verbatim transcripts\nand statements of individual Executive Directors and staff, prepared on or\nafter July 1, 2013, the Bank\u2019s prerogative to restrict access to such Board\nagreements. Moreover, the Bank makes publicly available the borrowers\u2019 audited annual financial\nstatements (or, in exceptional cases, an abridged version thereof) for investment project financings, for\nwhich the invitation to negotiate is issued on or after July 1, 2010.\n17 For the purposes of this policy statement, \u201cBoard papers\u201d refers to documents that are specifically\nprepared by staff for (a) discussion or consideration (decision) by the Board, (b) discussion by a Board\nCommittee, or (c) information of the Board or a Board Committee. For the purposes of this policy\nstatement, \u201cBoard records\u201d refers to records of Board or Board Committee discussions such as agreements. Moreover, the Bank makes publicly available the borrowers\u2019 audited annual financial\nstatements (or, in exceptional cases, an abridged version thereof) for investment project financings, for\nwhich the invitation to negotiate is issued on or after July 1, 2010.\nFor the purposes of this policy statement, \u201cBoard papers\u201d refers to documents that are specifically\nprepared by staff for (a) discussion or consideration (decision) by the Board, (b) discussion by a Board\nCommittee, or (c) information of the Board or a Board Committee. For the purposes of this policy\nstatement, \u201cBoard records\u201d refers to records of Board or Board Committee discussions such as\nverbatim transcripts, minutes, statements of Executive Directors, Chairman\u2019s Summings-up, and\nSummaries of Discussions (beginning July 1, 2013, Chairman\u2019s Summings-up and Summaries of\nDiscussions are replaced by a single category of Board records called Chair Summaries); not all of"}
{"input":" What are some education programs related to AI among countries?","output":" Programs include developing vocational training and lifelong learning programs in AI-related fields to help citizens keep up with technological and societal changes; providing financial and non-financial support to retrain and attract top AI talent, including migration quotas and new visa routes; fostering academic partnerships between public and private AI research institutions; using AI to match people to jobs based on skills; and monitoring the impact of AI on the labor market for policy intervention.","context":"strategies also include incentives to retain and attract both domestic and\nforeign skills and top talent in AI. All national AI strategies support a persistent\nand robust AI education ecosystem.\nIn 2020, the OECD launched the Programme on AI in Work, Innovation,\nProductivity and Skills (AI-WIPS), supported by German Ministry of Labour. AIWIPS analyses the impact of AI on the labour market, skills and social policy\nwhile providing opportunities for international dialogue and policy\nassessments.\nIn addition, as AI policy affects several sectors, ensuring cross-governmental\nco-ordination is one of the key considerations for governments when\nimplementing effective AI policies (Box 2). To that end, countries often\nco-ordinate and collaborate across government and with business, educational\nand non-profit communities when developing educational programmes, tools\nand technologies.\nBox 2. Key considerations when implementing AI strategies and policies categories of tools and instruments that were used\nto assess national AI strategies. The third section\npresents findings from eleven countries, beginning\nwith a more detailed analysis of the national AI\npolicies of Finland and the United Arab Emirates\n(UAE). This section also includes findings from an\nanalysis of a broader set of countries in Asia, Africa\nand South America, highlighting the approach\ntaken to AI in some developing countries. The final\nsection highlights future areas of research needed\nto expand upon this work. AI skills, jobs and labour market transformation\nAs AI systems take over some tasks previously performed by humans, new\nopportunities are expected to emerge in the workplace. However, AI will also\nbring new challenges and transitions in the labour market. Governments have\nbegun to adopt policies and strategies to prepare citizens, educators and\nbusinesses for the jobs of the future and to minimise the negative impacts.\nMany national AI policies emphasise retraining for those displaced by AI, and\neducation and training for workers coming into the labour force, including\nvocational training and lifelong learning programmes.\nIn parallel, several countries are offering fellowships, postgraduate loans and\nscholarships to increase domestic AI research and expertise. Many national AI\nstrategies also include incentives to retain and attract both domestic and\nforeign skills and top talent in AI. All national AI strategies support a persistent\nand robust AI education ecosystem. FINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re-\n\nview of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries."}
{"input":" What are the new application areas of AI?","output":" The areas include security, the environment, research and education, health, culture and trade and so on.","context":"software) that make the machine intelligent. Hence, AI               from AI run on a supercomputer in a research lab. This\ncomprises a very wide range of approaches, techniques                is also a reason why it is difficult to differentiate AI\nand technologies.                                                    impacts from those of the underlying, necessary\n                                                                     infrastructure and related technologies. In particular,\nTo understand AI\u2019s current and future impacts on                     DNN requires \u201cbig data\u201d applications, data centers and\nhumanity\u2019s global aspirations - such as the SDGs - the               cloud computing. Similarly, there is a continuum of\nfunctional    definition     above      which      specifies         process or system automation tasks \u2013 in factories run by\npsychological skills is useful. For example, this approach           robots to online digital automation of tasks - some of categories of tools and instruments that were used\nto assess national AI strategies. The third section\npresents findings from eleven countries, beginning\nwith a more detailed analysis of the national AI\npolicies of Finland and the United Arab Emirates\n(UAE). This section also includes findings from an\nanalysis of a broader set of countries in Asia, Africa\nand South America, highlighting the approach\ntaken to AI in some developing countries. The final\nsection highlights future areas of research needed\nto expand upon this work. Artificial Intelligence (AI) seeks to make computers do                         full implications of AI without exploring the specificities\nthe sorts of things that minds can do, and various                              of the AI technology clusters concerned. It is hoped that\nfunctional or descriptive definitions exist. It is                              this primer provides an easily accessible guide to these\nimportant to note that AI has a long history, dating back                       specificities. Indeed, a black-box approach to \u201cAI\u201d is only\nto the 19th century, with many techniques dating to the                         of limited usefulness to most policy questions.\n1950s. In practice, various clusters of AI types are used\n\u2013 also in hybrid formats. prompts.\u201d26 Generative AI applications include chatbots, such as OpenAI\u2019s ChatGPT, photo and video\nfilters, and virtual assistants (PB 57).\nAutomated decision-making        A term that the Office for AI* uses to\nrefer to \u201cboth solely automated decisions (no human judgement\ninvolved) and automated assisted decision-making (assisting human judgement).\u201d27\nTraining datasets             The set of data used to train an AI system, which often requires \nlabelling (such as captioning pictures) to explain what the data means.\nAlgorithm                 A set of instructions used to perform tasks (such as calculations and \ndata analysis) usually using a computer or another smart device (PB 57).\n* The Office for AI is an office within the Department for Science, Innovation, and Technology.\n3                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nBenefits and risks\nJob creation and displacement"}
{"input":" What is the potential global significance of the week's AI policy activities, as suggested by the article's closing remarks? ","output":" The article suggests that the week's AI policy activities might be seen as a turning point in the effective global governance of AI, with a particular focus on the opportunities and threats posed by AI. However, it also notes that the only substantial legislation that may soon become law is the EU AI Act, indicating that the outcome of these activities in terms of global governance remains uncertain.","context":"While there is much variation in the form a\nstrategy takes \u2013 from a public announcement, to a\nguiding document, to a more comprehensive plan\n\u2013 governments are uniquely situated to consider\nthe risks and opportunities of AI within their own\nnational contexts and provide strategic direction.\nPrevious research that has explored and compared\nnational AI strategies and policies includes the\n8\nannual Stanford AI Index (2021), the OECD AI\nPolicy Observatory launched in 2020,9 the annual\nOxford Insights Government AI Readiness Index\n10\n(2020), the Future of Life Institute\u2019s review of\nNational and International AI Strategies,11 AiLab\u2019s AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. determining pay9\n\u2022  aiding with local and national public sector decisions, such as social housing allocation,10 \nbenefit claims, and other issues10\u201315\n\u2022  self-driving cars*16,17\nIn the past few years, there have been significant advancements in AI capabilities. Single AI \nsystems can now perform a wide range of tasks (PB 57). Generative AI (see Table for definitions) \ncan generate realistic text, images, audio, and video.18\u201320\nThere have been significant public policy developments relating to AI in recent years.\nIn 2023, the Government published a white paper outlining a \u2018pro-innovation approach to AI\u2019,21 \nalong with announcements including \u00a3900m for an \u2018exascale\u2019 \u2020 supercomputer.22 The Prime Minister \nhosted a global \u2018AI Safety Summit\u2019 in November 2023.23 It resulted in a declaration on AI safety \nsigned by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety. FINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re-\n\nview of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries."}
{"input":" How does the scope of CAN\/CIOSC 101:2019\u2122 differ from CAN\/DGSI 103-2:2021? ","output":" CAN\/CIOSC 101:2019\u2122 specifies minimum requirements in protecting human values during the design, creation, and use of artificial intelligence systems, whereas CAN\/DGSI 103-2:2021 focuses on minimum requirements for a user-centric digital identity ecosystem. ","context":"Future-proof legislation:\nAs AI is a fast evolving technology, the proposal has a future-proof approach, allowing rules to adapt to technological change. AI applications should remain trustworthy even after they have been placed on the market. This requires ongoing quality and risk management by providers.\n\nNext steps:\nFollowing the Commission\u2019s proposal in April 2021, the regulation could enter into force late 2022\/early 2023 in a transitional period. In this period, standards would be mandated and developed, and the governance structures set up would be operational. The second half of 2024 is the earliest time the regulation could become applicable to operators with the standards ready and the first conformity assessments carried out. Another point to consider is that the G7 statements do not explicitly address the link between regulations and standards. Even if international standards are established, unless they are aligned with each country's regulatory content and civil liability systems, businesses could still face challenges due to regulatory inconsistencies. This suggests a need for ongoing discussions among nations, not just to support the development of standards, but also to understand the interplay between regulations and standards.\nFurther Collaboration in DFFT and Emerging Technologies MMC Ventures. 2019. The State of AI 2019 Divergence.\nhttps:\/\/www.mmcventures.com\/The-State-of-AI-2019-Divergence.pdf\nMulgan, G. 2019. \u201cWhy Regulation Needs an Overhaul.\u201d Nesta (Blog), March 12.\nhttps:\/\/www.nesta.org.uk\/blog\/regulation-needs-overhaul\/\nMurison, Malek. 2018. \u201cBrazil\u2019s National IoT Strategy Gets the Green Light.\u201d Internet of Business, March 28.\nhttps:\/\/internetofbusiness.com\/brazil-national-iot-strategy\/\nMzekandaba, Simnikiwe. 2019. \u201cANC Promises to Bolster Tech Innovation.\u201d ITWeb, January 14.\nhttps:\/\/www.itweb.co.za\/content\/GxwQDM1A9BrMlPVo\nNational Communication Authority. 2018. Ghana\u2019s Digital Agenda.\nhttps:\/\/www.nca.org.gh\/assets\/Uploads\/Key-NCA-Projects-2018.pdf (iv)   Within 240 days of the completion of the guidelines described in subsection 4.3(a)(iii) of this section, the Assistant to the President for National Security Affairs and the Director of OMB, in consultation with the Secretary of Homeland Security, shall coordinate work by the heads of agencies with authority over critical infrastructure to develop and take steps for the Federal Government to mandate such guidelines, or appropriate portions thereof, through regulatory or other appropriate action.  Independent regulatory agencies are encouraged, as they deem appropriate, to consider whether to mandate guidance through regulatory action in their areas of authority and responsibility."}
{"input":" Considering the current developments, what does the future hold for the international cooperation on AI? ","output":" The future of international cooperation on AI seems geared towards creating common definitions and standards, sharing data governance frameworks, and aligning regulatory policies to reduce trade barriers, incentivize AI development, and address global challenges collaboratively. The article suggests that enhanced cooperation and shared projects can lead to a more unified approach to AI governance and the development of AI for social benefit.","context":"In this article, we analysed the nature of AI policy within China and the context within which it has emerged, by mapping the major national-level policy initiatives that express the intention to utilise AI. We identified three areas of particular relevance: international competitiveness, economic growth, and social governance (construction). The development and deployment of AI in each of these areas have implications for China and for the international community. For example, although the \u2018trump-card\u2019 policy to gain a military advantage may not be something new, its application to AI technologies risks igniting an arms race and undermining international stability (Taddeo and Floridi 2018). Efforts to counteract this trend seem largely hollow. Our analysis indicates that China has some of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f16 | AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 17 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13"}
{"input":" What were some of the themes discussed in the session AI Safety Policy: Advancing and Operationalizing Solutions?","output":" The session discussed what policymakers mean by AI safety, whether their definitions and priorities align with those of industry, civil society, and academia, and highlighted the prioritization of AI safety by leaders from the G7, the White House, and 10 Downing Street.","context":"The Taskforce describes itself as a \u201cstart-up inside government.\u201d It was formed at the direction of UK Prime Minister Rishi Sunak to create an AI research team that can \u201cevaluate the risks at the frontier of AI.\u201d Later this week, at the AI Safety Summit, the group will showcase its initial program results and present a series of ten-minute demonstrations focused on four areas of risk, which include misuse, societal harm, loss of human control, and unpredictable progress. The Taskforce will soon transition to a more formal AI Safety Institute that will work to develop the \u201csociotechnical infrastructure needed to understand the risks of advanced AI and support its governance.\u201d Safety, security, and robustness: AI systems should function as intended and in a robust, secure, and safe way throughout the AI lifecycle, and risks should be continually identified, assessed, and managed. Safety-related risks will be sector specific, and regulators should take a proportionate approach to manage them. Regulators may require the corresponding AI lifecycle actors to regularly test or carry out due diligence on the functioning, resilience, and security of a system. The Center for a New American Security (CNAS) Artificial Intelligence Safety and Stability project aims to better understand AI risks and specific steps that can be taken to improve AI safety and stability in national security applications. Major lines of effort include: determining pay9\n\u2022  aiding with local and national public sector decisions, such as social housing allocation,10 \nbenefit claims, and other issues10\u201315\n\u2022  self-driving cars*16,17\nIn the past few years, there have been significant advancements in AI capabilities. Single AI \nsystems can now perform a wide range of tasks (PB 57). Generative AI (see Table for definitions) \ncan generate realistic text, images, audio, and video.18\u201320\nThere have been significant public policy developments relating to AI in recent years.\nIn 2023, the Government published a white paper outlining a \u2018pro-innovation approach to AI\u2019,21 \nalong with announcements including \u00a3900m for an \u2018exascale\u2019 \u2020 supercomputer.22 The Prime Minister \nhosted a global \u2018AI Safety Summit\u2019 in November 2023.23 It resulted in a declaration on AI safety \nsigned by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety."}
