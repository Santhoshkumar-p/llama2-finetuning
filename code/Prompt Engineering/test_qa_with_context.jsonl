{"input":"What is the trajectory of Chinese AI governance, and what milestone is it approaching?","output":"Chinese AI governance is heading towards drafting a comprehensive national AI law, mirroring the evolution of internet governance regulations, with a potential draft release by late 2023 or 2024 and subsequent revisions involving key stakeholders.","context":"In this series of three papers, I will attempt to reverse engineer Chinese AI governance. \nI break down the regulations into their component parts\u2014the terminology, key concepts, and specific requirements\u2014and then trace those components to their roots, revealing how 3.The \u201cPolicy Funnel\u201d of China\u2019s AI Governance Major governance initiatives tend to proceed from left to right through this funnel, though often not in a linear fashion. China now appears to be following that same blueprint for AI, though on an accelerated time line. There are no firm deadlines for the national AI law, but a draft version could be released in late 2023 or 2024, followed by six to eighteen months dedicated to revising the law. During that time, many of the organizations, individuals, and intellectual influences described in this paper will be shaping one of the world\u2019s most important pieces of legislation for AI governance. The subsequent papers in this series will dig deeper into key players in this process, illustrating how China formulates AI regulations and previewing what likely lies ahead.\nAbout the Author: Matt Sheehan is a fellow at the Carnegie Endowment for International Peace, where his research focuses on global technology issues, with a specialization in China\u2019s artificial intelligence ecosystem. The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. And despite China\u2019s drastically different political system, policymakers in the United States and elsewhere can learn from its regulations. China\u2019s regulations create new bureaucratic and technical tools: disclosure requirements, model auditing mechanisms, and technical per-formance standards. These tools can be put to different uses in different countries, ranging from authoritarian controls on speech to democratic oversight of automated decisionmaking.\nCharting the successes, failures, and technical feasibility of China\u2019s AI regulations can give policymakers elsewhere a preview of what is possible and what might be pointless when it comes to governing AI.\nSo what do China\u2019s AI regulations contain? How did its massive party and state bureaucracies formulate them? And is it possible to predict where Chinese AI governance is headed?\nThis is the first in a series of three papers that will tackle these questions using a novel approach: reverse engineering."}
{"input":"How does the Europe Artificial Intelligence Act regulate general-purpose AI systems and high-impact GPAI models?","output":"General-purpose AI systems must be transparent, while high-impact models with systemic risks face stricter evaluation, risk mitigation, incident reporting, and cybersecurity measures.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground Guardrails for general artificial intelligence systems: To account for the wide range of tasks AI systems can accomplish and the quick expansion of its capabilities, it was agreed that general-purpose AI (GPAI) systems, and the GPAI models they are based on, will have to adhere to transparency requirements as initially proposed by Parliament. These include drawing up technical documentation, complying with EU copyright law and disseminating detailed summaries about the content used for training. discussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial\nIntelligence\u201d \u2013 A European Approach to Excellence and Trust. The paper\nconsiders requiring a pre-marketing conformity assessment for \u201chigh-risk\u201d AI\napplications such as facial recognition, as a core element of a potential\nregulatory framework for AI. In addition, the white paper proposes a voluntary\n\u201cquality label\u201d for AI applications considered not to be high-risk. In parallel, the\nEuropean Commission is reviewing EU product safety and liability regimes in\nlight of AI.\nRead more:\nhttps:\/\/ec.europa.eu\/digital-single-market\/en\/artificial-intelligence.\nGlobal Partnership on AI (GPAI)\nResponsible entity: Consortium of countries\nDescription: GPAI is an international and multi-stakeholder initiative that\nadvances cutting-edge research and pilot projects on AI priorities to advance"}
{"input":"How do providers of high-risk AI systems manage post-market monitoring according to Europe Commission?","output":"Providers follow a step-by-step process, including conformity assessment and registration. Post-market, authorities conduct surveillance, users ensure human oversight, and providers implement monitoring systems, reporting incidents and malfunctions.","context":"step-by-step process for declaration of conformity\nStep1: A high risk AI system is developed\nStep2: It needs to undergo the conformity assessment and comply with AI requirements. For some systems a notified body is involved too\nStep3: Registration of stand-alone AI systems in an EU database\nStep4: A declartion of conformity needs to be signed and the AI system should bear the CE marketing. The system can be placed on the market. \nStep 5: If substantial changes happen in the AI system's lifecycle, go back to step 2\n\nHow does it all work in practice for providers of high risk AI systems?\nOnce an AI system is on the market, authorities are in charge of market surveillance, users ensure human oversight and monitoring, and providers have a post-market monitoring system in place. Providers and users will also report serious incidents and malfunctioning. The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. discussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial\nIntelligence\u201d \u2013 A European Approach to Excellence and Trust. The paper\nconsiders requiring a pre-marketing conformity assessment for \u201chigh-risk\u201d AI\napplications such as facial recognition, as a core element of a potential\nregulatory framework for AI. In addition, the white paper proposes a voluntary\n\u201cquality label\u201d for AI applications considered not to be high-risk. In parallel, the\nEuropean Commission is reviewing EU product safety and liability regimes in\nlight of AI.\nRead more:\nhttps:\/\/ec.europa.eu\/digital-single-market\/en\/artificial-intelligence.\nGlobal Partnership on AI (GPAI)\nResponsible entity: Consortium of countries\nDescription: GPAI is an international and multi-stakeholder initiative that\nadvances cutting-edge research and pilot projects on AI priorities to advance AI policy intelligence to monitor implementation\nAI Watch\nResponsible entity: European Commission Joint Research Centre\nDescription: The EU\u2019s 2018 Coordinated Action Plan on the development of AI\nannounced the creation of AI Watch, the \u201cEuropean Commission Knowledge\nService to Monitor the Development, Uptake and Impact of Artificial\nIntelligence Policy for Europe\u201d. AI Watch is a project developed by the Joint\nResearch Centre and DG CONNECT of the European Commission. It monitors AIrelated development and provides analyses to support the implementation of\nthe European AI initiatives. JRC is also developing a methodology to identify\nrisks and opportunities, drivers and barriers of the use of AI in public service\nprovision. In February 2020, JRC launched a report on national AI strategies of\nEU member countries, the objective of which is \u201cto present and gather\ninformation on all EU Member States' national AI strategies in a structured and"}
{"input":"What are the cross-sectoral principles of the UK regulatory framework, and how will they be implemented?","output":"The principles cover safety, transparency, fairness, accountability, and contestability. Regulators will implement them through guidance, technical standards, and assurance techniques.","context":"Five cross-sectoral principles: When implementing a context-specific approach, regulators must have regard to five cross-sectoral principles, as explained further below. Next Steps:\nThe government requested views on certain proposals, including the cross-sectoral principles, by June 21, 2023. The white paper also includes a long list of actions for the UK government to undertake over the coming year and beyond, which include the following:\nPublishing a portfolio of AI assurance techniques:\nPublishing an AI regulation roadmap for the central risk and monitoring functions\nEncouraging regulators to publish guidance on how cross-sectoral principles will apply within their remit\nPublishing a draft central, cross-economy AI risk register for consultation New centralized coordinating functions: The government will establish cross-sectoral monitoring, risk assessment, education, horizon-scanning, and other centralized functions in order to support implementation and coherence of the framework.\nAI assurance techniques and technical standards: The government suggests that these will play a critical role in supporting the framework, which it will encourage by publishing a toolkit of assurance techniques in collaboration with industry.\nTerritorial application: The framework applies across the United Kingdom and will not change the territorial application of any existing legislation. The UK government will work with international partners to promote interoperability and coherence between different approaches, noting the complex and cross-border nature of AI supply chains.\nCross-Sectoral Principles\n\nThe principles of the regulatory framework are explained further below: UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise."}
{"input":"What are some of the jobs that may be affected by Generative AI in the UK?","output":"Generative AI has the possibility to affect many chosen careers across the UK. These affects can range from providing assistive technology to posing an existential threat to some forms of work. Careers that are highly technical and require a large amount of human responsibility such as medical careers are largely safe, as are careers that require a diverse amount of manual labor. However, low skill jobs that are largely repetitive are at risk of becoming obsolete and others may be at risk of being subject to AI driven management automation.","context":"for the Future of Work.\n33. Briggs, J. et al. (2023). The Potentially Large Effects of Artificial Intelligence on Economic \nGrowth. Goldman Sachs.\n34. Department for Science, Innovation and Technology (2021). The potential impact of AI on UK \nemployment and the demand for skills. GOV.UK.\n35. Pawel Gmyrek, J. B. (2023). Generative AI and Jobs: A global analysis of potential effects on \njob quantity and quality. International Labour Organization.\n36. Ellingrud, K. et al. (2023). Generative AI and the future of work in America. McKinsey.\n37. Young, E. et al. (2023). Mind the gender gap: Inequalities in the\nemergent professions of artificial intelligence (AI) and data science. New Technol. Work Employ., \nVol 38, 391\u2013414.\n38. Hayton, J. et al. (2023). Briefing Paper: What drives UK firms to\nadopt AI and robotics, and what are the consequences for jobs? Zenodo.\n39. Wall, J. et al. (2023). Making AI work for Britain. techUK. 3                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nBenefits and risks\nJob creation and displacement\nIn 2022, a report commissioned for the Department for Science, Innovation & Technology stated that \ninvestment in the UK AI sector grew five-fold between 2019 and 2021, and that it brought in \u00a310.6 \nbillion in revenue and employed over 50,000 people.28\nOECD analysis published in July 2023 found that AI was changing the nature of work by assisting \nworkers and reducing the time spent on mundane tasks, rather than causing job losses.29\nHowever, some emerging academic research indicates that developments in generative AI may be linked \nto a loss in the quantity and earnings of white-collar jobs.30,31\nThere is potential for new jobs across all sectors to be created with improved productivity and \nglobal economic growth (PB 57).32 However, some reports state that certain jobs, such as clerical There is potential for new jobs across all sectors to be created with improved productivity and \nglobal economic growth (PB 57).32 However, some reports state that certain jobs, such as clerical \nwork,33,34 could become redundant.\nStakeholders have raised concerns that AI developments may disproportionately affect disadvantaged \ngroups. For example, the majority of clerical work is carried out by women.34\u201337\nA 2021 report commissioned by the former Department for Business, Energy and Industrial Strategy \nhighlighted a regional disparity in the net employment impacts of AI, with London and the South \nEast benefitting from net job gains more than Northern England and the Midlands.34\nSome academics and think tanks,38 and technology trade associations,39 have said that the \nGovernment should help workers retrain and gain relevant skills, and ensure that existing \ninequalities are not exacerbated (PN 697).\nIn October 2023, the Government announced \u00a3118m to increase the UK\u2019s AI skills adopt AI and robotics, and what are the consequences for jobs? Zenodo.\n39. Wall, J. et al. (2023). Making AI work for Britain. techUK.\n40. Department for Science, Innovation and Technology, Britain to be made AI match-fit with \u00a3118 \nmillion skills package. GOV.UK.\n41. Tuomi, A. et al. (2023). Riding Against the Algorithm: Algorithmic Management in On- Demand \nFood Delivery. in Information and Communication Technologies in Tourism 2023. (eds. Ferrer-Rosell, \nB. et al.) 28\u2013\n39. Springer Nature Switzerland.\n16                                  POSTnote 708 - 9 January 2024"}
{"input":"In what ways will the Federal Government lead global efforts in the responsible development and use of AI?","output":"The administration seeks to lead by developing frameworks for AI risk management and safety with international allies, promoting responsible AI principles globally, and engaging in collaborations to ensure AI benefits are shared worldwide without exacerbating inequities or harming human rights.","context":"(h)  The Federal Government should lead the way to global societal, economic, and technological progress, as the United States has in previous eras of disruptive innovation and change.  This leadership is not measured solely by the technological advancements our country makes.  Effective leadership also means pioneering those systems and safeguards needed to deploy technology responsibly \u2014 and building and promoting those safeguards with the rest of the world.  My Administration will engage with international allies and partners in developing a framework to manage AI\u2019s risks, unlock AI\u2019s potential for good, and promote common approaches to shared challenges.  The Federal Government will seek to promote responsible AI safety and security principles and actions with other nations, including our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is Effective implementation of national AI initiatives hinges on coordination\nCountries pursue different national governance models to co-ordinate the\nimplementation of their national AI policies across government, offering\nregulatory and ethical oversight (Figure 1). Models include:\n\u2022\n\nAssigning oversight of the development and implementation strategies\nto an existing ministry, department or body. Among existing ministries\nor agencies tasked with developing or implementing an AI strategy, the\nfollowing tend to drive the creation of AI strategies most often: 1)\ninformation technology and communications ministries; 2) economics or\nfinance ministries; or 3) education, science (and technology) and\ninnovation ministries.\n\n\u2022\n\nCreating a new governmental or independent AI co-ordination entity.\n\n\u2022 FUTURE RESEARCH\nAs policy and regulatory pathways for harnessing AI\nare still in their infancy, further time and research\nis needed to assess the outputs of these strategies,\nand their effectiveness in order to identify best\npractices for developing countries. Rather than\nconcluding with best practices, this article provides\nan initial review of policy and regulatory practices\ncurrently deployed in AI policymaking, illustrating\na wide range of tools and approaches that\ngovernments have taken in their attempt to harness\nAI technologies while mitigating anticipated risks.\nWhile many of the policies observed have been"}
{"input":"What rights do individuals have to notice and explanation regarding the use of automated systems?","output":"Individuals have the right to clear and accessible information about the use of automated systems, including how and why they impact outcomes. Systems should provide explanations that are meaningful and useful, with public reporting on the clarity and quality of this information.","context":"You should know that an automated system is being used and understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of automated systems should provide generally accessible plain language documentation including clear descriptions of the overall system functioning and the role automation plays, notice that such systems are in use, the individual or organization responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice should be kept up-to-date and people impacted by the system should be notified of significant use case or key functionality changes. You should know how and why an outcome impacting you was determined by an automated system, including when the automated system is not the sole input determining the outcome. Automated systems should provide explanations that are technically valid, meaningful and useful to you and to any operators or others who need to understand the system, accompanied by appropriate operator training, and should not impose an unreasonable burden on the public. Automated systems with an intended use within sensitive domains, including, but not limited to, criminal justice, employment, education, and health, should additionally be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting with the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includes a description of these human governance processes and assessment of their timeliness, accessibility, outcomes, and effectiveness should be made public whenever possible. accompanied by appropriate operator training, and should not impose an unreasonable burden on the public. Automated systems with an intended use within sensitive domains, including, but not limited to, criminal justice, employment, education, and health, should additionally be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting with the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includes a description of these human governance processes and assessment of their timeliness, accessibility, outcomes, and effectiveness should be made public whenever possible. You should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter. You should be able to opt out from automated systems in favor of a human alternative, where appropriate. Appropriateness should be determined based on reasonable expectations in a given context and with a focus on ensuring broad accessibility and protecting the public from especially harmful impacts. In some cases, a human or other alternative may be required by law. You should have access to timely human consideration and remedy by a fallback and escalation process if an automated system fails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration and fallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and should not impose an unreasonable burden on the public. Automated systems with an intended use within sensitive domains, including, but not"}
{"input":"How does this initiative fit into the broader efforts of the Biden-Harris Administration regarding AI?","output":"This initiative is part of a whole-of-government effort to address AI technology, with the Biden-Harris Administration announcing commitments from companies to advance safe and trustworthy AI and seeking bipartisan legislation for responsible AI development.","context":"on Digital Cooperation.\nhttps:\/\/www.un.org\/en\/pdfs\/DigitalCooperation-report-for%20web.pdf\nUSAID.2018. Reflecting the Past, Shaping the Future: Making AI Work for International Development. Washington, DC.\nhttps:\/\/www.usaid.gov\/sites\/default\/files\/documents\/15396\/AI-ML-in-Development.pdf\nVillani, C. and others. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy.\nParis: Conseil national du num\u00e9rique.\nVinuesa, R., Azizpour, H., Leite, I. et al. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals.\nNature Communications, 11, 233.\nhttps:\/\/doi.org\/10.1038\/s41467-019-14108-y\nWhittaker, M. and others. 2018. AI Now Report 2018. New York: AI Now Institute, New York University.\nhttps:\/\/ainowinstitute.org\/AI_Now_2018_Report.pdf\nWhite House. 2020. American Artificial Intelligence Initiative: Year One Annual Report. Office of Science and Technology Policy. While there is much variation in the form a\nstrategy takes \u2013 from a public announcement, to a\nguiding document, to a more comprehensive plan\n\u2013 governments are uniquely situated to consider\nthe risks and opportunities of AI within their own\nnational contexts and provide strategic direction.\nPrevious research that has explored and compared\nnational AI strategies and policies includes the\n8\nannual Stanford AI Index (2021), the OECD AI\nPolicy Observatory launched in 2020,9 the annual\nOxford Insights Government AI Readiness Index\n10\n(2020), the Future of Life Institute\u2019s review of\nNational and International AI Strategies,11 AiLab\u2019s determining pay9\n\u2022  aiding with local and national public sector decisions, such as social housing allocation,10 \nbenefit claims, and other issues10\u201315\n\u2022  self-driving cars*16,17\nIn the past few years, there have been significant advancements in AI capabilities. Single AI \nsystems can now perform a wide range of tasks (PB 57). Generative AI (see Table for definitions) \ncan generate realistic text, images, audio, and video.18\u201320\nThere have been significant public policy developments relating to AI in recent years.\nIn 2023, the Government published a white paper outlining a \u2018pro-innovation approach to AI\u2019,21 \nalong with announcements including \u00a3900m for an \u2018exascale\u2019 \u2020 supercomputer.22 The Prime Minister \nhosted a global \u2018AI Safety Summit\u2019 in November 2023.23 It resulted in a declaration on AI safety \nsigned by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety. The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with"}
{"input":"What role does EPIC play in AI policy development, and what stance does it take on these issues?","output":"EPIC advocates for comprehensive privacy protections, rigorous testing protocols, expanded resources for evaluating AI systems, and a government-wide effort to combat algorithmic discrimination. It tracks AI legislation at state and local levels and emphasizes the need for policies that protect individuals from algorithmic harm.","context":"EPIC.ORG AI Policy\n\nBACKGROUND\nAI policy must protect individuals, increase transparency and accountability, and center human rights.\n\nStates and municipalities are increasingly taking interest in Artificial Intelligence and filling the gaps left by federal inaction on algorithmic harm. States and cities have taken different routes, from notification and task forces to minimum privacy standards. Some of the recent efforts are highlighted below. EPIC is not including state and local laws primarily focused on investing resources in building more AI and general research, as they do not improve protection of individuals against algorithmic harm. At the federal level, EPIC is only including laws of this sort that have been enacted, and only including laws within the last several years. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. For government use of AI, the EO requires the development of guidance for agency use of AI and a faster and more efficient process for agencies to procure AI products and services. The directive also calls for the rapid hiring of AI professionals and the training of employees at all levels. The content of a forthcoming Office of Management and Budget memo will dictate the details of how government AI use will change.\n\nEPIC has long advocated for comprehensive privacy protections, rigorous testing protocols, expanded resources for evaluation of AI systems, and a government-whole effort to fighting algorithmic discrimination.\n\nTwo days later, on November 1st, 2023, the Office of Management and Budget released a memo dictating how Government Agencies should approach using and regulating AI. There is a 30-day comment period before it\u2019s finalized. FINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re-\n\nview of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries."}
{"input":"How might the government ensure that people negatively affected by AI can receive help?","output":"The government could help people negatively affecteed by AI in several ways that are currently available. There are numerous agencies that can support individuals who are victim to discrimination based on race, religion, sex, gender, etc. Additionally, providing a way for whistleblowers to voice their concerns anonymously could help government agencies to address issues with AI more quickly.","context":"of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions that emerge from using AI in the industry are currently lacking. Thus, AI can help foster increased productivity and high levels of growth, but its use is likely to intensify the inequalities present within society and even decrease support for the government and its policies. The AIDP also promotes AI as a way to help deal with some of the major social problems, ranging from pollution to standards of living. However, positive impact in this area seem to come with increased control over individuals\u2019 behaviour, with governance extending into the realm of moral behaviour and further erosion of privacy. Governments should look beyond traditional regulatory techniques to influence AI developments Why is AI important?\nDisruptive technologies like artificial intelligence (AI) have the potential to\ntransform governments. AI can be used as a tool that can deliver a personalized\nservice delivery experience, improve efficiency of back-end processes,\nstrengthen policy compliance and aid in the identification of fraud. Artificial\nIntelligence can be defined as the programming of software systems, often\nusing algorithms, to carry out some tasks that have in the past usually required\nhuman intelligence; tasks that involved human vision, speech, language,\nknowledge, and search. If used correctly and used together with human\nmanagement and decision-making, governments can also use AI to address\ndevelopment challenges. As government\u2019s confront the economic and health\nchallenges arising from COVID 19, the impact on government\u2019s fiscal stress\nand mobility limitations on citizens and civil servants are proving to be drivers or jeopardize safety or rights.  At the same time, my Administration will promote responsible uses of AI that protect consumers, raise the quality of goods and services, lower their prices, or expand selection and availability."}
{"input":"Why is it difficult to regulate AI? ","output":"AI is a general-purpose technology that is likely to be ubiquitous and apply to a wide range of applications. This makes it harder to regulate well.","context":"The rapid pace and diffusion of advanced AI technology will make multilateral regulation difficult. AI lacks the chokepoints of traditional models of nonproliferation, such as the global nuclear nonproliferation regime, meaning that it will be comparatively difficult for governments to control AI using traditional regulatory techniques. Governments should look beyond traditional regulatory techniques to influence AI developments Why do we need rules on AI?\nThe proposed AI regulation ensures that Europeans can trust what AI has to offer. While most AI systems pose limited to no risk and can contribute to solving many societal challenges, certain AI systems create risks that we must address to avoid undesirable outcomes.\n\nFor example, it is often not possible to find out why an AI system has made a decision or prediction and taken a particular action. So, it may become difficult to assess whether someone has been unfairly disadvantaged, such as in a hiring decision or in an application for a public benefit scheme.\n\nAlthough existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring. I. The AI Regulation Debate: A Wide Spectrum of Views. The AI regulation debate has become heated and politicized, reflecting the immense hopes and fears that we have invested in AI.   \n\nHysterical fear and paranoia. On one end, leading technologist and venture capitalist Marc Andreessen  says \u201cwe have a full-blown moral panic about AI right now.\u201d He views this as \u201chysterical fear\u201d that is \u201cirrational\u201d and compares it to earlier introduction of new technologies that were similarly feared but were overblown and subsequently managed. He believes that AI is simply a computer program that is owned, controlled and used by people, and that AI does not have its own goals or its own wants. Moreover, he emphasizes, AI can be a force for tremendous good."}
{"input":"Why was the U.S concerned about China\u0092s breakthrough in AI research? ","output":"U.S. political and military leaders were concerned that China's newfound AI capabilities would provide it with an asymmetric military advantage over the United States.","context":"The United States and China will lead in different ways\nAlthough U.S. policymakers often worry that China will take the lead in a race for AI, the consensus from the experts with whom we consulted was that China is highly unlikely to produce a major new advance in AI because of U.S. superiority in empowering private sector innovation. China is unlikely to catch up to or surpass the United States in producing AI breakthroughs, but it could lead in different arenas. China's demonstrated record of refining technological capabilities produced elsewhere and the Chinese Communist Party's preferred approach of societal use cases mean that China could see greater success integrating today's technologies into its workflows, weapons, and systems, whereas the United States could fall behind in end-to-end systems integration. to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response. The AIDP provides a longitudinal perspective on China\u2019s strategic situation regarding AI, including its comparative capabilities, the opportunities offered, and the potential risks. Following a technology-first approach, it may be tempting to concentrate one\u2019s attention on the stated capabilities of AI, to gain an insight into the types of technologies in which China is investing. However, this would likely offer only a short-term perspective and would soon be out of date as technological innovation advances rapidly. Furthermore, it would do little to explain why China is seeking to develop a strong AI sector in the decades to come. To this end, it is more useful to try to understand China\u2019s strategic focus from a policy-first approach, by analysing the areas where China considers that AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible"}
{"input":"Why is U.S. leadership on AI governance crucial? ","output":"U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4.","context":"On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 Finally, there remains one auxiliary goal: making China a leader in the governance and regulation of AI. U.S. and Chinese leaders frequently point out that China has laid out some of the world\u2019s first binding regulations on AI, with the latter using it as a point of pride and the former as an impetus to action. But the rhetorical emphasis on global leadership often leads to a mistaken impression that this is a major driver of Chinese actions. An examination of the regulations and conversations with Chinese policy actors indicates otherwise. For China, Matt Sheehan | 17\nbeing a global leader or model for AI governance is a \u201cnice-to-have\u201d\u2014a small bonus for its businesses and national soft power, but not a significant driver of these AI regulations.\n\nChina\u2019s choice of first targets for regulation\u2014recommendation algorithms and deep synthesis\u2014indicates that global leadership is not a core motivation for its AI governance. (h)  The Federal Government should lead the way to global societal, economic, and technological progress, as the United States has in previous eras of disruptive innovation and change.  This leadership is not measured solely by the technological advancements our country makes.  Effective leadership also means pioneering those systems and safeguards needed to deploy technology responsibly \u2014 and building and promoting those safeguards with the rest of the world.  My Administration will engage with international allies and partners in developing a framework to manage AI\u2019s risks, unlock AI\u2019s potential for good, and promote common approaches to shared challenges.  The Federal Government will seek to promote responsible AI safety and security principles and actions with other nations, including our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing"}
{"input":"Which international organizations are likely to spearhead the development of tools for trustworthy AI? ","output":"The first expected drivers of such tools are international organizations such as the OECD and United Nations Educational, Scientific and Cultural Organization (UNESCO), as well as multi-stakeholder initiatives such as the GPAI.","context":"\u2022\n\nOrganisations and individuals developing, deploying or operating AI\nsystems should be held accountable for their proper functioning in line with\nthe above principles\n\nConsistent with these principles, the OECD also provides five recommendations to\ngovernments:\n\u2022\n\nFacilitate public and private investment in research & development to spur\ninnovation in trustworthy AI.\n\n\u2022\n\nFoster accessible AI ecosystems with digital infrastructure and technologies\nand mechanisms to share data and knowledge.\n\n\u2022\n\nEnsure a policy environment that will open the way to the deployment of\ntrustworthy AI systems.\n\n\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI. Great Power Public-Private Consortium\nAcross the world, robust partnerships among government, global industry, civil society organizations, academia, and research institutions support the rapid development and deployment of AI. These partnerships form a consortium that carries out multi-stakeholder project collaborations that access large-scale computational data and training, computing, and storage resources. Through funding from many governments, the consortium develops joint solutions and benchmarking efforts to evaluate, verify, and validate the trustworthiness, reliability, and robustness of AI systems. New and existing international government bodies, including the Abu Dhabi AI Council, rely on diverse participation and contributions to set standards for responsible AI use. The result is a healthy AI sector that supports economic growth that occurs concurrently with the development and evaluation of equitable, safe, and secure AI systems. The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d 60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed"}
{"input":"How effective are current US export controls at limiting the unlawful sale and distribution of advanced technologies?","output":"Currently, export controls are not very effective at restricting access to advanced technology, foreign agents devote large amounts of resources to circumvent existing US export controls. Evidence has been found indicating the presence of Western manufacured technology like microchips in sanctioned nations' missiles.","context":"Improved Export Controls Enforcement Technology Needed for U.S. National Security\nExecutive Summary\nAs technology has become increasingly central to strategic competition with Russia and China, export controls have moved to the forefront of U.S. foreign policy on technology issues. Most notably, restricting Russia\u2019s access to advanced technology through export controls is a key part of the U.S. response to Russia\u2019s invasion of Ukraine, as U.S. government officials have repeatedly stated.\n\nUnfortunately, nearly all the debate is focused on whether and when to apply export controls, not how to ensure that export controls are effectively administered and enforced once applied. On October 7, 2022, the Department of Commerce announced major changes to export control policies toward China for the procurement of advanced AI and semiconductor technology used in both commercial and military AI systems. This policy document included important acknowledgments that previous export control approaches\u2014based on restricting the sale of dual-use technologies to military end users or for military end uses\u2014had diminished in effectiveness over the past decade. Specifically, the new policy stated that controls prior to October 7 \u201cgenerally only apply when the \u2018U.S. person\u2019 has knowledge that their activities are contributing to prohibited end uses or end users. China\u2019s military-civil fusion effort makes it more difficult to tell which items are made for restricted end uses, thereby diminishing the effect of these existing controls.\u201d The October 7, 2022, package of export controls against China is too new for comparable data to be available at the time of this writing, but the same caution applies: published government data does not necessarily tell the whole story, which must focus on both licit and illicit transactions. \n\nFor BIS, the scale and complexity of U.S. export control requirements have increased massively over the past three years due to new legislative and executive requirements.\n\nFor those exports that do require a license, the typical process for export controls administration and enforcement is depicted in Figure 1.\n\n\n\nIncreasingly sophisticated and well-resourced attempts by Russian and Chinese organizations to evade export controls are not the only reason that BIS\u2019s job has become significantly harder in recent years. Legislation and executive orders have posed new challenges to BIS in administering and enforcing export controls in at least three ways. The Huawei and ZTE export controls were a potent reminder to the U.S. foreign policy community of the power of export controls. U.S. technology export controls were able to do significant damage to the financial prospects of leading Chinese technology giants. The United States did this entirely with non-violent means and at a limited cost to the U.S. economy.\n\nHowever, major weaknesses in U.S. export control enforcement capacity are evident, as demonstrated by Russia\u2019s success in evading controls for weapons used in Ukraine."}
{"input":"How does AI potentially impact the Sustainable Development Goals (SDGs)?","output":"AI could enable the accomplishment of 134 SDG targets but also inhibit 59 targets, highlighting its dual potential to both advance and challenge global sustainability efforts.","context":"leading to new applications, new development models,\ncombined with other methods as need be. This will have\n                                                                                and also sustainability concerns. This has important\nimportant implications on our SDG aspirations until\n                                                                                implications for humanity\u2019s aspirations expressed in the\n2030, as will be detailed in this report.\n                                                                                SDGs.\nConclusion                                                                      Finally, it is impossible to adequately understand the\nArtificial Intelligence (AI) seeks to make computers do                         full implications of AI without exploring the specificities thing that \u201cwe know when we see it\u201d. For example, it is              list as a working basis to discuss SDG impacts. However,\nnotoriously difficult to measure human intelligence, yet             I am cognizant of the fact that such a \u201cblack-box\nit is pretty obvious to us when we meet a highly                     approach\u201d to AI which does not specify the underlying\nintelligent person.                                                  techniques and technologies is not sufficient to fully\nAs a working definition, I suggest following eminent                 understand all SDG impacts. The details of AI concepts,\ncognitive scientist Margaret Boden: \u201cArtificial                      techniques and technologies matter greatly, especially\nIntelligence (AI) seeks to make computers do the sorts of            with regard to sustainability and with regard to future\nthings that minds can do. Some of these (e.g., reasoning,            directions of AI and its physical, economic and socio- agribusiness, financial services, manufacturing and\ngender equality. AI-driven risk management may\nyield another major opportunity for developing\ncountries, including disease prevention and\nnatural disaster and crisis management (Strusani &\nHoungbonon 2019).\n\nAI applications have the potential to address some\nchallenging societal problems and provide solutions\nto achieving targets in each of the Sustainable\nDevelopment Goals (SDGs). AI innovation and\nadoption has the potential to help progress towards\ngoals in education, climate change, disaster relief,\nhealth care and the delivery of public sector\nservices, representing some of the most critical\ncross-sectoral SDG domains for developing\ncountries (Vinuesa et al. 2020). A recent report by\nthe IFC (2020) provides a comprehensive overview\nof AI development opportunities across several key\nsectors including power, transport, smart homes, 1Note: The views expressed in this brief are those of the author and do not necessarily reflect those of the United Nations or its\nsenior management.\n\f   Science-Policy Brief for the Multistakeholder Forum on Science, Technology and Innovation for the SDGs, May 2022\n\n                                                             Table 1. Basic AI functions\n\n                                                                    Basic AI functions\n                         It can \u201csee\u201d      It can \u201chear\u201d             It can \u201cread\u201d         It can move by     It can \u201creason\u201d\n                         and identify                                                      itself, based on     and looks for\n        What it can do"}
{"input":"What challenges are identified in achieving effective AI governance?","output":"The opacity of AI systems due to their complexity or commercial secrecy, territorial and fragmented governance, and the global reach of AI necessitating a coordinated international response to manage its development, deployment, and use effectively are some of the challenges identified in achieving effective AI governance.","context":"AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI governance that can otherwise stifle innovation and reduce opportunities for uptake and use of AI; and finding ways to broaden access globally to the computing power and data that is essential for building and training AI models. Accountability and governance: Businesses should put in place governance measures that ensure effective oversight of the supply and use of AI systems, with clear lines of accountability established across the AI lifecycle. Regulators will likely implement this principle through regulatory guidance and assurance techniques.\nContestability and redress: Users, impacted third parties, and actors in the AI lifecycle should be able to contest an AI decision or outcome that is harmful or creates material risk of harm. Regulators will be expected to clarify existing routes to contestability and redress and implement proportionate measures to ensure that the outcomes of AI use are contestable where appropriate. The government\u2019s initial non-statutory approach will not create new rights or new routes to redress at this stage. Measures to Address Challenges\n\u2022 Issue and make fully transparent\nthe legal framework, AI policy,\nethical principles and operating\nframework.\n\u2022 Establish special AI\/Innovation Hub\nor government unit as a center of\nexcellence.\n\u2022 Develop whole-of-government,\ndata fabric AI architecture.\n\u2022 Ensure human oversight to\nmanage risks and safeguard\nagainst machine-invoked bias."}
{"input":"What recourse is available if access to information is denied under the AI Policy?","output":"Individuals can file an appeal if they believe access has been improperly or unreasonably denied, with a structured process for reviewing decisions, including a potential appeal to the Access to Information Committee and the AI Appeals Board.","context":"AI POLICY HIGHLIGHTS\n\nWhat is the main difference between the Bank's Access to Information Policy (AI Policy) and the previous\nDisclosure policy?\n\nThe AI Policy, launched in 2010, moved the Bank from an approach that spelled out what information is disclosed to\none that allows the Bank to disclose any information in its possession that is not on a list of exceptions set out in the AI\nPolicy.\n\n    \uf0b7    Under the AI Policy, significantly more information on Bank operations and Board proceeding is available.\n    \uf0b7    The new Policy permits public release of some documents prior to discussion by the World Bank's Board of\n         Executive Directors.\n    \uf0b7    Certain restricted information is eligible for declassification after 5, 10, or 20 years.\n    \uf0b7    It also establishes an appeal mechanism that provides public recourse when the Bank denies access to\n         information. Go Top\n\fWhat can be disclosed under the AI Policy?\n\nThe Bank can disclose any information in its possession, provided that the information does not fall under one or more\nof the exceptions set out in the AI Policy or the Bank has decided, on an exceptional basis, to restrict access to the\ninformation.\n\n                                                                                                                    Go Top\n\nWhat are the exceptions to disclosing information?\n\nThe Policy explicitly sets out 10 categories of information that are considered to be exceptions and not eligible for\ndeclassification: In the UK\nWhile there is no current body of UK law specifically regulating AI, there are numerous laws that \nrestrict how AI can be used in practice,93,226 including (HoC Library briefing on AI and employment \nlaw):\n\u2022   data protection law, such as the Data Protection Act 2018,227 that affects data collection and \nprocessing for AI development, and is the remit of the Information Commissioner\u2019s Office\n\u2022   equalities, privacy and common law, such as the Equality Act 2010228 and the Human Rights Act \n1998.229 These laws affect the outcomes of AI systems and decisions which may have discrimination \nand human rights implications, and are the remit of the Equalities and Human Rights Commission. \nPrivacy and common laws may limit the degree to which employers can substitute AI decision-making \nfor their own judgement and places some restrictions on the use of surveillance tools to monitor \nworkers226 to complex issues, reviews proposals to disclose information that is on the list of\nexceptions,28 receives and rules on appeals under this policy statement,29 establishes\nservice fees and service standards, and issues guidelines to staff on policy\nimplementation. The AI Committee has the authority to interpret this Policy in line with\nthe Policy\u2019s guiding principles, and to uphold or reverse prior decisions to deny access,\nwith the exception of decisions made by the Bank\u2019s Board.\nF. Appeals\n36. A requester who is denied access to information by the Bank may file an appeal\nif:\n(a) the requester is able to establish a prima facie case that the Bank has violated\nthis Policy by improperly or unreasonably restricting access to information that\nit would normally disclose under the Policy; or\n(b) the requester is able to make a public interest case to override the Policy\nexceptions that restrict the information requested (limited to those exceptions"}
{"input":"What role does international cooperation play in the governance of AI technologies?","output":"International cooperation is crucial for developing a common understanding of AI, sharing good practices, and creating an evidence base to inform the design, implementation, and evaluation of AI policies.","context":"AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d International co-operation on AI\nInternational co-operation to promote the beneficial use of AI and address its\nchallenges is a priority for many countries. Co-operation for the development\nand adoption of AI and AI governance is being conducted at the bilateral,\nplurilateral, regional and international level. Moreover, many intergovernmental organisations with complementary mandates are engaged in AI\ninitiatives and projects. International co-operation on AI is taking place in fora\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021"}
{"input":"Does the World Bank regularly disclose any information pursuant to its list of exceptions?","output":"Yes, the World Bank regularly declassifies documents using a policy of 5 years for documents classified as \"Official Use Only\" and 20 years for documents classified as \"Confidential\", or \"Strictly Confidential\".","context":"31.\nThe Bank declassifies and discloses\u2014routinely on the Bank\u2019s external website,\nand in response to requests\u2014certain types of restricted information (including\ninformation prepared under earlier disclosure policies of the Bank) as their sensitivity\ndiminishes over time. Some restricted information is not eligible for declassification.\n32.\nInformation Not Eligible for Declassification. The information that is restricted\nfrom disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member Provided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member\ncountries, clients, donors, recipients, or vendors, including consultants) and the financial\nand deliberative information that contains or relates to such information, is not\ndeclassified or made publicly available. Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information?\n\n    \uf0b7    Countries: www.worldbank.org\/countries\n    \uf0b7    Topics: www.worldbank.org\/topics\n    \uf0b7    Projects: www.worldbank.org\/projects\n    \uf0b7    Documents & Reports: www.worldbank.org\/documents\n    \uf0b7    Open Knowledge Repository: openknowledge.worldbank.org\n    \uf0b7    Archives: www.worldbank.org\/archives\n    \uf0b7    Data: data.worldbank.org\n    \uf0b7    Financial data: finances.worldbank.org\n\n                                                                                                                    Go Top\n\nWhen do project documents become available?\n\nThe Bank routinely discloses a wide range of documents through its website as soon as key process milestones have\nbeen passed and the documents have been finalized. The Policy explicitly sets out 10 categories of information that are considered to be exceptions and not eligible for\ndeclassification:\n\n    1.  Personal information\n    2.  Communications of Executive Directors' offices\n    3.  Proceedings of Ethics Committee for Board Officials\n    4.  Information subject to attorney-client privilege\n    5.  Security and safety information\n    6.  Information under separate disclosure regimes and other investigative information\n    7.  Information provided by member countries or third parties in confidence\n    8.  Corporate administrative matters\n    9.  Deliberative information such as emails, notes, memoranda, draft reports, statistical analyses that inform the\n        Bank's internal decision-making processes and individual audit reports.\n    10. Financial Information such as banking or billing information of World Bank Group entities, member countries,\n        clients, donors, recipients, or vendors, including consultants."}
{"input":"What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients?","output":"In the ML-patient interaction case, it is possible to detect biases including: \nPrivilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology\/sensors disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the \u0091haves\u0092 and the \u0091have-nots\u0092 in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves to ensure accurately trained algorithms and those that do not.\nInformed mistrust bias that is given by the patients\u0092 diffidence based on historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data representativeness and accuracy of that group.\nAgency bias (deeply connected to privilege bias): protected groups may not have input into the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.","context":"In the specific domain of healthcare, four categories of biases have been detected as peculiar to the healthcare ML model (Rajkomar et al. 2018):\n\n(1) Biases in healthcare ML depending on model design (such as label biases and cohort biases).\n\n(2) Biases in healthcare ML depending on training data (such as minority bias, missing data bias, informativeness bias and training serving skew).\n\n(3) Biases in healthcare ML produced by the ML interactions with clinicians (such as automation bias, feedback loops, dismissal bias and allocation discrepancy).\n\n(4) Biases in healthcare ML produced by ML interactions with patients (such as privilege bias, informed mistrust and agency bias). In the latter case (i.e. ML\u2013patient interaction), it is possible to detect other kinds of biases, which include the following:\n\n(a)\nPrivilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology\/sensors disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the \u2018haves\u2019 and the \u2018have-nots\u2019 in terms of access to the digital healthcare ecosystem (Morley et al. 2020); in other words, those that generate enough data on themselves to ensure accurately trained algorithms and those that do not (Topol 2019). Fairness in healthcare ML is operationalised in this sense by assessing and re-equilibrating the representativeness of data for protected categories on the basis of which the healthcare ML model is trained and learns to identify patterns, which it uses to produce specific outcomes.\n\nThe last two categories identified above instead concern biases that can arise from the interaction between the healthcare ML model with clinicians or patients. In the former (i.e. ML\u2013clinician interaction), it is possible to distinguish the following biases:\n\n(a) Automation biases that are due to an overreliance of clinicians on the ML model, also caused by clinicians\u2019 unawareness of the inaccuracy of the ML model for a protected group, leading them to act inappropriately on inaccurate predictions. On the one hand, ML capacity to discover probabilistically correlations, find new patterns and thus produce novel knowledge for health is described as the promise of medicine (Hinton 2018; Norgeot et al. 2019; Chin-Yee and Upshur 2019). It already exhibits great potential in several health application fields, from clinical diagnosis (\u00c1lvarez-Machancoses et al. 2019; Fleming 2018), high levels of precision in cancer prediction (Kuo et al. 2001) and diabetes detection (Barakat et al. 2010; Gulshan et al. 2016), to personalised and prevention medicine (Barton et al. 2019), drug discovery (Hay et al. 2013) and epidemiology (Fleming 2018; \u00c1lvarez-Machancoses and Fern\u00e1ndez-Mart\u00ednez 2019). Those who advocate for the use of ML in healthcare indeed stress how ML can fix flaws of clinicians, such as their predisposition towards cognitive biases and hence to commit diagnostic errors, and how ML can increase operational efficiency in the healthcare system by reducing resource waste and increasing"}
{"input":"How might AI affect competition between nations in the future?","output":"If development and competition within the field of Artificial Intelligence largely continues unchecked, it could precipitate a different form of arms race with a wide range of detrimental economic factors as powerful nations attempt to control compute resources and materials necessary to build more complex AI models.","context":"to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response. What have we learned?\nCountries and companies will clash in new ways, and AI could become an actor, not just a factor AI and Geopolitics\nHow Might AI Affect the Rise and Fall of Nations?\n\nPublished Nov 3, 2023\n\nby Barry Pavel, Ivana Ke, Michael Spirtas, James Ryseff, Lea Sabbag, Gregory Smith, Keller Scholl, Domenique Lumpkin\n\nNations across the globe could see their power rise or fall depending on how they harness and manage the development of artificial intelligence (AI). Regardless of whether AI poses an existential risk to humanity, governments will need to develop new regulatory frameworks to identify, evaluate, and respond to the variety of AI-enabled challenges to come. our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing other harms."}
{"input":"How will Singapore contribute to international AI developments? \n","output":"(1) Anchoring key bilateral relationships with selected partners from government and industry, through substantive initiatives and technical cooperation. These will allow Singapore to \u201cstart small and move quickly\u201d, to establish common ground with like-minded partners, as pathfinders to broad-based multilateral cooperation. \n(2) Demonstrating alignment with key international fora and supporting worthwhile platforms. Singapore will support and actively participate in substantive multilateral, multi-stakeholder, or plurilateral initiatives, that seek to achieve an inclusive, practical, and rules-based global environment for AI. \n(3) Sharing Singapore\u2019s experience and curating meaningful partnerships for capacity building. We will actively profile Singapore\u2019s approaches to AI through public engagements and conferences such as Asia Tech x Singapore and SCAI. We are the convenor of the Forum of Small States (FOSS), which now has a digital pillar of engagement. Together with government and industry partners, we will develop AI-related capacity building initiatives to benefit the 108 members of FOSS.\n","context":"Description: The Singapore National AI Strategy was launched in 2019 and\naims to accelerate the development and deployment of AI in Singapore. The\nstrategy adopts a human-centric approach to AI and focuses on an initial five\nnational projects of high social and economic value in logistics, municipal\nservices, healthcare, education and border security. It also outlines plans to\nstrengthen five ecosystem enablers, which include talent and education, data\narchitecture, a progressive and trusted environment, international\ncollaboration, and a \u201ctriple-helix\u201d partnership between the research\ncommunity, industry and government. The strategy also identifies key nontechnological hurdles in AI deployment which Singapore believes it is uniquely\npositioned to address (e.g. stakeholder buy-in, governance, process change)\nand outlines Singapore\u2019s plans to become a global hub for developing, testbedding, deploying, and scaling AI solutions.\nRead more: https:\/\/oecd.ai\/dashboards\/countries\/Singapore. Except for four economies, the Asia-Pacific region is also not well advanced in\nAI. Slower adoption of AI in client countries may have the potential to lead to\nfurther inequality between the rich and the poor nations. To ensure our clients are aware of the potential for AI to advance their\ndevelopment, this note highlights opportunities and risks that need to be managed, which may be of interest to those clients\nseeking to explore AI in their modernization strategies. What have we learned?\nCountries and companies will clash in new ways, and AI could become an actor, not just a factor AI policy intelligence to monitor implementation\nTo evaluate the implementation of their national AI policies and strategies\nsome countries have launched issued annual reports. By July 2020, Canada, the\nUnited Kingdom, the United States, Germany, and the European Union\npublished reports that monitored and evaluated the implementation of their\nAI strategies and highlighted milestones and accomplishments. Singapore had\npublished detailed information on the implementation of its AI strategy.\nMonitoring and evaluation are expected to become more prevalent across\ncountries as national AI strategies move into later stages of implementation.\nSome countries also report more detailed monitoring assessments of the\nimplementation of their AI strategies and policies, including information such\nas budgets, funding, and specific targets. In addition, several national or\nregional institutions have established AI observatories to oversee the"}
{"input":"What are the basic principles of China\u2019s New Generation Artificial Intelligence Development","output":"(1) Technology-Led. Grasp the global development trend of AI, highlight the deployment of forward-looking research and development, explore the layout in key frontier domains, long-term support, and strive to achieve transformational and disruptive breakthroughs in theory, methods, tools, and systems; comprehensively enhance original innovation capability in AI, accelerate the construction of a first-mover advantage, to achieve high-end leading development.\n(2) Systems Layout. According to the different characteristics of foundational research, technological research and development, industrial development, and commercial applications, formulate a targeted systems development strategy. Fully give play to the advantages of the socialist system to concentrate forces to do major undertakings, promote the planning and layout of projects, bases, and a talent pool, organically link already-deployed major projects and new missions, continue current urgent needs and long-term development echelons, construct innovation capacity, create a collaborative force for institutional reforms and the policy environment.\n(3) Market-Dominant. Follow the rules of the market, remain oriented toward application, highlight companies\u2019 choices on the technological line and primary role in the development of commercial product standards, accelerate the commercialization of AI technology and results, and create a competitive advantage. Grasp well the division of labor between government and the market, better take advantage of the government in planning and guidance, policy support, security and guarding, market regulation, environmental construction, the formulation of ethical regulations, etc.\n(4) Open-Source and Open. Advocate the concept of open-source sharing, and promote the concept of industry, academia, research, and production units each innovating and in principal pursuing joint innovation and sharing. Follow the coordinated development law for economic and national defense construction; promote two-way conversion and application for military and civilian scientific and technological achievements and co-construction and sharing of military and civilian innovation resources; form an all-element, multi-domain, highly efficient new pattern of civil-military integration. Actively participate in global research and development and management of AI, and optimize the allocation of innovative resources on a global scale.\n","context":"A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. This is the first in a series of three papers that will tackle these questions using a novel approach: reverse engineering.\nThe approach begins with the finished product: the regulations on AI and algorithms that China has already adopted. The papers will break down the regulations into their component parts\u2014the terminology, concepts, and requirements embedded in them\u2014and then trace those components backward. They will trace their progress through China\u2019s \u201cpolicy funnel\u201d (see figure 1) by examining the political and social roots of the ideas; how they were shaped by CCP ideology, influenced by international AI discourse, and debated by Chinese scholars and companies; and finally formalized by bureaucratic entities. This approach will clarify the specific aims and likely impacts of China\u2019s AI regulations and help to build a conceptual model for how China makes AI policy. Introduction: Over the past two years, China has rolled out some of the world\u2019s first binding national regulations on artificial intelligence (AI). \nThese regulations target recommendation algorithms for disseminating content, synthetically generated images and video, and generative AI systems like OpenAI\u2019s ChatGPT. \nThe rules create new requirements for how algorithms are built and deployed, as well as for what information AI developers must disclose to the government and the public. \nThose measures are laying the intellectual and bureaucratic groundwork for a comprehensive national AI law that China will likely release in the years ahead, a potentially momentous development for global AI governance on the scale of the European Union\u2019s pending AI Act. \nTogether, these moves are turning China into a laborato-ry for experiments in governing perhaps the most impactful technology of this era. 2 AI legislation in China"}
{"input":"The UK Department for Science, Innovation, and Technology (DSIT) released on March 29, 2023 an artificial intelligence (AI) white paper to describe its new approach to regulating AI. Describe the white paper in general.","output":"The proposal seeks to create a pro-innovation regulatory framework that promotes public trust in AI by creating rules proportionate to the risks associated with different sectors\u2019 use of AI. It also commits to establishing a regulatory sandbox to bring together regulators and innovators, so they better understand how regulation affects emerging AI technologies.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. [9]\n\nOECD (2021), Measuring the AI content of publicly funded R&D projects - A proof of\nconcept for the OECD Fundstat initiative, OECD Publishing, Paris,\nhttps:\/\/doi.org\/10.1787\/7b43b038-en.\n\n[3]\n\nOECD (2021), State of implementation of the OECD AI Principles: Insights from\nnational AI policies, https:\/\/doi.org\/10.1787\/1cd40c44-en.\n\n[1]\n\nOECD (2020), Identifying and measuring developments in artificial intelligence,\nOECD Publishing, Paris, https:\/\/doi.org\/10.1787\/5f65ff7e-en.\n\n[10]\n\nOECD (2020), Trustworthy Artificial Intelligence in Health, OECD Publishing, Paris,\nhttps:\/\/www.oecd.org\/health\/trustworthy-artificial-intelligence-in-health.pdf.\n\n[4]\n\nOECD (2020), Using artificial intelligence to help combat COVID-19, OECD\nPublishing, Paris, https:\/\/doi.org\/10.1787\/5b0fd8cd-en.\n\n[5]\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021 Policy implications of artificial intelligence (AI)\nHuman-Centered Artificial Intelligence (HAI).\n19. Jones, E. Explainer: What is a foundation model? Ada Lovelace Institute.\n20. Google Google Generative AI.\nGoogle AI.\n21. Department for Science, Innovation and Technology (2023). AI regulation: a pro- innovation \napproach. GOV.UK.\n22. Department for Science, Innovation and Technology (2023). Bristol set to host UK\u2019s most \npowerful supercomputer to turbocharge AI innovation. GOV.UK.\n23. Foreign, Commonwealth & Development Office et al. (2023). AI Safety Summit 2023. GOV.UK.\n24. Department for Science, Innovation and Technology et al. (2023). The Bletchley Declaration by \nCountries Attending the AI Safety Summit, 1-2 November 2023.\n25. Donelan, M. (2023). Introducing the AI Safety Institute. GOV.UK.\n26. The Alan Turing Institute (online). Data science and AI glossary.\n27. Cabinet Office et al. (2021). Ethics, Transparency and Accountability Framework for Automated Next Steps:\nThe government requested views on certain proposals, including the cross-sectoral principles, by June 21, 2023. The white paper also includes a long list of actions for the UK government to undertake over the coming year and beyond, which include the following:\nPublishing a portfolio of AI assurance techniques:\nPublishing an AI regulation roadmap for the central risk and monitoring functions\nEncouraging regulators to publish guidance on how cross-sectoral principles will apply within their remit\nPublishing a draft central, cross-economy AI risk register for consultation"}
{"input":"At the heart of the EU AI act stands its risk categorization system, what belong to the unacceptable risk systems?","output":"Risk systems include those that have a significant potential for manipulation either through subconscious messaging and stimuli, or by exploiting vulnerabilities like socioeconomic status, disability, or age. AI systems for social scoring, a term that describes the evaluation and treatment of people based on their social behavior, are also banned. ","context":"Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. The Regulatory Framework defines 4 levels of risk in AI:\npyramid showing the four levels of risk: Unacceptable risk; High-risk; limited risk, minimal or no risk\n \nAll AI systems considered a clear threat to the safety, livelihoods and rights of people will be banned, from social scoring by governments to toys using voice assistance that encourages dangerous behaviour.\n\nHigh risk:\nAI systems identified as high-risk include AI technology used in: The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition. The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct."}
{"input":"What was the purpose of developing a set of overarching questions for each dimension of posture assessment in the AI assessment for the Department of Defense?","output":"The set of overarching questions for each dimension of posture assessment served as broad themes in the interview protocols to guide data collection efforts and later formed the basis of the code tree used to analyze the interview notes","context":"U.S. Department of Defense AI Test and Evaluation: Military AI systems must be reliable, secure, and trusted by end-users. Mistakes on the battlefield can have dire consequences. How can DoD create test, evaluation, verification, and validation (TEVV) practices that are appropriate for AI and machine learning systems? What new DoD standards and metrics are needed for AI? This project focuses primarily on direct engagement with the defense community, including military and civilian DoD officials, scientists at DoD research labs, and the defense industry. Department of Defense, the Department of State, other relevant agencies, and the Intelligence Community to address the national security risks and potential benefits posed by AI.  In particular, the memorandum shall: (iv)   Within 365 days of the date of this order, the Secretary of HHS shall, in consultation with the Secretary of Defense and the Secretary of Veterans Affairs, establish an AI safety program that, in partnership with voluntary federally listed Patient Safety Organizations:\n\n               (A)  establishes a common framework for approaches to identifying and capturing clinical errors resulting from AI deployed in healthcare settings as well as specifications for a central tracking repository for associated incidents that cause harm, including through bias or discrimination, to patients, caregivers, or other parties; \n\n               (B)  analyzes captured data and generated evidence to develop, wherever appropriate, recommendations, best practices, or other informal guidelines aimed at avoiding these harms; and\n\n               (C)  disseminates those recommendations, best practices, or other informal guidance to appropriate stakeholders, including healthcare providers. (B)  submit a report to the President that describes the progress of these efforts, including an assessment of the types of AI models that may present CBRN risks to the United States, and that makes recommendations for regulating or overseeing the training, deployment, publication, or use of these models, including requirements for safety evaluations and guardrails for mitigating potential threats to national security."}
{"input":"What are some of the key principles outlined in the White House's Blueprint for an AI Bill of Rights? ","output":"The Blueprint emphasizes principles such as the right to safe automated systems, protection from algorithmic discrimination, data privacy, notice of AI use, and meaningful human alternatives ","context":"Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. Monday\u2019s White House action builds on its previous commitments to the safe and responsible development of AI. Some of these include its support for developing the Blueprint for an AI Bill of Rights, which safeguards Americans\u2019 rights and safety; securing voluntary commitments from Anthropic, Google, OpenAI, and other tech companies to thoroughly test systems before releasing them \u2013 also called \u201cred-teaming\u201d \u2013 and clearly labeling all AI-generated content; and granting $140 million to the National Science Foundation to establish seven new AI research institutes. The Order is also meant to complement the G7 leaders\u2019 Guiding Principles and Code of Conduct, which was also issued on Monday. (See below for more details.) To advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by From Principles to Practice\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, or access to critical needs. (d)  Artificial Intelligence policies must be consistent with my Administration\u2019s dedication to advancing equity and civil rights.  My Administration cannot \u2014 and will not \u2014 tolerate the use of AI to disadvantage those who are already too often denied equal opportunity and justice.  From hiring to housing to healthcare, we have seen what happens when AI use deepens discrimination and bias, rather than improving quality of life.  Artificial Intelligence systems deployed irresponsibly have reproduced and intensified existing inequities, caused new types of harmful discrimination, and exacerbated online and physical harms.  My Administration will build on the important steps that have already been taken \u2014 such as issuing the Blueprint for an AI Bill of Rights, the AI Risk Management Framework, and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies"}
{"input":"Why does Japan not have general, legally-binding regulations for AI use, according to the AI Governance in Japan Ver. 1.1 report?","output":"According to the report, Japan deems legally-binding horizontal requirements for AI systems unnecessary at the moment because of the difficulties in keeping up with the speed and complexity of AI innovation. A prescriptive, static, and detailed regulation could stifle innovation, so Japan prefers respecting companies' voluntary efforts and providing nonbinding guidance for AI governance.","context":"The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. 2 AI legislation in China But in the West, China\u2019s regulations are often dismissed as irrelevant or seen purely through the lens of a geopolitical competition to write the rules for AI. \nInstead, these regulations deserve careful study on how they will affect China\u2019s AI trajectory and what they can teach policymakers around the world about regulating the technology. Even if countries fundamentally disagree on the specific content of a regulation, they can still learn from each other when it comes to the underlying structures and technical feasibility of different regulatory approaches.\nIn this series of three papers, I will attempt to reverse engineer Chinese AI governance."}
{"input":"What is the likely outcome for AI regulation in the United States according to the document?","output":"The likely outcome for AI regulation in the United States is a decentralized, bottom-up approach, resulting in a patchwork quilt of executive branch actions and domain-specific agency actions, rather than a broad national AI law. This approach might be messier and have gaps, but it could also be more grounded in the expertise of specific agencies and tailored to innovation\u200b","context":"Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground This viewpoint proposes ten key parameters of successful AI regulatory design and the likely outcomes. AI raises many profound questions. Nearly all agree that AI promises both huge benefits for society and yet poses major risks. The challenge is getting the balance right between innovation and societal risks \u2013 which governments don't have a good record of achieving. The devil is in the details, making rules adaptable for a technology that is likely to change rapidly and be pervasive. The existential question of Artificial General Intelligence (AGI) is generating much debate. However, this is more distant and tough to forecast accurately. We focus here on issues like model safety, bias, transparency, privacy, security, trust, copyright, content regulation, education, and economic impacts (job loss, work force adjustment, productivity). For commercial actors, the key question is regulation\u2019s impact on innovation, and does it address society\u2019s concerns so that adoption is broad and"}
{"input":"How did the GAO assess the AI inventories of federal agencies?","output":"The GAO reviewed the AI inventories against the requirements for developing an AI inventory outlined in federal guidance, identifying instances where agencies' inventories lacked comprehensive and accurate information for each AI use case.","context":"offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these objectives, agencies shall also consider opportunities to increase coordination, communication, and engagement about AI as appropriate with community-based organizations; civil-rights and civil-liberties organizations; academic institutions; industry; State, local, Tribal, and territorial governments; and other stakeholders. (ii)    the Chief Artificial Intelligence Officers\u2019 roles, responsibilities, seniority, position, and reporting structures;\n\n          (iii)   for the agencies identified in 31 U.S.C. 901(b), the creation of internal Artificial Intelligence Governance Boards, or other appropriate mechanisms, at each agency within 60 days of the issuance of the guidance to coordinate and govern AI issues through relevant senior leaders from across the agency; (ix)    requirements for public reporting on compliance with this guidance.\n\n     (c)  To track agencies\u2019 AI progress, within 60 days of the issuance of the guidance established in subsection 10.1(b) of this section and updated periodically thereafter, the Director of OMB shall develop a method for agencies to track and assess their ability to adopt AI into their programs and operations, manage its risks, and comply with Federal policy on AI.  This method should draw on existing related efforts as appropriate and should address, as appropriate and consistent with applicable law, the practices, processes, and capabilities necessary for responsible AI adoption, training, and governance across, at a minimum, the areas of information technology infrastructure, data, workforce, leadership, and risk management.  \n\n     (d)  To assist agencies in implementing the guidance to be established in subsection 10.1(b) of this section: (e)  To improve transparency for agencies\u2019 use of AI, the Director of OMB shall, on an annual basis, issue instructions to agencies for the collection, reporting, and publication of agency AI use cases, pursuant to section 7225(a) of the Advancing American AI Act.  Through these instructions, the Director shall, as appropriate, expand agencies\u2019 reporting on how they are managing risks from their AI use cases and update or replace the guidance originally established in section 5 of Executive Order 13960.\n\n     (f)  To advance the responsible and secure use of generative AI in the Federal Government:"}
{"input":"What role do whistleblowers and complaints play in the AI regulatory process?","output":"Whistleblowers and individual complaints are crucial for uncovering specific insights or harms related to algorithmic systems, offering unique information that can lead to formal investigations and further regulatory action against non-compliant or harmful AI practices.","context":"scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms. Accountability and governance: Businesses should put in place governance measures that ensure effective oversight of the supply and use of AI systems, with clear lines of accountability established across the AI lifecycle. Regulators will likely implement this principle through regulatory guidance and assurance techniques.\nContestability and redress: Users, impacted third parties, and actors in the AI lifecycle should be able to contest an AI decision or outcome that is harmful or creates material risk of harm. Regulators will be expected to clarify existing routes to contestability and redress and implement proportionate measures to ensure that the outcomes of AI use are contestable where appropriate. The government\u2019s initial non-statutory approach will not create new rights or new routes to redress at this stage. Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm. that might be less discriminatory instead of simply detecting biased results. Further,regulators can encourage and highlight AI assurance companies that establishprocesses which enable some degree of independent scrutiny, such as with consistentevaluation standards, although this is challenging to do when AI assurance companiesdepend on AI developers for revenue.\nRegulators should also welcome information from affected individuals andwhistleblowers from AI developers\u2014both of whom may have unique information aboutalgorithmic systems.\nIndividuals who are subjected to algorithmic systems may have specific insight intothe function of those systems. Several U.S. agencies, such as the Equal EmploymentOpportunity Commission, explicitly"}
{"input":"What measures are proposed to support American workers in the context of AI development?","output":"The order emphasizes adapting job training and education to the AI age, ensuring AI does not undermine worker rights, and engaging workers and unions in AI development discussions.","context":"(c)  The responsible development and use of AI require a commitment to supporting American workers.  As AI creates new jobs and industries, all workers need a seat at the table, including through collective bargaining, to ensure that they benefit from these opportunities.  My Administration will seek to adapt job training and education to support a diverse workforce and help provide access to opportunities that AI creates.  In the workplace itself, AI should not be deployed in ways that undermine rights, worsen job quality, encourage undue worker surveillance, lessen market competition, introduce new health and safety risks, or cause harmful labor-force disruptions.  The critical next steps in AI development should be built on the views of workers, labor unions, educators, and employers to support responsible uses of AI that improve workers\u2019 lives, positively augment human work, and help all people safely enjoy the gains and opportunities from technological innovation. Sec. 6.  Supporting Workers.(a)  To advance the Government\u2019s understanding of AI\u2019s implications for workers, the following actions shall be taken within 180 days of the date of this order:\n\n          (i)   The Chairman of the Council of Economic Advisers shall prepare and submit a report to the President on the labor-market effects of AI.\n\n          (ii)  To evaluate necessary steps for the Federal Government to address AI-related workforce disruptions, the Secretary of Labor shall submit to the President a report analyzing the abilities of agencies to support workers displaced by the adoption of AI and other technological advancements.  The report shall, at a minimum: (ii)  The Secretary of State and the Administrator of the United States Agency for International Development, in collaboration with the Secretary of Energy and the Director of NSF, shall develop a Global AI Research Agenda to guide the objectives and implementation of AI-related research in contexts beyond United States borders.  The Agenda shall:\n\n               (A)  include principles, guidelines, priorities, and best practices aimed at ensuring the safe, responsible, beneficial, and sustainable global development and adoption of AI; and\n\n               (B)  address AI\u2019s labor-market implications across international contexts, including by recommending risk mitigations. (A)  assess how current or formerly operational Federal programs designed to assist workers facing job disruptions \u2014 including unemployment insurance and programs authorized by the Workforce Innovation and Opportunity Act (Public Law 113-128) \u2014 could be used to respond to possible future AI-related disruptions; and\n\n               (B)  identify options, including potential legislative measures, to strengthen or develop additional Federal support for workers displaced by AI and, in consultation with the Secretary of Commerce and the Secretary of Education, strengthen and expand education and training opportunities that provide individuals pathways to occupations related to AI.\n\n     (b)  To help ensure that AI deployed in the workplace advances employees\u2019 well-being:"}
{"input":"What are the Universal Guidelines for Artificial Intelligence endorsed by EPIC?","output":"The Universal Guidelines include rights to transparency, human determination, and fairness, obligations for identification, assessment, accountability, accuracy, data quality, public safety, and cybersecurity, and prohibitions on secret profiling and unitary scoring.","context":"In April 2023, EPIC Senior Counsel Ben Winters and Equal Justice Works Fellow Grant Fergusson published a series of blog posts explaining how each of these instruct how entities using AI can do it more responsibly.\n\nUniversal Guidelines for Artificial Intelligence\nIn October 2018, over 250 experts and 60 organizations, representing more than 40 countries, endorsed the Universal Guidelines for Artificial Intelligence (\u201cUGAI\u201d). The guidelines were organized by the Public Voice. The guidelines in full are: EPIC.ORG AI Policy\n\nBACKGROUND\nAI policy must protect individuals, increase transparency and accountability, and center human rights.\n\nStates and municipalities are increasingly taking interest in Artificial Intelligence and filling the gaps left by federal inaction on algorithmic harm. States and cities have taken different routes, from notification and task forces to minimum privacy standards. Some of the recent efforts are highlighted below. EPIC is not including state and local laws primarily focused on investing resources in building more AI and general research, as they do not improve protection of individuals against algorithmic harm. At the federal level, EPIC is only including laws of this sort that have been enacted, and only including laws within the last several years. Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. For government use of AI, the EO requires the development of guidance for agency use of AI and a faster and more efficient process for agencies to procure AI products and services. The directive also calls for the rapid hiring of AI professionals and the training of employees at all levels. The content of a forthcoming Office of Management and Budget memo will dictate the details of how government AI use will change.\n\nEPIC has long advocated for comprehensive privacy protections, rigorous testing protocols, expanded resources for evaluation of AI systems, and a government-whole effort to fighting algorithmic discrimination.\n\nTwo days later, on November 1st, 2023, the Office of Management and Budget released a memo dictating how Government Agencies should approach using and regulating AI. There is a 30-day comment period before it\u2019s finalized."}
{"input":"What role do trade agreements play in AI governance?","output":"Free trade agreements (FTAs) and DEAs include provisions that enhance access to AI and support its governance, such as commitments to cross-border data flows and avoiding data localization requirements. They also include AI-specific commitments focused on cooperation and alignment in areas like AI standards.","context":"AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC. Domain Specific Outcomes \n\nTrade conflict emerges with the EU and others, creating a fragmented global regulatory environment. The United States likely globally advances its AI principles and voluntary business commitments to influence international rules (vs. China and EU approaches). IPR and copyright will be especially important for creative industries. The EU\u2019s AI Act will likely influence other major nations, frustrating U.S. leadership. Leading companies like Amazon, Apple, Google, Meta, Microsoft, and Nvidia likely face multiple AI regimes around the world.   \n\nAntitrust agencies lead the effort to forestall \u201cbig tech\u201d companies dominating AI, false and deceptive practices, and AI-driven fraud. The high cost and scale of AI foundation models likely leads to market concentration. We expect FTC actions designed as warnings shots to industry."}
{"input":"How does the AIA affect AI systems in regulated products?","output":"AI systems in regulated products sold in the EU will need to meet new requirements, such as a risk management process, higher data standards, and human oversight. This may lead to global conformity as companies adapt to these rules to access the EU market.","context":"Although this industry is distinct from governance, regulators should actively engagewith the AI assurance industry to advance democratic goals, perhaps best\n. Regulators can issue guidance that encourages regulated companies toconsider using AI assurance tools, even possibly noting this could be interpreted as apotential signal of regulatory compliance. Further, regulators can inform and learn fromthe AI assurance industry. By communicating about specific technical functions andthe societal impacts of algorithmic systems in a regulated field, regulators can help AIassurance companies strive towards not just nominal compliance, but meaningfullybetter outcomes. For instance, regulators concerned with discrimination couldencourage relevant AI assurance companies to Accountability and governance: Businesses should put in place governance measures that ensure effective oversight of the supply and use of AI systems, with clear lines of accountability established across the AI lifecycle. Regulators will likely implement this principle through regulatory guidance and assurance techniques.\nContestability and redress: Users, impacted third parties, and actors in the AI lifecycle should be able to contest an AI decision or outcome that is harmful or creates material risk of harm. Regulators will be expected to clarify existing routes to contestability and redress and implement proportionate measures to ensure that the outcomes of AI use are contestable where appropriate. The government\u2019s initial non-statutory approach will not create new rights or new routes to redress at this stage. Leverage the AI Assurance Industry 4.\nthereproducibility that is necessary\nA common strand across the entire AI assurance industry is a mixed business modelthat advertises both profit-motivated improvements to algorithmic systems and betterpreparedness for regulatory and legal compliance. For instance, several AI assurancecompanies stress the value of internal monitoring, so corporate leaders canunderstand and scrutinize the function of their own algorithms, in addition tohighlighting future legal requirements. This likely a stronger sales pitch to potentialclients, especially given that most AI laws are still being drafted, rather than beingimplemented.\nAlthough this industry is distinct from governance, regulators should actively engagewith the AI assurance industry to advance democratic goals, perhaps best \u201cCarrots\u201d encouraging \u201cgood AI\u201d outcomes and \u201csticks\u201d for enforcement and penalties. Regulation often works best when it uses carrots (incentives, safe harbors) as well as sticks (penalties). Google and others focus on \u201cgood AI\u201d outcomes such as an AI-ready workforce, investing in AI innovation and competitiveness, and supporting broad AI adoption. The EU AI Act takes a strong enforcement approach and has big financial penalties. U.S. AI efforts have been mostly voluntary, such as the White House July 21, 2023 commitments from AI companies.  \n\nScope. AI supply chain including cloud data centers. For high risk uses, some want regulation to apply not just to the model developer, but to applications and the IT infrastructure it runs on, even including components. For example, AI systems that support critical infrastructure would be deployed only in licensed AI data centers. This potentially expands regulation to new areas."}
{"input":"What is the significance of a common definition of AI for regulatory purposes?","output":"A common definition of AI for regulatory purposes is important for guiding future cooperation in AI and determining the level of ambition that can be reached by such a process. It helps in converging on a common understanding of AI and its configurations and techniques","context":"Defining AI: There will be no legal definition of AI. Instead, \u201cAI\u201d is defined by reference to the combination of two characteristics: (1) adaptivity\u2014i.e., being \u201ctrained\u201d and operating by inferring patterns and connections in data which are not easily discernible to humans; and (2) autonomy\u2014i.e., making decisions without the express intent or ongoing control of a human. Defining AI with reference to functional capabilities is intended to future-proof the framework against unanticipated new technologies that are autonomous and adaptive.\nContext-specific \u2013 Regulating use, not technology: The framework will regulate outcomes that AI is likely to generate. This approach could even extend to failure to use AI; the government highlighted feedback received from regulators that failure to exploit AI capabilities may in fact risk harm, such as not utilizing AI in safety-critical processes. I. The AI Regulation Debate: A Wide Spectrum of Views. The AI regulation debate has become heated and politicized, reflecting the immense hopes and fears that we have invested in AI.   \n\nHysterical fear and paranoia. On one end, leading technologist and venture capitalist Marc Andreessen  says \u201cwe have a full-blown moral panic about AI right now.\u201d He views this as \u201chysterical fear\u201d that is \u201cirrational\u201d and compares it to earlier introduction of new technologies that were similarly feared but were overblown and subsequently managed. He believes that AI is simply a computer program that is owned, controlled and used by people, and that AI does not have its own goals or its own wants. Moreover, he emphasizes, AI can be a force for tremendous good. This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":"What are the economic, ethical, and strategic issues that call into question the continuation of collaboration on AI with China?","output":"Issues include the ethical use of AI, strategic concerns about China's growing capabilities and ambitions in AI, and the economic implications of collaboration for global competitiveness","context":"boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory of, China\u2019s AI strategy. It emphasises that the Chinese government is aware of the potential benefits, practical risks, and the ethical challenges that AI presents, and that the direction of China\u2019s AI strategy will largely be determined by the interplay of these factors and by the extent to which government\u2019s interests may outweigh ethical concerns. Section 5 concludes the paper by summarising the key findings of our analysis. structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible China is a central actor in the international debate on the development and governance of AI. It is important to understand China\u2019s internal needs, ambitions in the international arena, and ethical concerns, all of which are shaping the development of China\u2019s AI policies. It is also important to understand all this not just externally, from a Western perspective, but also internally, from a Chinese perspective. However, some ethical safeguards, constraints and desiderata are universal and are universally accepted and cherished, such as the nature and scope of human rights.Footnote15 They enable one to evaluate, after having understood, China\u2019s approach to the development of AI. This is why in this article we have sought to contribute to a more comprehensive and nuanced analysis of the structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are To provide this contextualised understanding, Sect. 2 maps relevant AI legislation in China. We argue that, although previous policy initiatives have stated an intent to develop AI, these efforts have been fractious and viewed AI as one of many tools in achieving a different set goal. In contrast, the AIDP is the first national-level legislative effort that focuses explicitly on the development of AI as a unified strategy. Following this, Sect. 3 analyses the interventions and impact of the AIDP on three strategic areas identified in the document, namely: international competition, economic growth, and social governance. Section 4 focuses on China\u2019s aim to develop ethical norms and standards for AI. There we argue that, although the debate is in its early stages, the desire to define normative boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory"}
{"input":"What role does education play in the framework for ethical AI at the UN?","output":"Education is seen as pivotal in the framework, with the recommendation to establish an educational program for UN staff. This program aims to enhance awareness and understanding of ethical AI principles, ensuring that staff are well-equipped to develop, implement, and use AI technologies responsibly.","context":"\u201cPrincipled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI\u201d\nBerkman Klein Center Research Publication No. 2020-1\nhttps:\/\/ssrn.com\/abstract=3518482 or http:\/\/dx.doi.org\/10.2139\/ssrn.3518482\nFleming, Sean. 2018. \u201cIn Rwanda, High-Speed Drones are Delivering Blood to Remote Communities.\u201d World Economic Forum.\nhttps:\/\/www.weforum.org\/agenda\/2018\/12\/in-rwanda-drones-are-delivering-blood-to-remote-communities\/\nFloridi, Luciano, and others. 2018. \u201cAI4People\u2014An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.\u201d Minds & Machines 28 (4): 689\u2013707.\nhttps:\/\/doi.org\/10.1007\/s11023-018-9482-5 software & knowledge; regulation, testbeds and documentation; skills and\nlabour markets; and international co-operation.\nRead more: https:\/\/oecd.ai.\nUNESCO\u2019s Ad Hoc Expert Group on AI (AHEG)\nResponsible entity: UNESCO\nDescription: UNESCO has organised events to exchange knowledge about AI,\nfocusing on the dimensions of ethics, policy and capacity building. In March\n2020, UNESCO appointed 24 leading experts to an Ad Hoc Expert Group (AHEG)\non the ethics of AI. UNESCO\u2019s November 2019 General Conference tasked the\nAHEG with elaborating a recommendation on the ethics of artificial intelligence.\nRead more: https:\/\/en.unesco.org\/artificial-intelligence. Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors. emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to establish ethical principles but did form an AI ethics committee in mid-2018 with this purpose in mind (\u201cAI association to draft ethics guidelines\u201d 2019)."}
{"input":"What role has the World Bank played in supporting AI in the public sector?","output":"The World Bank launched the GovTech Global Partnership to aid public sector modernization through technology, providing financial support, expertise, and an innovation lab for exploring AI applications and ensuring ethical use.","context":"What has the World Bank done to support\nits client countries?\nThe World Bank launched the GovTech Global Partnership in 2019 to support the\nmodernization of client governments through the use of technology. GovTech is\na whole-of-government approach to public sector modernization that promotes\nsimple, accessible, and efficient government. It aims to promote the use of technology to transform the public sector, improve service delivery to citizens and\nbusinesses, and increase efficiency, transparency and accountability. To better\nunderstand the role AI can play in public sector transformation, the World Bank\nproduced a series of papers in partnership with the Swiss State Secretariat for\nEconomic Affairs. One of these is Artificial Intelligence in the Public Sector, which\naims to distill the existing knowledge on the use of AI in the public sector and to\nsummarize the lessons learned from early adopters. The GovTech team also 11\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\nWorld Bank Support\n\u2022 Compiled international experience\nthrough a report on \u2018AI in Public\nSector\u2019\n\u2022 Financial Support \u2013 launched\na call-for-proposals to provided\naccess to funds to the task teams\nfor GovTech initiatives, including AI\n\u2022 GovTech team accessible to\ntask teams for brainstorming and\nsupport\n\u2022 Project-level funding to pilot\n\u2022 Innovation Lab under ITS offers\nguidance, sandbox\n\u2022 Industry expertise network for deep\ndives\n\n\fWhat can policymakers do?\n\u2022 8\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nThe typology of AI use cases in the public sector includes:\n\u2022\n\nCITIZEN ENGAGEMENT. The introduction of AI tools such as chatbots that\nanswer citizen queries. For example, how can I apply for social welfare\nbenefits? Additionally, aggregation and pattern determination can be used\nto collect feedback from millions of citizens, for example on a draft policy or\nlegislation published online for public consultations.\n\n\u2022\n\nCITIZEN ENGAGEMENT: AI was able to analyze citizen sentiments from 21\nmillion comments received on the US Government\u2019s policy on NetNeutrality shared online;\n\n\u2022\n\nCOMPLIANCE AND RISK MANAGEMENT: In Armenia, AI helped the revenue agency increase its revenue by 6 times. The World Bank supported\nthe underlying technologies for this project. For more details, please watch\nthe World Bank hosted event.\n\n\u2022 4\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fHow can countries address these challenges?\nAI policy incorporating legislative foundations, institutional arrangements and a\ngovernance framework for the use of AI could provide the necessary enabling\nenvironment. This could be part of a broader Digital Transformation Policy or a\nstand-alone framework.\nA central innovation hub for AI in government could help pool scarce human\nand financial resources to support the initiatives of line ministries and start to\ndevelop a whole-of-government approach. In the use cases examined here,\nmost governments have already set-up the main hub for AI issues that serves\nas a central authority over AI projects being delivered by line agencies.\nThe AI hub\u2019s role could also be to connect industry expertise to the government\ndepartments and line agencies, to promote research, and build alliances with\nacademic institutions and the private sector."}
{"input":"How does the World Bank handle declassification of information?","output":"The policy outlines a schedule for declassifying and making available certain types of restricted information over time, considering the diminishing sensitivity of the information.","context":"The Policy explicitly sets out 10 categories of information that are considered to be exceptions and not eligible for\ndeclassification:\n\n    1.  Personal information\n    2.  Communications of Executive Directors' offices\n    3.  Proceedings of Ethics Committee for Board Officials\n    4.  Information subject to attorney-client privilege\n    5.  Security and safety information\n    6.  Information under separate disclosure regimes and other investigative information\n    7.  Information provided by member countries or third parties in confidence\n    8.  Corporate administrative matters\n    9.  Deliberative information such as emails, notes, memoranda, draft reports, statistical analyses that inform the\n        Bank's internal decision-making processes and individual audit reports.\n    10. Financial Information such as banking or billing information of World Bank Group entities, member countries,\n        clients, donors, recipients, or vendors, including consultants. 31.\nThe Bank declassifies and discloses\u2014routinely on the Bank\u2019s external website,\nand in response to requests\u2014certain types of restricted information (including\ninformation prepared under earlier disclosure policies of the Bank) as their sensitivity\ndiminishes over time. Some restricted information is not eligible for declassification.\n32.\nInformation Not Eligible for Declassification. The information that is restricted\nfrom disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member by the Bank\u2019s Board of Executive Directors.\n34 See paragraph 36 (a).\n35 See paragraph 36 (b).\n17\ninitial decision to deny access to the information requested, the notice specifies the\nreasons. If the AI Committee or Appeals Board reverses the decision to deny access, the\nrequester is notified of the decision and of the process for making the information\navailable to the requester.\n18\nANNEX\nDECLASSIFICATION SCHEDULE FOR CERTAIN HISTORICAL DOCUMENTS\nCREATED BEFORE JULY 1, 2010\nPursuant to paragraph 33 of the policy statement, the final documents listed in this\nAnnex\u2014if they were created before July 1, 2010, and were not already made public\npursuant to earlier disclosure policies of the World Bank\u2014are eligible for\ndeclassification according to the following schedule:\n(a) 5 years after the date of a document classified as \u201cOfficial Use Only,\u201d and\n(b) 20 years after the date of a document classified as \u201cConfidential,\u201d or \u201cStrictly\nConfidential.\u201d Provided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member\ncountries, clients, donors, recipients, or vendors, including consultants) and the financial\nand deliberative information that contains or relates to such information, is not\ndeclassified or made publicly available."}
{"input":"How did the G7 leaders contribute to AI governance?","output":"G7 leaders issued international Guiding Principles and a voluntary Code of Conduct for AI developers, focusing on democratic values, human rights, and challenges like climate crisis and global health.","context":"The Path to Trustworthy AI G7 Outcomes and Implications for Global AI Governance\n\nCommentary by Hiroki Habuka\n\nPublished June 6, 2023\n\nIntroduction\nThe G7 Summit, held from May 19 to 21, 2023, left a strong impression of unity among global leaders. Emerging technology such as artificial intelligence (AI), along with national security and energy, was highlighted as a key area requiring a strong alliance among G7 countries toward fundamental values such as democracy and human rights. The G7 Digital and Tech Ministers\u2019 Meeting, which took place a little earlier, also discussed responsible AI and global AI governance as one of the central topics. With the recent launch of large language AI models such as GPT-4, society anticipates a rapid integration of AI technologies, making it crucial to discuss the responsible use of AI and its governance. However, even the like-minded G7 countries approach AI governance differently, ranging from strict, comprehensive laws to sector-specific guidelines. Because of this, the consensus among G7 leaders on trustworthy AI will have a big impact on the way AI systems should be developed and operated around the world. This commentary takes a close look at two important outcome documents from the 2023 G7 summit that broach the subject of AI governance: the G7 Summit Communiqu\u00e9 and the Declaration of the G7 Digital and Tech Ministers\u2019 Meeting\u2014 collectively referred to as the G7 statements\u2014to predict potential cooperation on AI governance among the G7 countries.\n\nShared Understanding of Values, Principles, and Risks\nEstablishing Common Values The European Union (EU) Commission embraced the G7 leaders\u2019 agreement and swiftly issued a call to action. \u201cI am pleased to welcome the G7 international Guiding Principles and the voluntary Code of Conduct, reflecting EU values to promote trustworthy AI. I call on AI developers to sign and implement this Code of Conduct as soon as possible,\u201d said Commission President Ursula von der Leyen in a press release. (Later this week, Von der Leyen will be joining other global leaders in Buckinghamshire for the two-day UK AI Safety Summit. More details below.) G7 Leaders issue \u2018Guiding Principles\u2019 and voluntary \u2018Code of Conduct\u2019 on AI\nOn Monday, Group of Seven (G7) leaders issued a set of international Guiding Principles on artificial intelligence as well as a voluntary Code of Conduct for AI developers under the \u2018Hiroshima Artificial Intelligence Process.\u2019 In a joint statement, leaders called on organizations developing advanced AI systems to commit to the code of conduct as well as ministers to accelerate development of the Process\u2019 policy framework.\n\nThe Principles and Code are the culmination of work that began in May 2023 at the G7 summit, where the Hiroshima AI Process \u2013 born out of the EU-US Trade and Technology Council ministerial \u2013 was formed to find common policy priorities and promote guardrails for advanced AI systems globally. The G7 bloc includes Canada, France, Germany, Italy, Japan, Britain, the US, and the European Union."}
{"input":"What is a potential alternative to creating a centralized algorithmic regulatory agency?","output":"The Critical Algorithmic System Classification (CASC) approach is an alternative that would empower covered agencies to issue and enforce regulations on ADSs in critical socioeconomic determinations within each agency\u2019s statutory authority and grant administrative subpoena authority for covered agencies to investigate and audit ADSs that affect processes related to each covered agency\u2019s statutory authority.","context":"Governments around the world are implementing foundational policies to regulateartificial intelligence (AI) and algorithmic systems more generally. While legislation isadvancing, regulators should not wait idly for legislators to act. Instead, regulatorsshould be actively learning about the algorithmic systems in their regulatory domainand evaluating those systems for compliance under existing statutory authority.\nMany regulatory agencies have started this work, including the U.S. Federal TradeCommission\u2019s (FTC)\nand Consumer Financial Protection Bureau(CFPB), new algorithmic regulators in\nand\n, and onlineplatform regulators such as the UK\u2019s\n(OFCOM) and the\n. These agencies and others havestarted to implement novel approaches and policies for AI regulation.\nOffice of Technology\nthe Netherlands\nSpain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023 Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm. Agencies should use the tools they have to understandand regulate AI\nRegulators should actively consider what steps are necessary and valuable in theirdomains to ensure their regulatory mission is preserved. This includes cataloging andobserving emerging uses of algorithmic systems in their field, exploring what theirexisting statutory authority allows for, and hiring staff with expertise in algorithmicsystems. Regulators may benefit from a gap analysis\u2014identifying where current\ndocumented\nthousands ofpages\nled to congressional hearings\noffer cash rewards for whistleblowers\nauthorities and capacities are lacking so that they can inform legislators, who are farless likely to understand the nuances of every regulatory subfield.\nWhile regulators may often lack the most appropriate and best suited tools forinformation gathering about algorithmic systems, many will have some authority toperform information gathering. Beyond the interventions explored here, regulators canalso learn from unique to their circumstances (https:\/\/www.brookings.edu\/articles\/acomprehensive-\nand-distributed-approach-to-ai-regulation\/)\n. As a result, regulatorsneed to build robust and persistent strategies to gather information for informedpolicymaking, oversight, and enforcement actions. Collectively, the emerging efforts ofthese agencies will continue to compose a regulatory toolkit upon which much futureAI governance will be built."}
{"input":"How is AI related to the United States' geopolitical relations with China? ","output":"The U.S.-China relationship looms large over AI governance: as Beijing pursues a national strategy aimed at making China the global leader in \u201cAI theories, technologies, and applications\u201d by 2030, policymakers in Washington are struggling with how to place guardrails around AI development without undermining the United States\u2019 technological edge.","context":"In this article, we analysed the nature of AI policy within China and the context within which it has emerged, by mapping the major national-level policy initiatives that express the intention to utilise AI. We identified three areas of particular relevance: international competitiveness, economic growth, and social governance (construction). The development and deployment of AI in each of these areas have implications for China and for the international community. For example, although the \u2018trump-card\u2019 policy to gain a military advantage may not be something new, its application to AI technologies risks igniting an arms race and undermining international stability (Taddeo and Floridi 2018). Efforts to counteract this trend seem largely hollow. Our analysis indicates that China has some of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions 3 China\u2019s AI strategic focus AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the perceived risks present in each of these areas. The AIDP highlights three areas where AI can make a substantial difference within China: international competition, economic development, and social governance. They are strictly interrelated but, for the sake of clarity, we shall analyse them separately, and contextualise each of them by discussing the relevant literature surrounding the broader political backdrop and contemporary policy debates. structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible"}
{"input":"What happens if companies don't comply the EU AI act regulations?","output":"Non-compliance with the rules can lead to fines ranging from 35 million euro or 7% of global turnover to 7.5 million or 1.5 % of turnover, depending on the infringement and size of the company.","context":"A broad-based national AI law like the EU Act is unlikely over the next few years. The administration likely builds on its AI \u201cbill of rights\u201d that spans different sectors, voluntary commitments, and executive orders. Voluntary rules are seen by many as a stop-gap measure, but a divided Congress is unlikely to pass a major law with new mandatory rules.  \n\nActions addressing sensitive areas such as health care, financial services (lending, insurance, housing), work force practices (discrimination), and child safety are more likely. Executive branch agencies are likely to move forward with existing authorities in the absence of new laws. \n\nPrivate tech companies advance their own responsible AI initiatives to serve their customers and avoid tougher government action. These are largely voluntary and seen as insufficient. Shaping Europe\u2019s digital future - AI Act\n\nThe Commission is proposing the first-ever legal framework on AI, which addresses the risks of AI and positions Europe to play a leading role globally.\nThe regulatory proposal aims to provide AI developers, deployers and users with clear requirements and obligations regarding specific uses of AI. At the same time, the proposal seeks to reduce administrative and financial burdens for business, in particular small and medium-sized enterprises (SMEs).\n\nThe proposal is part of a wider AI package, which also includes the updated Coordinated Plan on AI. Together, the Regulatory framework and Coordinated Plan will guarantee the safety and fundamental rights of people and businesses when it comes to AI. And, they will strengthen uptake, investment and innovation in AI across the EU. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA). The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition."}
{"input":"Do you own your own image in the UK?","output":"No, in the UK, there is no specific legal right to own your image. However, various legal avenues such as privacy legislation, laws concerning misrepresentation, contract law, and other intellectual property rights may offer some level of protection for individuals who seek to control the use of their image. Nonetheless, some legal scholars argue that the existing legal framework may not adequately safeguard individuals' images, particularly in the context of AI.","context":"recreate the likeness of living or dead performers in perpetuity,180,181 with implications for what \nfair remuneration for performers looks like.*\nIn the United States and many EU member states there exists a legal right to own your image.182,183 \nThere is no such right in the UK. However, privacy legislation, laws around misrepresentation, \ncontract law, and other intellectual property rights could provide some protection for people who \nwish to control the use of their image.183\nSome legal academics argue that the current body of law is not sufficient to protect\npeople\u2019s image in the context of AI.184\nThe Government has committed to ratifying the Beijing Treaty on Audio-visual Performances\u2020, which \nwould give intellectual property rights to performers.185\nIntellectual property\nGenerative AI tools are trained using datasets, which may or may not be open to the public (PB 57). \nGenerative AI tools can output written, visual, aural, or audio-visual works that can mimic the data consisting of the use of personal data to evaluate certain personal aspects relating to a \nnatural person, in particular to analyse or predict aspects concerning that natural person's \nperformance at work, economic situation, health, personal preferences, interests, reliability, \nbehaviour, location or movements\u201d,163 with profiling in the policing context referring to the aim \nof predicting an individual\u2019s propensity to crime.\n8                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nImage ownership\nAI can be used to recreate voices and images imitating living or deceased individuals.178 This can \nhave benefits in the arts, such as using AI for the consensual de-aging of performers.179\nOn the other hand, creative sector trade unions have raised concerns around companies being able to \nrecreate the likeness of living or dead performers in perpetuity,180,181 with implications for what make \u201bcompanies\u2014and regulators\u2014wary of overseas listings, which would entail greater disclosure and scrutiny\u2019 (Lucas 2019). Although this, and other similar practices, do involve anonymisation, they are in stark contrast with the European and US approaches to the use of medical data, which prioritise individual autonomy and privacy, rather than social welfare. A fair balance between individual and societal needs is essential for an ethical approach to personal data, but there is an asymmetry whereby an excessive emphasis on an individualistic approach may be easily rectified with the consensus of the individuals, whereas a purely societal approach remains unethical insofar as it overrides too easily individual rights and cannot be rectified easily. in Artificial Intelligence, Technical University of Munich.\n150. Mobilio, G. (2023). Your face is not new to me \u2013 Regulating the surveillance power of facial \nrecognition technologies. Internet Policy Rev., Vol 12.\n151. Ovide, S. (2020). A Case for Banning Facial Recognition. The New York Times.\n152. Murray, D. (2019). Live facial recognition: the impact on human rights and participatory \ndemocracy. University of Essex.\n153. Minderoo Centre for Technology and Democracy (2022). A Sociotechnical Audit: Assessing Police \nuse of Facial Recognition. mctd.ac.uk.\n154. Ada Lovelace Institute (2019). Beyond face value: public attitudes to facial recognition \ntechnology.\n155. Big Brother Watch Team (2023). 65 parliamentarians call for\n\u201cimmediate stop\u201d to live facial\n21                                  POSTnote 708 - 9 January 2024"}
{"input":"What are some criticisms or concerns regarding the AI Act?","output":"Rights groups are concerned about exemptions, lack of protection for AI systems used in migration and border control, and loopholes that may allow developers to opt out of high-risk classifications.","context":"I. The AI Regulation Debate: A Wide Spectrum of Views. The AI regulation debate has become heated and politicized, reflecting the immense hopes and fears that we have invested in AI.   \n\nHysterical fear and paranoia. On one end, leading technologist and venture capitalist Marc Andreessen  says \u201cwe have a full-blown moral panic about AI right now.\u201d He views this as \u201chysterical fear\u201d that is \u201cirrational\u201d and compares it to earlier introduction of new technologies that were similarly feared but were overblown and subsequently managed. He believes that AI is simply a computer program that is owned, controlled and used by people, and that AI does not have its own goals or its own wants. Moreover, he emphasizes, AI can be a force for tremendous good. Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI\nPress Release 09-12-2023 - 00:04 \nSummary:\n1. Safeguards agreed on general purpose artificial intelligence\n2. Limitation for the of use biometric identification systems by law enforcement\n3. Bans on social scoring and AI used to manipulate or exploit user vulnerabilities\n4. Right of consumers to launch complaints and receive meaningful explanations\n5. Fines ranging from 35 million euro or 7% of global turnover to 7.5 million or 1.5% of turnover International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d."}
{"input":"What is the primary goal of the Artificial Intelligence Act according to the European Parliament?","output":"The Act aims to ensure AI in Europe is safe and respects fundamental rights and democracy while enabling businesses to thrive and expand.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Introduction: MEPs reached a political deal with the Council on a bill to ensure AI in Europe is safe, respects fundamental rights and democracy, while businesses can thrive and expand. On Friday, Parliament and Council negotiators reached a provisional agreement on the Artificial Intelligence Act. This regulation aims to ensure that fundamental rights, democracy, the rule of law and environmental sustainability are protected from high risk AI, while boosting innovation and making Europe a leader in the field. The rules establish obligations for AI based on its potential risks and level of impact. Printer of Acts of Parliament.\n228. Equality Act 2010. Statute Law Database.\n229. Human Rights Act 1998. Statute Law Database.\n230. Copyright, Designs and Patents Act 1988. Statute Law Database.\n231. Competition and Markets Authority et al. (2023). The Digital Regulation Cooperation Forum. \nGOV.UK.\n232. European Parliament (2023). EU AI Act: first regulation on artificial intelligence. European \nParliament.\n233. Blueprint for an AI Bill of Rights | OSTP. The White House.\n234. The White House (2023). FACT SHEET: President Biden Issues Executive Order on Safe, Secure, \nand Trustworthy Artificial Intelligence. The White House.\n235. Andrews, E. (2023). Liberty\u2019s Written Submission to a pro- innovation approach to AI \nregulation consultation. Liberty.\n24                                  POSTnote 708 - 9 January 2024 Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d."}
{"input":"What challenges do China's AI regulations pose to AI developers?","output":"Ensuring compliance with detailed regulatory requirements, such as truth and accuracy in training data and outputs, could be technically and operationally challenging.","context":"These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. And despite China\u2019s drastically different political system, policymakers in the United States and elsewhere can learn from its regulations. China\u2019s regulations create new bureaucratic and technical tools: disclosure requirements, model auditing mechanisms, and technical per-formance standards. These tools can be put to different uses in different countries, ranging from authoritarian controls on speech to democratic oversight of automated decisionmaking.\nCharting the successes, failures, and technical feasibility of China\u2019s AI regulations can give policymakers elsewhere a preview of what is possible and what might be pointless when it comes to governing AI.\nSo what do China\u2019s AI regulations contain? How did its massive party and state bureaucracies formulate them? And is it possible to predict where Chinese AI governance is headed?\nThis is the first in a series of three papers that will tackle these questions using a novel approach: reverse engineering. 2 AI legislation in China So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable."}
{"input":"What are the potential benefits and risks of using AI in public services?","output":"AI could improve healthcare outcomes and educational resources but may also exacerbate inequalities and create barriers for digitally excluded communities.","context":"secure processing;\n\u2022 Targeting key sectors for AI adoption for economic growth and human development; and\n\u2022 Adopting AI to improve public service delivery and resource efficiency while building capability\nand expertise. or jeopardize safety or rights.  At the same time, my Administration will promote responsible uses of AI that protect consumers, raise the quality of goods and services, lower their prices, or expand selection and availability. and implemented. As an initial step there needs to be a greater investment in digital skills, so that the government can formulate\nthe appropriate policies and risk management strategies which are context specific and not just adopted from other country\ncontexts. The shift in the public sector employment needs to more high-skilled workers will take place gradually over the long\nterm, but it is a key consideration today because building digital skills in the public sector and overcoming skills shortages more\ngenerally, takes a long time.\nHaving in place a policy for the ethical use of AI is fundamental to managing the adverse consequences of AI use in public policy.\nThe ethical use of AI means that these systems should not harm humans. Rather, they are used to enhance overall human\nwellbeing. Since the software decision-making may result in bias, it is critical that there are policies in place for dealing with the\ngovernance and oversight of the use of AI solutions. critical infrastructures (e.g. transport), that could put the life and health of citizens at risk;\neducational or vocational training, that may determine the access to education and professional course of someone\u2019s life (e.g. scoring of exams);\nsafety components of products (e.g. AI application in robot-assisted surgery);\nemployment, management of workers and access to self-employment (e.g. CV-sorting software for recruitment procedures);\nessential private and public services (e.g. credit scoring denying citizens opportunity to obtain a loan);\nlaw enforcement that may interfere with people\u2019s fundamental rights (e.g. evaluation of the reliability of evidence);\nmigration, asylum and border control management (e.g. verification of authenticity of travel documents);\nadministration of justice and democratic processes (e.g. applying the law to a concrete set of facts).\nHigh-risk AI systems will be subject to strict obligations before they can be put on the market:"}
{"input":"Should there be biases and issues of equity?","output":"No, there should be algorithmic discrimination protections, data privacy, and human alternatives, considerations, and fallback.","context":"Fair equality of opportunity entails that the distribution of shares\u2014and, more specifically, access to opportunities\u2014is not improperly influenced by socio-economic contingencies, namely by a person\u2019s place in the social system (Rawls 1971, p. 63). It does not only require a formal equality of opportunity, ensured, for example, through the legal system; it entails the substantial promotion of opportunities as real chances for every person to express their agency and, therefore, the development of adequate conditions for people to afford them. For this reason, fair equality of opportunity needs to be considered along with the difference principle, which emphasises attention to the expectations and conditions of the least advantaged, not only to access but also to substantially enjoy those chances. The paper is structured as follows. In the second section, we provide an overview of the state of the art of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. At the end of the section, we question whether the concept of fairness so understood is adequate for the discussion of fairness in HMLA or whether the latter calls for a more complex concept of fairness that requires more than just non-discrimination and an exclusively distributive dimension, and that includes features and criteria that extend beyond the consideration of biases. Moreover, the different distributive justice options mentioned above are sometimes incompatible (Friedler et al. 2016; Dieterich et al. 2016); for example, a model may be fair with respect to the outcomes but unfair with respect to the allocation (and vice versa), but it is extremely hard, if not impossible, for any ML model to satisfy all conditions (Chouldechova 2017).\n\nThis general difficulty in defining what fairness in HMLA is (Friedler et al. 2016) and how to ensure and promote it emerges in the debate as clear proof of the fact that \u2018ML fairness is not just a task for ML specialists, but requires ethical reasoning\u2019 (Rajkomar et al. 2018) and that \u2018framing fairness as a purely technical problem is problematic\u2019 (McCradden et al. 2020). administration charged with delivering them; but several risks also arise. These\ninclude managing inherent biases in data and algorithms that exacerbate or\ncreate problems."}
{"input":"How does this paper expect to attract talent in AI?","output":"Making it easier for noncitizens to travel to the US to work in the field, improving visa processing times, as well as as expand the categories of nonimmigrants who qualify for the domestic visa renewal program covered under 22 CFR 41.111(b) to include academic J\u20131 research scholars and F\u20131 students in science, technology, engineering, and mathematics (STEM).","context":"adopt AI and robotics, and what are the consequences for jobs? Zenodo.\n39. Wall, J. et al. (2023). Making AI work for Britain. techUK.\n40. Department for Science, Innovation and Technology, Britain to be made AI match-fit with \u00a3118 \nmillion skills package. GOV.UK.\n41. Tuomi, A. et al. (2023). Riding Against the Algorithm: Algorithmic Management in On- Demand \nFood Delivery. in Information and Communication Technologies in Tourism 2023. (eds. Ferrer-Rosell, \nB. et al.) 28\u2013\n39. Springer Nature Switzerland.\n16                                  POSTnote 708 - 9 January 2024 https:\/\/www.ifc.org\/wps\/wcm\/connect\/publications_ext_content\/ifc_external_publication_site\/publications_listing_page\/artificial+intelligence+in+emerging+markets\nILO (International Labour Organization). 2018. The Economics of artificial intelligence: Implications for the future of work. Geneva: ILO.\nhttps:\/\/www.ilo.org\/wcmsp5\/groups\/public\/---dgreports\/---cabinet\/documents\/publication\/wcms_647306.pdf\nINSEAD.2020. The Global Talent in the Age of Artificial Intelligence: The Global Talent Competitiveness Index 2020. Paris: INSEAD.\nhttps:\/\/gtcistudy.com\/wp-content\/uploads\/2020\/01\/GTCI-2020-Report.pdf\nITU (International Telecommunication Union). 2018. Accessing the Economic Impact of Artificial Intelligence.\nhttps:\/\/www.itu.int\/dms_pub\/itu-s\/opb\/gen\/S-GEN-ISSUEPAPER-2018-1-PDF-E.pdf\nJackson, T. 2017. \u201cSA\u2019s hearX Group Crowdfunding Campaign Oversubscribed.\u201d Disrupt Africa, August 9.\nhttp:\/\/disrupt-africa.com\/2017\/08\/sas-hearx-group-crowdfunding-campaign-oversubscribed\/ and others 2020). The inadequate supply of highly\nskilled AI workers being produced combined with\nthe movement of skilled workers to high-income\ncountries means that there is a shortage of AI\nscientists in developing countries, particularly\ncountries that lack AI research and industry hubs\n(McKinsey Global Institute 2020). Many countries\ntherefore face a challenging problem of not only\ndeveloping, but also retaining local AI talent. The\nconstraints on investment and workforce talent\ninputs reflects the globally asymmetric outputs in\nAI research and development (R&D), which are\nconcentrated in just a small number of countries\n(Savage 2020).\n\nworldbank.org\/digitaldevelopment\n3\n\n\fANALYTICAL INSIGHTS - NOTE 4 GLOBAL DIVIDES IN AI INVESTMENTS, TALENT AND RESEARCH\nInvestment in AI startups over the past decade has\nbeen dominated by the United States (US) and China\n(CB Insights 2019). A few other OECD countries\nincluding Japan, Sweden, Germany and France, as\nwell as Russia and South Korea have also generated\nsignificant investment in AI (OECD 2019b). In\nterms of human capital, AI talent is also scarce and\nunequally distributed across industries, sectors, and\ncountries. More than half of the population in the\ndeveloping world lacks basic digital skills (World\nBank 2019a), and the vast majority of those with\ndigital skills lack more advanced skills in AI and\nmachine learning (Gaskell 2020). The shortage of\nAI talent is even more acute amongst women, who\nremain highly underrepresented in the field (Gagne"}
{"input":"What are some considerations for Congress?","output":"Congress may, for example, consider legislation clarifying whether AI-generated works are copyrightable, who should be considered the author of such works, or when the process of training generative AI programs constitutes fair use. They are effectively on a wait-and-see policy.","context":"U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. (B)  submit a report to the President that describes the progress of these efforts, including an assessment of the types of AI models that may present CBRN risks to the United States, and that makes recommendations for regulating or overseeing the training, deployment, publication, or use of these models, including requirements for safety evaluations and guardrails for mitigating potential threats to national security. The evolving geopolitical threat landscape presents Congress with a historic opportunity to couple its support of Ukraine and its defense of democracy with new technological capabilities that are more effective in identifying circumvention of export controls and which also contribute to the long-term degradation of a potential military adversary. II. Top Ten Parameters Driving Regulatory Design and Outcomes   \n\nTo help policy makers and regulators, we define 10 key parameters that must be addressed in some fashion. This does not propose solutions for each domain or issue.   \n\nTransparency, fairness, explainability, security and trust are over-arching goals. Unless these goals can be achieved, AI regulation is unlikely to succeed, and AI adoption will be stunted. Many specific issues, e.g., informing users of risks, disclosing model characteristics, model-bias, and independent model safety testing, support these broader goals."}
{"input":"What are some of the advantages of the CASC approach?","output":"Some of the advantages are as follows:\n - Enables sectoral agencies to audit and regulate ADSs within their authority\n - Addresses gaps in regulation, clarifies legal uncertainties and empowers agencies to oversee ADSs impacting critical socioeconomic determinations\n - Allows US to be recognized as an undisputed leader in trustworthy AI, leading to global business development opportunities\n - Regulatory flexibility, enables better international alignment and strengthens trade relationships","context":"(b)  based on input from the process described in subsection 4.6(a) of this section, and in consultation with the heads of other relevant agencies as the Secretary of Commerce deems appropriate, submit a report to the President on the potential benefits, risks, and implications of dual-use foundation models for which the model weights are widely available, as well as policy and regulatory recommendations pertaining to those models. Country Assistance Strategies (CAS)\nCAS Progress Reports\nInterim Strategy Notes\nCAS Completion Reports (included as an annex to the follow-on CAS)\nPublic Information Notice for a CAS\nJoint Staff Advisory Notes on Poverty Reduction Strategy Papers (PRSPs)\nCountry financing parameters (including the analyses that underpin their\npreparation)\nEconomic and Sector Work and Non-Lending Technical Assistance reports\nFactual Technical Documents that underpin project preparation\nProject Appraisal Documents (previously Staff Appraisal Reports)\nProject Papers\nProgram Documents for Development Policy Operation (DPOs)\nSupplemental Financing Documents (DPOs)\nTranche Release Documents (DPOs)\nIntegrated Safeguards Data Sheets\nCountry Assessment Reports on the Use of Country Systems (UCS)\nProject Assessments for UCS pilot countries\nImplementation Completion and Results Reports (ICR)\nNote on Cancelled Operations (NCO) (previously Project Completion Note)\nMonthly Loan and Credit Statements Since companies are primarily basing their export decisions on compliance with the lists and the regulations that apply to them, this means that no amount of private sector compliance resources and technology can effectively substitute for a BIS that cannot rapidly and effectively update the list.\n\nThe primary barriers to improving BIS digital and data infrastructure and the efficacy of export controls are a lack of funding and a lack of staff with the required skill sets. (b)  To promote equitable administration of public benefits:\n\n          (i)   The Secretary of HHS shall, within 180 days of the date of this order and in consultation with relevant agencies, publish a plan, informed by the guidance issued pursuant to section 10.1(b) of this order, addressing the use of automated or algorithmic systems in the implementation by States and localities of public benefits and services administered by the Secretary, such as to promote:  assessment of access to benefits by qualified recipients; notice to recipients about the presence of such systems; regular evaluation to detect unjust denials; processes to retain appropriate levels of discretion of expert agency staff; processes to appeal denials to human reviewers; and analysis of whether algorithmic systems in use by benefit programs achieve equitable and just outcomes."}
{"input":"What are some ways in which governments can mitigate potential threats posed by AI?","output":"Some ways in which governments can mitigate potential threats posed by AI include:\n - Strengthening resilience to AI threats: Adopting strategies of resilience to mitigate potential threats by focusing on areas like biosecurity, countering cybersecurity threats, strengthening democratic resilience, and developing emergency response options for various threats from state, sub-state, and non-state actors.\n - Non-traditional regulatory techniques: In order to truly encapsulate the everchanging AI landscape it is important for governments to look into non-traditional regulatory frameworks like investing in publicly owned data sets for AI research, issuing challenge grants for socially beneficial AI uses, establishing uniform liability rules for developers, and setting requirements for AI assessment.\n - Funding innovation: By funding national AI resources and partnering with private sector, the government can introduce improvements in risk assessments for dealing with unexpected AI-enabled issues","context":"What should government policymakers do to protect humanity?\nThe potential dangers posed by AI are many. At the extreme, they include the threat of human extinction, which could come about by an AI-enabled catastrophe, such as a well-designed virus that spreads easily, evades detection, and destroys our civilization. Less dire, but considerably worrisome, is the threat to democratic governance if AIs gain power over people.[2] Even if AIs do not kill humans or overturn democracy, authoritarian regimes, terrorist groups, and organized crime groups could use AI to cause great harm by spreading disinformation and manipulating public opinion. Governments need to view the AI landscape as a regulatory training ground in preparation for the threats posed by even more-advanced AI capabilities, including the potential arrival of artificial general intelligence. Governments should focus on strengthening resilience to AI threats In addition to more-traditional regulatory practices, government policies on AI should focus on strategies of resilience to mitigate potential AI threats because strategies aimed solely at denial will not work. AI cannot be contained through regulation, so the best policy will aim to minimize the harm that AI might do. This will probably be most critical in biosecurity, [3] but harm reduction also includes countering cybersecurity threats, strengthening democratic resilience, and developing emergency response options for a wide variety of threats from state and sub- and non-state actors. Governments will either need to adopt entirely new capabilities to put this policy into action or expand existing agencies, such as the Cybersecurity and Infrastructure Security Agency. Governments should take a more comprehensive approach to regulation beyond hardware controls, which will not be enough to mitigate harms in the long run. Governments should look beyond traditional regulatory techniques to influence AI developments"}
{"input":"How can international collaboration in areas like Data Free Flow with Trust (DFFT) and agile governance contribute to fostering responsible AI development?","output":"Collaborative initiatives like DFFT aim to enhance cross-border data flow while ensuring trust in privacy, security, and intellectual property rights. Similarly, agile governance frameworks promote multi-stakeholder involvement, agile regulatory processes, and effective enforcement mechanisms to address emerging challenges in AI governance.","context":"Further Collaboration in DFFT and Emerging Technologies\nThis commentary analyses the AI-focused sections (\u201cDigital\u201d in the communiqu\u00e9 and \"Responsible AI and Global AI Governance\" in the declaration), but they are not the only points concerning AI governance. Other sections, specifically those on Data Free Flow with Trust (DFFT) and Emerging and Disruptive Technologies in Innovative Society and Economy, offer additional valuable insights into AI governance. This list will delve into these sections and distill their key implications for AI governance. Yet it is not entirely clear what kind of cooperation might extend beyond the existing national policies and partnerships with international initiatives. The additional sections of the G7 statements that are closely related to AI governance give some insights.\n\nIn the DFFT section, the creation of the new IAP has been agreed on. This arrangement would be rooted in multi-stakeholder problem-solving, guided by shared principles. Further, in the section on \u201cEmerging and Disruptive Technologies in Innovating Society and Economy,\u201d the necessity of an agile, dispersed, and multi-stakeholder governance approach for AI-driven cyber-physical systems is recognized, and a policy package has been suggested to actualize this. hard law approaches for managing AI development\nacross any of the key policy domains, though it has\nindicated committing to ethical standards through\nframeworks and toolkits, and providing strategic\ninvestments and partnerships to advance adoption\nof AI within targeted sectors. Both governments\nalso place high strategic importance on engaging\nmulti-stakeholder collaborations to facilitate\nAI. However, while the UAE has taken a hybrid\napproach with more top-down directed initiatives\nto accelerate the development of an AI ecosystem,\nFinland has instead exhibited a mostly bottom-up\napproach. Other key differences include the greater\nfocus the UAE has had on acquiring AI talent\nfrom abroad, as well as their more emphatic role in\ndirecting sectoral adoption of AI.\n\nworldbank.org\/digitaldevelopment\n23\n\n\fANALYTICAL INSIGHTS - NOTE 4 These endeavors have a direct link to trustworthy AI. For instance, having trust in the dataset used to train an AI system is crucial to ensuring its transparency and accountability. Moreover, the question of how to build trust in data generated by AI systems is an inevitable challenge in realizing the DFFT. Discussions on how to make such trustworthy AI ecosystems will be promoted under the umbrella of the IAP in partnership with the public and private sectors.\nImplementing Agile, Distributed, and Multi-stakeholder Governance through Policy Incentives"}
{"input":"How the EU-US are collaborating on AI risk through Trade and Technology council?","output":"The EU and US are collaborating on AI risk through the Trade and Technology Council by engaging in projects focused on advancing trustworthy AI. These include developing common terminology, metrics, and methodologies for AI risk assessment, coordinating with international standards bodies, and piloting Privacy-Enhancing Technologies  in sectors like health and medicine.","context":"International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules. | 15\nincluding the Council of Europe, the EU, the Global Partnership on AI (GPAI), the\nGroup of Seven (G7), the Group of Twenty (G20), the Inter-American\nDevelopment Bank (IDB), the International Telecommunications Union (ITU),\nthe OECD, the United Nations, the United Nations Educational, Scientific and\nCultural Organization (UNESCO) and the World Bank.\nCross-border research on AI is also significant. For example, the French National\nResearch Agency, the German Research Foundation and the Japan Science and\nTechnology Agency have called for trilateral French-German-Japanese\ncollaborative research on AI over three years (2019-2021). In 2020, the United\nKingdom and the United States signed a declaration on co-operation for AI R&D\nthrough which they plan to drive technological breakthroughs, promote\nresearch collaboration and advance the development of trustworthy AI.\nMany European Union member states are also participating in European AI AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. discussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial\nIntelligence\u201d \u2013 A European Approach to Excellence and Trust. The paper\nconsiders requiring a pre-marketing conformity assessment for \u201chigh-risk\u201d AI\napplications such as facial recognition, as a core element of a potential\nregulatory framework for AI. In addition, the white paper proposes a voluntary\n\u201cquality label\u201d for AI applications considered not to be high-risk. In parallel, the\nEuropean Commission is reviewing EU product safety and liability regimes in\nlight of AI.\nRead more:\nhttps:\/\/ec.europa.eu\/digital-single-market\/en\/artificial-intelligence.\nGlobal Partnership on AI (GPAI)\nResponsible entity: Consortium of countries\nDescription: GPAI is an international and multi-stakeholder initiative that\nadvances cutting-edge research and pilot projects on AI priorities to advance"}
{"input":"What would congressional action on the subject look like?","output":"Congressional action on the subject would involve passing legislation to codify the principles outlined in the Blueprint for an AI Bill of Rights and expanding coverage to include law enforcement and national security. This legislation would establish enforceable guidelines for data privacy, auditing automated decisions, and ensuring accountability, addressing the challenges in enforcing criteria driven by proprietary interests and providing a credible enforcement regime.","context":"(b)  This order shall be implemented consistent with applicable law and subject to the availability of appropriations.\n\n     (c)  This order is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person.\n\n                             JOSEPH R. BIDEN JR.\n\nTHE WHITE HOUSE,\n  October 30, 2023. U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. (B)  submit a report to the President that describes the progress of these efforts, including an assessment of the types of AI models that may present CBRN risks to the United States, and that makes recommendations for regulating or overseeing the training, deployment, publication, or use of these models, including requirements for safety evaluations and guardrails for mitigating potential threats to national security. President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated."}
{"input":"What are the two groups that have formed in terms of the G7 countries and their approaches to AI related regulations?","output":"Two groups have emerged in terms of countries' approaches to AI-related regulations: the first group advocates for a comprehensive, binding framework focusing on governance, transparency, and security, including countries like France, Germany, Italy (under the EU AI Act), and Canada (proposing AIDA). The second group adopts a sector-specific, nonbinding guidance approach, emphasizing appropriate AI governance with a focus on transparency and data protection, represented by countries like Japan and the United Kingdom, with the United States potentially shifting towards this approach pending legislation like the Algorithmic Accountability Act.","context":"Risk-Based and Forward-Looking Approaches under Different Policy Frameworks\n\nFirst, it is important to note that the G7 statements do not attempt to harmonize the approaches and policy instruments of the G7 members, declaring that \u201cthe common vision and goal of trustworthy AI may vary across G7 members.\u201d Furthermore, the declaration emphasizes that policies and regulations should take into account not only technical and institutional characteristics, but also social and cultural impacts, including geographic, sectoral, and ethical aspects. In fact, there are policy differences among G7 members, from the European Union (France, Germany, and Italy) and Canada, which promote comprehensive and binding regulations for AI, to Japan, the United Kingdom, and the United States, which promote sector-specific guidance-based policies. G7 Leaders issue \u2018Guiding Principles\u2019 and voluntary \u2018Code of Conduct\u2019 on AI\nOn Monday, Group of Seven (G7) leaders issued a set of international Guiding Principles on artificial intelligence as well as a voluntary Code of Conduct for AI developers under the \u2018Hiroshima Artificial Intelligence Process.\u2019 In a joint statement, leaders called on organizations developing advanced AI systems to commit to the code of conduct as well as ministers to accelerate development of the Process\u2019 policy framework.\n\nThe Principles and Code are the culmination of work that began in May 2023 at the G7 summit, where the Hiroshima AI Process \u2013 born out of the EU-US Trade and Technology Council ministerial \u2013 was formed to find common policy priorities and promote guardrails for advanced AI systems globally. The G7 bloc includes Canada, France, Germany, Italy, Japan, Britain, the US, and the European Union. International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules. The Path to Trustworthy AI G7 Outcomes and Implications for Global AI Governance\n\nCommentary by Hiroki Habuka\n\nPublished June 6, 2023\n\nIntroduction\nThe G7 Summit, held from May 19 to 21, 2023, left a strong impression of unity among global leaders. Emerging technology such as artificial intelligence (AI), along with national security and energy, was highlighted as a key area requiring a strong alliance among G7 countries toward fundamental values such as democracy and human rights. The G7 Digital and Tech Ministers\u2019 Meeting, which took place a little earlier, also discussed responsible AI and global AI governance as one of the central topics."}
{"input":"How are the White House Voluntary AI Commitments influencing international AI outcomes?","output":"The White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7, influencing international AI outcomes.","context":"The United States is likely to take a decentralized bottom-up approach that is messy and may have gaps and inconsistencies. This will likely disappoint proponents of strong AI regulation. Yet, over time this may produce a patchwork quilt of AI rules that if implemented well is grounded in the expertise of specific agencies and more tailored to innovation. The tremendous risks and opportunities of AI have made it a presidential-level issue. The White House is coordinating executive agencies, as each moves ahead with actions in its own domain. The lack of meaningful regulation of social media is widely seen as a failure in Washington, and bolsters efforts to be more proactive with AI. Yet the decentralized structure of the U.S. government, political differences, and the complexities of AI forestall big, quick actions. There is also a real but less-likely probability that a big AI-related failure sparks a strong national government action. On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC. on Responsible Military Use of Artificial Intelligence and Autonomy that responsibly and lawfully harnesses AI capabilities, like autonomous functioning systems. And finally, the White House announced a philanthropic-giving initiative with ten foundations that have collectively committed more than $200 million to advance AI priorities."}
{"input":"What are some notable Chinese AI chip design companies mentioned in the text?","output":"One notable Chinese AI chip design company mentioned in the text is Cambricon.","context":"Since 2013, China has published several national-level policy documents, which reflect the intention to develop and deploy AI in a variety of sectors. For example, in 2015, the State Council released guidelines on China\u2019s \u2018Internet\u2009+\u2019 action. It sought to integrate the internet into all elements of the economy and society. The document clearly stated the importance of cultivating emerging AI industries and investing in research and development. In the same year, the 10-year plan \u2018Made in China 2025\u2019 was released, with the aim to transform China into the dominant player in global high-tech manufacturing, including AI (McBride and Chatzky 2019). Another notable example is the Central Committee of the Communist Party of China\u2019s (CCP) 13th 5-year plan,Footnote2 published in March 2016. The document mentioned AI as one of the six critical areas for developing the country\u2019s emerging industries (CCP 2016), and as an important factor in stimulating economic growth. When read together, these https:\/\/www.reuters.com\/article\/us-lattice-m-a-canyonbridge-trump\/trump-bars-chinese-backed-firm-from-buying-u-s-chipmaker-latticeidUSKCN1BO2ME\nBanga, Karishma and Dirk Willem te Velde. 2018 Digitalization and the Future of Manufacturing in Africa. London: Supporting Economic\nTransformation. https:\/\/set.odi.org\/wp-content\/uploads\/2018\/03\/SET_Digitalisation-and-future-of-African-manufacturing_Final.pdf\nBarton, D., J. Woetzel, J. Seong, Q. Tian. 2017. Artificial Intelligence: Implications for China. McKinsey Global Institute.\nhttps:\/\/www.mckinsey.com\/featured-insights\/china\/artificial-intelligence-implications-for-china AU-EU Digital Economy Task Force (AU-EU DETF). 2019. New Africa-Europe Digital Economy Partnership: Accelerating the Achievement\nof the Sustainable Development Goals.\nhttps:\/\/ec.europa.eu\/digital-single-market\/en\/news\/new-africa-europe-digital-economy-partnership-report-eu-au-digital-economy-task-force\nAvasant. 2019. Applied AI and Advanced Analytics Services 2019.\nhttps:\/\/avasant.com\/report\/applied-ai-and-analytics-services-2019-radarview\/\nBAAI (Beijing Academy of Artificial Intelligence). 2019. \u201cBeijing AI Principles.\u201d\nhttps:\/\/www.baai.ac.cn\/news\/beijing-ai-principles-en.html?fbclid=IwAR2HtIRKJxxy9Q1Y953H-2pMHl_bIr8pcsIxho93BtZY-FPH39vV9v9B2eY\nBaker, Liana B. 2017. \u201cTrump Bars Chinese-Backed Firm from Buying U.S. Chipmaker Lattice.\u201d Reuters, September 13.\nhttps:\/\/www.reuters.com\/article\/us-lattice-m-a-canyonbridge-trump\/trump-bars-chinese-backed-firm-from-buying-u-s-chipmaker-latticeidUSKCN1BO2ME Government affiliated bodies and private companies have also developed their own AI ethics principles. For example, the Beijing Academy of Artificial Intelligence, a research and development body including China\u2019s leading companies and Beijing universities, was established in November 2018 (Knight 2019). This body then released the \u2018Beijing AI Principles\u2019 to be followed for the research and development, use, and governance of AI (\u201cBeijing AI Principles\u201d 2019). Similar to the principles forwarded by the AIDP Expert Committee, the Beijing Principles focus on doing good for humanity, using AI \u2018properly\u2019, and having the foresight to predict and adapt to future threats. In the private sector, one of the most high-profile ethical framework has come from the CEO of Tencent, Pony Ma. This framework emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to"}
{"input":"For AI governance principles drafted in this report, which existing institutions did the report refer to?","output":"FATF, FSB, IAEA, ICANN, ICAO, ILO, IMO, IPCC, ITU, SWIFT and UNOOSA","context":"The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. In this series of three papers, I will attempt to reverse engineer Chinese AI governance. \nI break down the regulations into their component parts\u2014the terminology, key concepts, and specific requirements\u2014and then trace those components to their roots, revealing how 3.The \u201cPolicy Funnel\u201d of China\u2019s AI Governance Major governance initiatives tend to proceed from left to right through this funnel, though often not in a linear fashion. AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents."}
{"input":"What are some recommendations for the government regarding building human capacity?","output":"Empower people to effectively use and interact with AI systems, equip them with necessary skills, ensure a fair transition for workers, enhance safety of workers and the quality of jobs","context":"Governments can adopt policies and governance frameworks that promote\nhuman-centric AI while maximizing opportunities.\nPolicymaker Actions\n\u00bb Develop AI policy anchored in ethical principles and tailored to specific\nsettings to provide the authorizing environment. Governance and op\u2022 Provide leadership based on global\nerational framework based on ethical principles are essential to speciguiding principles\nfy broad guidelines and institutional arrangements.\n\u2022 Adopt policies and governance\n\u00bb Establish an innovation hub to pool talent, establish partnerships with\nframeworks to enable humanacademia and the private sector, promote research, and facilitate excentric AI solutions\nperimentation by line ministries. Innovative procurement approaches\n\u2022 Invest in human capital, digital\nare then adopted to leverage private sector skills with agility to allow\ninfrastructure, training in digital\niterative, problem-driven approaches to requests-for-proposals.\nskills, and research secure processing;\n\u2022 Targeting key sectors for AI adoption for economic growth and human development; and\n\u2022 Adopting AI to improve public service delivery and resource efficiency while building capability\nand expertise. 3.3 Social governance\nSocial governance, or more literally in Chinese \u2018social construction\u2019,Footnote7 is the third area in which AI is promoted as a strategic opportunity for China. Alongside an economic slowdown, China is facing emerging social challenges, hindering its pursuit of becoming a \u2018moderately prosperous society\u2019 (AIDP 2017). An aging population and constraints on the environment and other resources are explicit examples provided in the AIDP of the societal problems that China is facing. Thus, the AIDP outlines the goal of using AI within a variety of public services to make the governance of social services more precise and, in doing so, mitigate these challenges and improve people\u2019s lives. (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is"}
{"input":"How was the role of government in the Finnish and UAE National AI strategies similar and different?","output":"- Similar: active roles  in developing and executing AI policies and initiatives - expansion of AI in public service, developing local scientific research capabilities, and developing AI training programs for students and government employees through public-private partnerships.\n - Different: While the UAE has taken a hybrid approach with more top-down directed initiatives to accelerate the development of an AI ecosystem, Finland has instead exhibited a mostly bottom-up approach. Other key differences include the greater focus the UAE has had on acquiring AI talent from abroad, as well as their more emphatic role in directing the sectoral adoption of AI.","context":"41 https:\/\/www.smartdubai.ae\/\n42 https:\/\/government.ae\/en\/about-the-uae\/strategies-initiatives-and-awards\/federal-governments-strategies-and-plans\/the-uae-strategy-for-the-fourth-industrial-revolution\n43 https:\/\/www.government.ae\/en\/about-the-uae\/strategies-initiatives-and-awards\/local-governments-strategies-and-plans\/dubai-internet-of-things-strategy\n44 https:\/\/ai.gov.ae\/wp-content\/uploads\/resources\/UAE_National_Strategy_for_Artificial_Intelligence_2031.pdf ; 28.\n45 Ibid, 27.\n46 Ibid, 26.\n47 Ibid, 28.\n\nworldbank.org\/digitaldevelopment\n20\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nComparing the Role of Government\nin the Finnish and UAE National AI\nstrategies FINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re-\n\nview of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries. categories of tools and instruments that were used\nto assess national AI strategies. The third section\npresents findings from eleven countries, beginning\nwith a more detailed analysis of the national AI\npolicies of Finland and the United Arab Emirates\n(UAE). This section also includes findings from an\nanalysis of a broader set of countries in Asia, Africa\nand South America, highlighting the approach\ntaken to AI in some developing countries. The final\nsection highlights future areas of research needed\nto expand upon this work. KEY:\n\nDIRECTING\n\nENABLING\n\nNO INITIATIVE\n22\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nThe governments of both Finland and the UAE\nhave demonstrated active roles in developing and\nexecuting AI policies and initiatives. Four areas of\nflagship policies emerge in Finland\u2019s AI strategy\nincluding supporting scientific research and\ndeveloping AI talent, supporting entrepreneurial\necosystems for AI, enhancing data access and\nestablishing ethical data and algorithm governance\nguidance through collaboration with stakeholders,\nand finally a focus on developing AI for public\nservice delivery. Similarly, the UAE government\u2019s\napproach to AI adoption also focuses on the\nexpansion of AI in public service, developing local\nscientific research capabilities, and developing AI\ntraining programs for students and government\nemployees through public-private partnerships. The\nUAE strategy has limited reference to regulatory or"}
{"input":"Which elected officials have actively supported the Affordable Connectivity Program (ACP)?","output":"Democrat officials broadly support ACP. In the House, 12 Republican co-sponsors exist. Senators J.D. Vance (R-Ohio) and Kevin Cramer (R-N.D.) have supported it. Additionally, 26 governors from both parties endorse its extension.","context":"2.2 Implementing the AIDP (iii)  Within 180 days of the date of this order, the Secretary of Transportation shall direct the Advanced Research Projects Agency-Infrastructure (ARPA-I) to explore the transportation-related opportunities and challenges of AI \u2014 including regarding software-defined AI enhancements impacting autonomous mobility ecosystems.  The Secretary of Transportation shall further encourage ARPA-I to prioritize the allocation of grants to those opportunities, as appropriate.  The work tasked to ARPA-I shall include soliciting input on these topics through a public consultation process, such as an RFI. (e)  The Federal Communications Commission is encouraged to consider actions related to how AI will affect communications networks and consumers, including by:\n\n          (i)    examining the potential for AI to improve spectrum management, increase the efficiency of non-Federal spectrum usage, and expand opportunities for the sharing of non-Federal spectrum;\n\n          (ii)   coordinating with the National Telecommunications and Information Administration to create opportunities for sharing spectrum between Federal and non-Federal spectrum operations;\n\n          (iii)  providing support for efforts to improve network security, resiliency, and interoperability using next-generation technologies that incorporate AI, including self-healing networks, 6G, and Open RAN; and Notes and Consultation Plan for an SSP\n\uf0b7 All other operational policy and strategy papers\n\uf0b7 Numerical Country Policy and Institutional Assessment (CPIA) Ratings for\ncountries eligible for IDA financing\n\uf0b7 Numerical IDA Country Performance Ratings (derived from CPIA ratings)\n\uf0b7 Funding proposals for activities financed through Bank-administered trust\nfunds\n\uf0b7 Preliminary, decision-point, and completion-point documents prepared under\nthe Heavily Indebted Poor Country (HIPC) Initiative\n\uf0b7 QAG synthesis\nB. Financial information\n\uf0b7 Annual Budget Document\nC. Documents prepared by a member country\n\uf0b7 Poverty Reduction Strategy Papers\n\uf0b7 Letters of Development Policy\n\uf0b7 Procurement Plans and updates\nD. Board proceedings\n\uf0b7 Executive Directors\u2019 Work Program"}
{"input":"What have leaders said about the signing of the Executive Order on AI?","output":"Senate Majority Leader Schumer praised the Executive Order as a crucial step for US AI leadership but stressed the need for further Congressional action. The NSF also supported the Order, ready to implement its actions and principles.","context":"At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups. U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023"}
{"input":"Considering the ethical implications, how could on-chip governance mechanisms balance security with user privacy and rights?","output":"On-chip governance uses privacy-preserving 'verification' and 'operating licenses' in data centers. This involves verifiable chip usage claims without sensitive data exposure, enabled by a Trusted Execution Environment.","context":"persuade, and deceive citizens (Brundage and\nothers 2018). In a commercial setting, the imbalance\nin power between data collectors and processors\nand the individuals providing their data may result\nin undesirable market outcomes (Klein 2020). In\nthe case of governments, these technologies raise\nindividual privacy concerns and can be used in\nways that infringe human rights and individual\nfreedoms (Risse 2018).\n\nworldbank.org\/digitaldevelopment\n5\n\n\fANALYTICAL INSIGHTS - NOTE 4 When relating this back to AI policy, China has benefited from the abundance of data that historically lax privacy protections have facilitated (Ding 2018). On the surface, China\u2019s privacy legislation seems to contradict other development commitments, such as the Social Credit System, which requires extensive personal data. This situation creates a dual ecosystem whereby the government is increasingly willing to collect masses of data, respecting no privacy, while simultaneously admonishing tech companies for the measures they employ (Sacks and Laskai 2019). Recall that private companies, such as the AI National Team, are relied upon for governance at both a national and local level, and therefore may receive tacit endorsement rather than admonishment in cases where the government\u2019s interests are directly served. As a result, the \u2018privacy strategy\u2019 within China appears to aim to protect the privacy of a specific type of consumer, rather than that of citizens as a whole, allowing the Privacy and data protection\nAccountability\nSafety and security\nTransparency and explainability\nFairness\nHuman control of technology\nProfessional responsibility\nPromotion of human values\n\n\u2022\n\nPRIVACY AND DATA PROTECTION. AI solutions should respect an individual\u2019s right to privacy and civil liberties. Individuals should have control over\ntheir data, with their consent required for repurposing, redistributing or erasure.\n\n\u2022\n\nACCOUNTABILITY. Mechanisms must ensure accountable behavior during the life cycle of AI design and implementation. An\nagency or body should be responsible for monitoring accountability.\n\n\u2022\n\nSAFETY AND SECURITY. Cybersecurity is critical. Leaders must ensure the well-being of society at large and private individual\nhumans.\n\n\u2022 the use of data and programming that influences decision making, roles and responsibilities concerning cybersecurity, openness and\ntransparency when it comes to what and how AI is being used and explainability of the results, fairness and non-discrimination in the\nuse of AI, the extent of human control of technology, and the foundational human values.\nHuman oversight should provide an additional safeguard against machine-invoked bias. Introducing human oversight can help detect\nskewed results from influences such as bias in data selection, data manipulation, forgery, and intentional programmed bias. Governance\nframeworks need to be established that promote transparency with respect to the use of AL, algorithms and data, self-assessment, peer\nreview. Public inclusion in the governance framework is critical to strengthen compliance with the policy principles and legal frameworks."}
{"input":"What are the consequences for AI systems categorized as posing an unacceptable risk?","output":"AI systems classified as posing an unacceptable risk are prohibited, including those capable of manipulation or social scoring, and proposals exist to ban real-time remote biometric identification in public spaces.","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d The Regulatory Framework defines 4 levels of risk in AI:\npyramid showing the four levels of risk: Unacceptable risk; High-risk; limited risk, minimal or no risk\n \nAll AI systems considered a clear threat to the safety, livelihoods and rights of people will be banned, from social scoring by governments to toys using voice assistance that encourages dangerous behaviour.\n\nHigh risk:\nAI systems identified as high-risk include AI technology used in: Risk-based approach. AI is a general-purpose technology that is likely to be ubiquitous and apply to a wide range of applications. This makes it harder to regulate well. Many propose a risk-based approach where higher-risk AI applications that can cause greater damage have proportionately greater regulation and lower-risk applications have less. This has merits, but there are many implementation obstacles including who and how risk is defined and measured, what are the minimum requirements to offer AI services, and what AI uses are deemed illegal. Further, Google argues that focusing regulation on the highest risks may deter innovation that also has the highest value to society. critical infrastructures (e.g. transport), that could put the life and health of citizens at risk;\neducational or vocational training, that may determine the access to education and professional course of someone\u2019s life (e.g. scoring of exams);\nsafety components of products (e.g. AI application in robot-assisted surgery);\nemployment, management of workers and access to self-employment (e.g. CV-sorting software for recruitment procedures);\nessential private and public services (e.g. credit scoring denying citizens opportunity to obtain a loan);\nlaw enforcement that may interfere with people\u2019s fundamental rights (e.g. evaluation of the reliability of evidence);\nmigration, asylum and border control management (e.g. verification of authenticity of travel documents);\nadministration of justice and democratic processes (e.g. applying the law to a concrete set of facts).\nHigh-risk AI systems will be subject to strict obligations before they can be put on the market:"}
{"input":"How does China view AI in terms of international competition and military capabilities?","output":" AI is seen as a critical element for enhancing national competitiveness and security. China aims to leverage AI to leapfrog developments in military capabilities, positioning itself as a significant player in international competition.","context":"3 China\u2019s AI strategic focus In this article, we analysed the nature of AI policy within China and the context within which it has emerged, by mapping the major national-level policy initiatives that express the intention to utilise AI. We identified three areas of particular relevance: international competitiveness, economic growth, and social governance (construction). The development and deployment of AI in each of these areas have implications for China and for the international community. For example, although the \u2018trump-card\u2019 policy to gain a military advantage may not be something new, its application to AI technologies risks igniting an arms race and undermining international stability (Taddeo and Floridi 2018). Efforts to counteract this trend seem largely hollow. Our analysis indicates that China has some of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the perceived risks present in each of these areas. The AIDP highlights three areas where AI can make a substantial difference within China: international competition, economic development, and social governance. They are strictly interrelated but, for the sake of clarity, we shall analyse them separately, and contextualise each of them by discussing the relevant literature surrounding the broader political backdrop and contemporary policy debates."}
{"input":" What role do central functions play in the UK's AI regulatory framework?","output":" Central functions aim to support regulators by monitoring the regulatory framework's effectiveness, assessing AI risks, and promoting interoperability with international frameworks, among other tasks.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground Governments should look beyond traditional regulatory techniques to influence AI developments New centralized coordinating functions: The government will establish cross-sectoral monitoring, risk assessment, education, horizon-scanning, and other centralized functions in order to support implementation and coherence of the framework.\nAI assurance techniques and technical standards: The government suggests that these will play a critical role in supporting the framework, which it will encourage by publishing a toolkit of assurance techniques in collaboration with industry.\nTerritorial application: The framework applies across the United Kingdom and will not change the territorial application of any existing legislation. The UK government will work with international partners to promote interoperability and coherence between different approaches, noting the complex and cross-border nature of AI supply chains.\nCross-Sectoral Principles\n\nThe principles of the regulatory framework are explained further below:"}
{"input":" How do existing AI policies address ethical considerations, and what principles are commonly emphasized?","output":" Existing AI policies focus on ethics, emphasizing principles like accountability, fairness, transparency, and privacy. These principles aim to guide the responsible and ethical use of AI technologies.","context":"This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory of, China\u2019s AI strategy. It emphasises that the Chinese government is aware of the potential benefits, practical risks, and the ethical challenges that AI presents, and that the direction of China\u2019s AI strategy will largely be determined by the interplay of these factors and by the extent to which government\u2019s interests may outweigh ethical concerns. Section 5 concludes the paper by summarising the key findings of our analysis. Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors. FUTURE RESEARCH\nAs policy and regulatory pathways for harnessing AI\nare still in their infancy, further time and research\nis needed to assess the outputs of these strategies,\nand their effectiveness in order to identify best\npractices for developing countries. Rather than\nconcluding with best practices, this article provides\nan initial review of policy and regulatory practices\ncurrently deployed in AI policymaking, illustrating\na wide range of tools and approaches that\ngovernments have taken in their attempt to harness\nAI technologies while mitigating anticipated risks.\nWhile many of the policies observed have been"}
{"input":" What are the Universal Guidelines for Artificial Intelligence (UGAI), and which rights do they emphasize?","output":" The Universal Guidelines for Artificial Intelligence, endorsed by over 250 experts and 60 organizations in October 2018, emphasize rights such as Transparency, Human Determination, Fairness, Assessment and Accountability, Accuracy, Reliability, Validity, Data Quality, Public Safety, Cybersecurity, and prohibitions on Secret Profiling and Unitary Scoring. These guidelines aim to ensure that AI systems are transparent, fair, and accountable and that they respect the privacy and safety of individuals.","context":"In April 2023, EPIC Senior Counsel Ben Winters and Equal Justice Works Fellow Grant Fergusson published a series of blog posts explaining how each of these instruct how entities using AI can do it more responsibly.\n\nUniversal Guidelines for Artificial Intelligence\nIn October 2018, over 250 experts and 60 organizations, representing more than 40 countries, endorsed the Universal Guidelines for Artificial Intelligence (\u201cUGAI\u201d). The guidelines were organized by the Public Voice. The guidelines in full are: with existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA).\nThe Act defines different risk levels with corresponding levels of regulation. It bans certain \nhigh-risk applications, such as live facial recognition.\nIn the US, a \u2018Blueprint for an AI Bill of Rights\u2019 has been outlined.233 Currently these are \nnon-binding guidelines that aim to address discrimination, data privacy, and transparency. In \nOctober 2023, US President Joe Biden signed an Executive Order on AI mandating standards and \ndisclosures for the largest AI companies, and measures to protect workers and disadvantaged \ngroups.234\nPotential future regulations\nHuman intervention in automated decision- making\nSome civil society groups235 and academics236,237 have suggested a law to enshrine a right to human \nintervention in automated decision-making. There are two ways to implement this: In line with this publication, the Standardization Administration of the People\u2019s Republic of China, the national-level body responsible for developing technical standards, released a white paper on AI standards. The paper contains a discussion of the safety and ethical issues related to technology (Ding and Triolo 2018). Three key principles for setting the ethical requirements of AI technologies are outlined. First, the principle of human interest states that the ultimate goal of AI is to benefit human welfare. Second, the principle of liability emphasises the need to establish accountability as a requirement for both the development and the deployment of AI systems and solutions. Subsumed within this principle is transparency, which supports the requirement of understanding what the operating principles of an AI system are. Third, the principle of consistency of [sic] rights and responsibilities emphasised that, on the one hand, data should be properly recorded and oversight Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed."}
{"input":" What is the Global Partnership on Artificial Intelligence (GPAI), and what is its mandate?","output":" Launched in June 2020, GPAI is a multi-stakeholder initiative focused on advancing AI in alignment with democratic values and human rights. Its mandate includes project-oriented collaboration on responsible AI, data governance, the future of work, and commercialization and innovation.","context":"Responsible entity: Consortium of countries\nDescription: GPAI is an international and multi-stakeholder initiative that\nadvances cutting-edge research and pilot projects on AI priorities to advance\nthe responsible development and use of AI that respects human rights and\nshared democratic values, as elaborated in the OECD\u2019s Recommendation on AI.\nThe Partnership was conceived by Canada and France during their G7\npresidencies and, at its launch on 15 June 2020, counted 13 other founding\nmembers: Australia, the EU, Germany, India, Italy, Japan, Korea, Mexico, New\nZealand, Singapore, Slovenia, the United Kingdom and the United States. With\nits Secretariat hosted at the OECD, the GPAI brings together experts from\nindustry, government, civil society and academia.\nGPAI\u2019s mission is to \u201csupport the development and use of AI based on human\nrights, inclusion, diversity, innovation, and economic growth while seeking to\naddress the United Nations Sustainable Development Goals\u201d. Two Centres of AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. The first expected drivers of such tools are international organizations such as the OECD and United Nations Educational, Scientific and Cultural Organization (UNESCO), as well as multi-stakeholder initiatives such as the GPAI. In particular, the OECD is the organization mentioned most often in the G7 statements, not only as the issuer of the OECD AI Principles, but also as the research hub such as mapping the commonalities and differences between trustworthy AI frameworks. UNESCO published a document, Recommendation on the Ethics of Artificial Intelligence, in 2021, and various projects are underway to put it into practice. GPAI is a multi-stakeholder, academia-led initiative that aims to bridge the gap between theory and practice on AI, including the research on use of privacy enhancing technologies, the use of AI to solve environmental problems, and many other projects. discussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial\nIntelligence\u201d \u2013 A European Approach to Excellence and Trust. The paper\nconsiders requiring a pre-marketing conformity assessment for \u201chigh-risk\u201d AI\napplications such as facial recognition, as a core element of a potential\nregulatory framework for AI. In addition, the white paper proposes a voluntary\n\u201cquality label\u201d for AI applications considered not to be high-risk. In parallel, the\nEuropean Commission is reviewing EU product safety and liability regimes in\nlight of AI.\nRead more:\nhttps:\/\/ec.europa.eu\/digital-single-market\/en\/artificial-intelligence.\nGlobal Partnership on AI (GPAI)\nResponsible entity: Consortium of countries\nDescription: GPAI is an international and multi-stakeholder initiative that\nadvances cutting-edge research and pilot projects on AI priorities to advance"}
{"input":" What does Directive 026-11 concerning the use of face recognition technologies mandate?","output":" Directive 026-11 mandates thorough testing of face recognition and capture technologies to prevent unintended bias or disparate impact, periodic evaluation to meet performance goals, and establishes a right for U.S. citizens to opt-out of certain non-law enforcement uses of face recognition.","context":"Directive 026-11, \u201cUse of Face Recognition and Face Capture Technologies:\u201d This directive dictates that all uses of face recognition and face capture technologies will be thoroughly tested to ensure there is no unintended bias or disparate impact in accordance with national standards. DHS will review all existing uses of this technology and conduct periodic testing and evaluation of all systems to meet performance goals. The Directive also requires that U.S. citizens be afforded the right to opt-out of face recognition for specific,\u202fnon-law enforcement uses, prohibits face recognition from being used as the sole basis of any law or civil enforcement related action, and establishes a process for Department oversight offices including the Privacy Office, the Office for Civil Rights and Civil Liberties (CRCL), and the Office of the Chief Information Officer, to review all new uses of face recognition and face capture technologies. in Artificial Intelligence, Technical University of Munich.\n150. Mobilio, G. (2023). Your face is not new to me \u2013 Regulating the surveillance power of facial \nrecognition technologies. Internet Policy Rev., Vol 12.\n151. Ovide, S. (2020). A Case for Banning Facial Recognition. The New York Times.\n152. Murray, D. (2019). Live facial recognition: the impact on human rights and participatory \ndemocracy. University of Essex.\n153. Minderoo Centre for Technology and Democracy (2022). A Sociotechnical Audit: Assessing Police \nuse of Facial Recognition. mctd.ac.uk.\n154. Ada Lovelace Institute (2019). Beyond face value: public attitudes to facial recognition \ntechnology.\n155. Big Brother Watch Team (2023). 65 parliamentarians call for\n\u201cimmediate stop\u201d to live facial\n21                                  POSTnote 708 - 9 January 2024 criticised by some civil society groups for \u201cwatering down protections\u201d245,246 present in current \nlaw against automated decision-making.\nBan on automated decision-making and ban on live facial recognition\nRather than regulating automated decision-making, some legal scholars propose banning it entirely \nand argue that it is an \u201cillegitimate source of authority in liberal democracy\u201d, is not compatible \nwith societal values such as equality and fairness, and that it can infer intimate knowledge about \nhumans.* 247,248\nCivil liberty campaigners and some Parliamentarians have called for a ban on live facial \nrecognition (see Surveillance)155 akin to the ban proposed by the European Parliament in its draft \nAI Act.232\nIn 2020, the Equality and Human Rights Commission recommended the suspension of live facial \nrecognition due to \u201cdiscriminatory impacts\u201d.249 The Metropolitan Police and Home Office have for individuals.129,161\u2013167 Sharing of non-consensual pornographic deepfakes has been criminalised \nby the Online Safety Act 2023.168\u2013170\nGenerative AI can be used to create fake personas online171, or impersonate real people.172\u2013176 \nThis increases security risks, such as confidential information being unwittingly released to \nmalicious actors, and convincing phishing and scam calls.177\n* Live facial recognition refers to the use of facial recognition in real time via surveillance \ncameras. Retrospective facial recognition refers to the use of facial recognition after \nphotographic or video evidence has been captured and\/or taken from a different party. Facial \nrecognition tools use AI (PB 57).\n\u2020 Profiling is defined by the UK GDPR and ICO to be \u201cany form of automated processing of personal \ndata consisting of the use of personal data to evaluate certain personal aspects relating to a \nnatural person, in particular to analyse or predict aspects concerning that natural person's"}
{"input":" What is an AI regulatory sandbox and how does it facilitate the interaction between regulators and AI developers?","output":" An AI regulatory sandbox is a framework designed to improve communication between regulators and AI developers, easing regulatory compliance and providing legal certainty. It allows for voluntary participation, where regulators and developers can collaborate to identify potential legal issues during the development of AI systems, enabling earlier and potentially less costly adjustments.","context":"scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms. There is no specific technical definition of an AI sandbox; the term can refer to a rangeof approaches from a simple ongoing exchange of documentation (from companies)and feedback (from regulators) all the way to a computing environment shared by acompany and regulators. This creates some uncertainty\u2014for instance, while theEuropean Parliament\u2019s version of the AI Act requires each EU member state toestablish at least one regulatory sandbox, it is not clear what precisely each countrywould implement.\nThe first such AI sandbox has been\nby a partnership between theEuropean Commission and the Spanish government, but regulatory sandboxes forother industries are not new. Over 50 countries have experimented with usingregulatory sandboxes for\n, and the\nothers in biotechnology, health, energy, and waste treatment. Some of thesesandboxes have performed assessments on AI systems, such as those from the UKICO and the U.S. , and the\nothers in biotechnology, health, energy, and waste treatment. Some of thesesandboxes have performed assessments on AI systems, such as those from the UKICO and the U.S.\n. Particularly valuable are thepublic reports published by the ICO when a financial technology application leaves theregulatory sandbox. These reports can\non how aspecific algorithmic system can comply with regulatory requirements, therebyinforming the public and other companies building similar applications.\nAI sandboxes have many distinguishing qualities relative to other AI regulatoryinterventions. First, they require ongoing collaboration between regulators andregulated companies, and may be less adversarial than an algorithmic audit.Sandboxes may require more work for companies (such as sharing updated data orensuring an algorithmic system works in a government computing environment).\nDevelop Regulatory AI Sandboxes 3.\nrecently launched\ndigital financial services\nOECD has documented AI regulatory sandboxes can also demand more from regulators, especially if theyentail developing a computing environment. Beyond the skills necessary foralgorithmic auditing, regulators would need to ensure that their computingenvironments can accommodate a broad range of algorithmic software in order toallow various AI developers to use the sandboxes. Further, regulators may have todevelop regulatory sandboxes that are capable of testing many distinct types ofalgorithmic systems, including algorithms built into phone apps, online platforms, andphysical products. Holding algorithmic systems indefinitely in government computingenvironments during development may increase risks to intellectual property,increasing the stakes of strong cybersecurity. Due to the significant workload requiredfor AI sandboxes, they may be more appropriate for relatively high-stakes algorithmicsystems."}
{"input":" How does the document suggest the U.S. might manage AI risks without passing a broad national AI law?","output":" The document suggests that the U.S. will likely manage AI risks through domain-specific agency actions in areas like healthcare, financial services, housing, workforce, and child safety, along with multiple executive orders.","context":"The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with INTERNATIONAL LAWS\nSeveral other counties are more advanced than the U.S. in terms of AI policy development that works to protect people from algorithmic harm. For more information on AI laws and norms internationally, please see EPIC\u2019s International Policy page. The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7]"}
{"input":" What specific actions does the EOAI mandate for the Departments of State and Commerce?","output":" The EOAI tasks the Departments of State and Commerce with establishing robust international frameworks for harnessing AI benefits and managing its risks, ensuring safety, and accelerating the development of AI standards with international partners in standards organizations.","context":"(e)  The Secretary of Commerce is hereby authorized to take such actions, including the promulgation of rules and regulations, and to employ all powers granted to the President by the International Emergency Economic Powers Act, 50 U.S.C. 1701 et seq., as may be necessary to carry out the purposes of subsections 4.2(c) and (d) of this section.  Such actions may include a requirement that United States IaaS Providers require foreign resellers of United States IaaS Products to provide United States IaaS Providers verifications relative to those subsections.\n\n     4.3.  Managing AI in Critical Infrastructure and in Cybersecurity.  (a)  To ensure the protection of critical\ninfrastructure, the following actions shall be taken: First, new responsibilities have been added to BIS without additional resources to carry them out. For example, in 2021, the responsibility for regulating and administering exports of firearms moved from the Department of State to the Department of Commerce. However, none of the staff or budget that was performing this function made a move. Similarly, two executive orders during the Trump administration made the Department of Commerce responsible for a broad swath of activities regulating imports of foreign telecommunications equipment, a job ultimately given to BIS. In the Biden administration\u2019s Commerce Department budget request for FY 2023, BIS has identified more than $53 million in unfunded requirements from recent legislation and executive orders. (iii)  Provide that the Secretary of Commerce, in accordance with such standards and procedures as the Secretary may delineate and in consultation with the Secretary of Defense, the Attorney General, the Secretary of Homeland Security, and the Director of National Intelligence, may exempt a United States IaaS Provider with respect to any specific foreign reseller of their United States IaaS Products, or with respect to any specific type of account or lessee, from the requirements of any regulation issued pursuant to this subsection.  Such standards and procedures may include a finding by the Secretary that such foreign reseller, account, or lessee complies with security best practices to otherwise deter abuse of United States IaaS Products. (iii)  Within 180 days of the date of this order, the Secretary of HHS shall, in consultation with relevant agencies as the Secretary of HHS deems appropriate, consider appropriate actions to advance the prompt understanding of, and compliance with, Federal nondiscrimination laws by health and human services providers that receive Federal financial assistance, as well as how those laws relate to AI.  Such actions may include:\n\n               (A)  convening and providing technical assistance to health and human services providers and payers about their obligations under Federal nondiscrimination and privacy laws as they relate to AI and the potential consequences of noncompliance; and\n\n               (B)  issuing guidance, or taking other action as appropriate, in response to any complaints or other reports of noncompliance with Federal nondiscrimination and privacy laws as they relate to AI."}
{"input":" What is agile governance in the context of Japan's AI regulation?","output":" Agile governance in Japan's AI regulation refers to the approach of respecting companies' voluntary efforts for AI governance while providing nonbinding guidance, based on multistakeholder dialogue, to support or guide these efforts, with the guidance being continuously updated in a timely manner.","context":"As the word \u201cstakeholder\u201d frequently appears in the G7 statements, the future of AI governance will likely not rely solely on top-down, government-led rule-setting. Instead, it suggests an approach where multi-stakeholder initiatives are discussed on a case-by-case basis and updated in an agile manner. Regulations and technical standards serve as critical tools to facilitate these efforts in building trustworthy AI. However, it is crucial to note that the key players in AI governance, who develop and use these tools, will be not only the government entities, but rather the private stakeholders including AI developers, users, and civil society organizations, and so on. AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. In this series of three papers, I will attempt to reverse engineer Chinese AI governance. \nI break down the regulations into their component parts\u2014the terminology, key concepts, and specific requirements\u2014and then trace those components to their roots, revealing how 3.The \u201cPolicy Funnel\u201d of China\u2019s AI Governance Major governance initiatives tend to proceed from left to right through this funnel, though often not in a linear fashion. The TGDS is an expert group of leading academics and practitioners from G7 member countries. It proposed the \u201cGovernance Principles for a Society Based on Cyber-Physical Systems\u201d framework, which emphasizes the need for an agile, distributed and multi-stakeholder process (so-called Agile Governance) for Cyber-Physical Systems with AI at its core. The report further suggested a policy package to promote Agile Governance, that includes (1) proactive governance by organizations and individuals that deploy or operate aspects of CPS, (2) expert involvement and utilization of digital tools, (3) agile regulatory governance, (4) reliable certification mechanisms, (5) effective enforcement systems and appropriately tailored liability systems, and (6) legal remedial measures. Such an integrated policy framework will provide an important perspective for the development of an interoperable governance framework for responsible AI.\nConclusion"}
{"input":" Who hosted the conversation on December 5, 2022, to discuss the White House's Blueprint for an AI Bill of Rights?","output":" The Brookings Center for Technology Innovation hosted the conversation on December 5, 2022, to discuss the White House's Blueprint for an AI Bill of Rights.","context":"Monday\u2019s White House action builds on its previous commitments to the safe and responsible development of AI. Some of these include its support for developing the Blueprint for an AI Bill of Rights, which safeguards Americans\u2019 rights and safety; securing voluntary commitments from Anthropic, Google, OpenAI, and other tech companies to thoroughly test systems before releasing them \u2013 also called \u201cred-teaming\u201d \u2013 and clearly labeling all AI-generated content; and granting $140 million to the National Science Foundation to establish seven new AI research institutes. The Order is also meant to complement the G7 leaders\u2019 Guiding Principles and Code of Conduct, which was also issued on Monday. (See below for more details.) Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. The Senate hearing\u2019s theme coincides with Senator Schumer\u2019s third AI \u2018Insight Forum,\u2019 which will bring big bank and union leaders together to establish a \u201cnew foundation for AI policy in the workplace\u201d and \u201censure the use of AI benefits everyone,\u201d as first reported by FedScoop. Tuesday\u2019s hearing will be at least the fourth that Congress has hosted this month alone, including a House hearing on Safeguarding Data and Innovation (see a transcript on Tech Policy Press) and the Senate Commerce hearing on CHIPS and Science Implementation and Oversight.\n\n11:30 AM \u2013 The bipartisan group of US Senators that calls itself the AI \u201cGang of Four\u201d \u2013 Sen. Martin Heinrich (D-NM), Sen. Mike Rounds (R-SD), Sen. Charles Schumer (D-NY), and Sen. Todd Young (R-IN) \u2013 heads to the White House to meet with President Biden. The convening comes less than 24 hours after President Biden issued his Executive Order on AI and on the eve of the third Senate AI \u2018Insight Forum\u2019 led by Sen. Schumer. To advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by From Principles to Practice\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, or access to critical needs."}
{"input":" How does the World Bank classify its document types?","output":" Bank documents are assigned one of the following four classifications: Public, Official Use Only, Confidential, or Strictly Confidential.","context":"providing digital or hard copies, particularly for requests that are complex or timeconsuming.\nPublications, some specialized databases, and other knowledge products\n(including subscription-based services) may be purchased through the Bank\u2019s Office of\nthe Publisher. The full text of books published by the Office of the Publisher may be\nviewed and downloaded free of charge on the World Bank website or viewed on Google\nBooks. The core statistical databases may be accessed free of charge on the World Bank\nwebsite.\nB. Classification\n28. Bank documents are assigned one of the following four classifications: \u201cPublic,\u201d\n\u201cOfficial Use Only,\u201d \u201cConfidential,\u201d or \u201cStrictly Confidential.\u201d\n25 Unsupported requests include those that (a) do not provide enough information to enable the Bank to\nlocate the requested document in the Bank\u2019s records management system and\/or (b) are for documents\nthat the Bank is unable to locate in the Bank\u2019s records management system after a reasonable search.\n12 Provided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member\ncountries, clients, donors, recipients, or vendors, including consultants) and the financial\nand deliberative information that contains or relates to such information, is not\ndeclassified or made publicly available. For the purposes of this policy statement, the term \u201cWorld Bank\u201d or \u201cBank\u201d means the International\nBank for Reconstruction and Development (IBRD) and the International Development Association\n(IDA); \u201cWorld Bank Group\u201d means the IBRD, IDA, International Finance Corporation (IFC),\nMultilateral Investment Guarantee Agency (MIGA), and International Centre for Settlement of\nInvestment Disputes (ICSID); \u201cloans\u201d may include IDA credits and grants, grants made out of the\nBank\u2019s net income and administrative budget, and trust fund grants; \u201cborrowers\u201d includes credit and\ngrant recipients, and guarantors; \u201cinformation\u201d means documents of any type (for example, paper,\nelectronic, photograph, film, sound recordings, videotapes) prepared or received by the Bank in the\ncourse of its official business; and \u201cthird party\u201d means any individual, group of individuals,\norganization, or other entity that provides information to the Bank. The terms \u201cdisclose,\u201d Classification\n\n28.\nBank documents are assigned one of the following four classifications: \u201cPublic,\u201d\n\u201cOfficial Use Only,\u201d \u201cConfidential,\u201d or \u201cStrictly Confidential.\u201d\n\n25\n\nUnsupported requests include those that (a) do not provide enough information to enable the Bank to\nlocate the requested document in the Bank\u2019s records management system and\/or (b) are for documents\nthat the Bank is unable to locate in the Bank\u2019s records management system after a reasonable search.\n\n11"}
{"input":" What is the process to file an appeal?","output":" All appeals must be submitted electronically through the link provided in the Bank\u2019s response to a request. Appeals must be filed with the Access to Information Committee (AIC) within 60 calendar days of the Bank\u2019s decision to deny access. Second-level appeals to the AI Appeals Board must be filed within 60 calendar days after the AIC\u2019s decision to uphold the Bank\u2019s initial decision to deny access.","context":"interest reasons, an override of the Policy\u2019s exceptions is justified.\n40.\nNotification of Decision. Requesters are notified if the appeal is dismissed:\n(a) for failure to file within the required time; (b) for failure to provide sufficient\ninformation that would reasonably support the appeal; or (c) for appealing a matter that\nthe AI Committee or Appeals Board does not have authority to consider. After the\nAI Committee or the Appeals Board reaches its decision concerning the appeal, the\nrequester is notified of the decision. If the AI Committee or Appeals Board upholds the\n33 appeals within 45 working days of receiving an appeal (delays are communicated in\nwriting to the requester).\n38.\nAppealing the AI Committee\u2019s Denial\u2014Appeals Board. The Bank has\nestablished an independent Appeals Board33 to consider appeals alleging that the Bank\nviolated this Policy by restricting access to information that it would normally disclose\nunder the Policy,34 if the AI Committee upholds the initial decision to deny access; the\nAppeals Board does not consider appeals concerning requests to override the Policy\u2019s\nexceptions.35 The Appeals Board has the authority to uphold or reverse the relevant\ndecisions of the AI Committee, and the Appeals Board\u2019s decisions in such instances are\nfinal. Appeals to the Appeals Board must be filed, in writing, within 60 calendar days\nafter the AI Committee\u2019s decision to uphold the Bank\u2019s initial decision to deny access.\nThe Appeals Board makes its best efforts to consider all appeals that are received within deny access.\n16\nappeals within 45 working days of receiving an appeal (delays are communicated in\nwriting to the requester).\n38. Appealing the AI Committee\u2019s Denial\u2014Appeals Board. The Bank has\nestablished an independent Appeals Board33 to consider appeals alleging that the Bank\nviolated this Policy by restricting access to information that it would normally disclose\nunder the Policy,34 if the AI Committee upholds the initial decision to deny access; the\nAppeals Board does not consider appeals concerning requests to override the Policy\u2019s\nexceptions.35 The Appeals Board has the authority to uphold or reverse the relevant\ndecisions of the AI Committee, and the Appeals Board\u2019s decisions in such instances are\nfinal. Appeals to the Appeals Board must be filed, in writing, within 60 calendar days\nafter the AI Committee\u2019s decision to uphold the Bank\u2019s initial decision to deny access.\nThe Appeals Board makes its best efforts to consider all appeals that are received within after the AI Committee\u2019s decision to uphold the Bank\u2019s initial decision to deny access.\nThe Appeals Board makes its best efforts to consider all appeals that are received within\na reasonable time period before the next scheduled Appeals Board session.\n39. Submitting an Appeal. All appeals must be submitted in writing to the Access to\nInformation Secretariat within 60 calendar days of the decisions to deny access. Appeals\nreceived beyond the 60-day period for filing an appeal with the AI Committee (first stage\nof appeal) and the Appeals Board (second stage of appeal) are deemed out of time and are\nnot considered. Appeals may be submitted electronically through the Bank\u2019s website at\nwww.worldbank.org\/wbaccess. Appeals also may be set out in a brief letter and sent by\nmail to Access to Information Secretariat, World Bank, 1818 H Street, NW, Washington,\nDC, 20433. Appeals should be addressed to either the AI Committee (first stage of"}
{"input":" What international cooperation initiatives exist regarding AI?","output":" Many countries are engaged in international co-operation for AI, which is taking place in fora including the Trade and Technology Council (TTC), the Council of Europe(CoE), the EU, the G7 and G20, the Global Partnership on AI (GPAI), the Global Privacy Assembly (GPA), the Ibero-American Data Protection Network (RIPD), the Inter-American Development Bank (IDB), the International Telecommunications Union (ITU), the UN, UNESCO, and the World Bank. Cooperation on AI research is also a priority.","context":"International co-operation on AI\nInternational co-operation to promote the beneficial use of AI and address its\nchallenges is a priority for many countries. Co-operation for the development\nand adoption of AI and AI governance is being conducted at the bilateral,\nplurilateral, regional and international level. Moreover, many intergovernmental organisations with complementary mandates are engaged in AI\ninitiatives and projects. International co-operation on AI is taking place in fora\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021 on Digital Cooperation.\nhttps:\/\/www.un.org\/en\/pdfs\/DigitalCooperation-report-for%20web.pdf\nUSAID.2018. Reflecting the Past, Shaping the Future: Making AI Work for International Development. Washington, DC.\nhttps:\/\/www.usaid.gov\/sites\/default\/files\/documents\/15396\/AI-ML-in-Development.pdf\nVillani, C. and others. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy.\nParis: Conseil national du num\u00e9rique.\nVinuesa, R., Azizpour, H., Leite, I. et al. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals.\nNature Communications, 11, 233.\nhttps:\/\/doi.org\/10.1038\/s41467-019-14108-y\nWhittaker, M. and others. 2018. AI Now Report 2018. New York: AI Now Institute, New York University.\nhttps:\/\/ainowinstitute.org\/AI_Now_2018_Report.pdf\nWhite House. 2020. American Artificial Intelligence Initiative: Year One Annual Report. Office of Science and Technology Policy. 60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed Effective implementation of national AI initiatives hinges on coordination\nCountries pursue different national governance models to co-ordinate the\nimplementation of their national AI policies across government, offering\nregulatory and ethical oversight (Figure 1). Models include:\n\u2022\n\nAssigning oversight of the development and implementation strategies\nto an existing ministry, department or body. Among existing ministries\nor agencies tasked with developing or implementing an AI strategy, the\nfollowing tend to drive the creation of AI strategies most often: 1)\ninformation technology and communications ministries; 2) economics or\nfinance ministries; or 3) education, science (and technology) and\ninnovation ministries.\n\n\u2022\n\nCreating a new governmental or independent AI co-ordination entity.\n\n\u2022"}
{"input":" How do you view the idea of artificial intelligence replacing humans?","output":" I do not agree with it. AI is humanity's new frontier. Once this boundary is crossed, AI will lead to a new form of human civilization. The guiding principle of AI is not to become autonomous or replace human intelligence. Therefore, we must ensure that it is developed through a humanist approach, based on values and human rights. We are faced with a crucial question: what kind of society do we want for tomorrow? The AI revolution opens up exciting new prospects, but the anthropological and social upheaval it brings in its wake warrants careful consideration.","context":"environmental toll. The Guardian.\n202. Hsu, J. (2023). Shifting where data is processed for AI can reduce environmental harm. New \nScientist.\n203. M\u00fcller, V. C. et al. (2016). Future Progress in Artificial Intelligence: A Survey of Expert \nOpinion. in Fundamental Issues of Artificial Intelligence. (ed. M\u00fcller, V. C.) 555\u2013572. Springer \nInternational Publishing.\n204. Bubeck, S. et al. (2023). Sparks of Artificial General Intelligence: Early experiments with \nGPT-4. arXiv.\n205. Metz, C. (2023). Microsoft Says New AI Shows Signs of Human Reasoning. The New York Times.\n206. Nathan, A. et al. (2023). Generative AI: hype, or truly transformative? Goldman Sachs.\n23                                  POSTnote 708 - 9 January 2024 thing that \u201cwe know when we see it\u201d. For example, it is              list as a working basis to discuss SDG impacts. However,\nnotoriously difficult to measure human intelligence, yet             I am cognizant of the fact that such a \u201cblack-box\nit is pretty obvious to us when we meet a highly                     approach\u201d to AI which does not specify the underlying\nintelligent person.                                                  techniques and technologies is not sufficient to fully\nAs a working definition, I suggest following eminent                 understand all SDG impacts. The details of AI concepts,\ncognitive scientist Margaret Boden: \u201cArtificial                      techniques and technologies matter greatly, especially\nIntelligence (AI) seeks to make computers do the sorts of            with regard to sustainability and with regard to future\nthings that minds can do. Some of these (e.g., reasoning,            directions of AI and its physical, economic and socio- they work. The Conversation.\n175. David, D. (2021). Analyzing The Rise Of Deepfake Voice Technology. Forbes.\n176. Green, J. et al. (2023). Experts warn of rise in scammers using AI to mimic voices of loved \nones in distress. ABC News.\n177. Brundage, M. et al. (2018). The Malicious Use of Artificial Intelligence. Malicious AI Report.\n178. Leffer, L. (2023). Can AI Replace Actors? Here\u2019s How Digital Double Tech Works. Scientific \nAmerican.\n179. Smith, D. (2023). \u2018We\u2019re going\nthrough a big revolution\u2019: how AI is de-ageing stars on screen. The Guardian.\n180. Equity (online). AI Vision Statement.\n181. Maddaus, G. (2023). SAG-AFTRA\nStrike: AI Fears Mount for Background Actors. Variety.\n182. Barbas, S. (2015). Laws of Image: Privacy and Publicity in America. Stanford University Press.\n22                                  POSTnote 708 - 9 January 2024 adopt AI and robotics, and what are the consequences for jobs? Zenodo.\n39. Wall, J. et al. (2023). Making AI work for Britain. techUK.\n40. Department for Science, Innovation and Technology, Britain to be made AI match-fit with \u00a3118 \nmillion skills package. GOV.UK.\n41. Tuomi, A. et al. (2023). Riding Against the Algorithm: Algorithmic Management in On- Demand \nFood Delivery. in Information and Communication Technologies in Tourism 2023. (eds. Ferrer-Rosell, \nB. et al.) 28\u2013\n39. Springer Nature Switzerland.\n16                                  POSTnote 708 - 9 January 2024"}
{"input":" What are the main priorities outlined in President Biden's AI Executive Order?","output":" The Executive Order focuses on creating safety and security standards for AI, protecting consumer privacy, evaluating potentially harmful AI-related healthcare practices, supporting workers, promoting innovation and competition, implementing AI standards globally with international partners, guiding federal agencies' use and procurement of AI, and advancing equity and civil rights to prevent algorithmic discrimination.","context":"President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated. At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups. The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI"}
{"input":" What is the purpose of CAN\/DGSI 103-1:2023 according to its scope? ","output":" The purpose of CAN\/DGSI 103-1:2023 is to specify minimum requirements and a set of controls for digital identity services as per the CIO Strategy Council.","context":"incentivised \u2018wish list\u2019, to nudge, and coordinate other relevant stakeholders, rather than a central directive (Sheehan 2018). This is why the 3-year plan promoting the AIDP (2018\u20132020) emphasises coordination between provinces and local governments. Another point to consider is that the G7 statements do not explicitly address the link between regulations and standards. Even if international standards are established, unless they are aligned with each country's regulatory content and civil liability systems, businesses could still face challenges due to regulatory inconsistencies. This suggests a need for ongoing discussions among nations, not just to support the development of standards, but also to understand the interplay between regulations and standards.\nFurther Collaboration in DFFT and Emerging Technologies (iv)   Within 240 days of the completion of the guidelines described in subsection 4.3(a)(iii) of this section, the Assistant to the President for National Security Affairs and the Director of OMB, in consultation with the Secretary of Homeland Security, shall coordinate work by the heads of agencies with authority over critical infrastructure to develop and take steps for the Federal Government to mandate such guidelines, or appropriate portions thereof, through regulatory or other appropriate action.  Independent regulatory agencies are encouraged, as they deem appropriate, to consider whether to mandate guidance through regulatory action in their areas of authority and responsibility. Third, the government has become increasingly creative in creating complex new rules that are often more difficult to administer and enforce. For example, the U.S. approach to export controls has changed significantly via the Foreign Direct Product (FDP) Rule, which consists of an extraterritorial application of U.S. export controls on items manufactured with U.S. equipment, inputs, or design. The application of the FDP rule imposes new license requirements on exports to Russia and China and institutes a presumption of denial policy for license applications for exports or re-exports to Russia and China. The FDP rule is intended to restrict Russia and China\u2019s abilities to acquire critical goods such as microelectronics, telecommunications items, and other components with military applications, including from third countries."}
{"input":" What potential benefits could arise from aligning AI regulation internationally? ","output":" By aligning AI regulation, specialized AI firms could thrive globally, competition would be encouraged, markets would be healthier, and there would be more innovation in AI. Moreover, it could lead to reduced barriers to innovation and diffusion, facilitating a larger market for AI solutions and fostering international trade.","context":"Governments should look beyond traditional regulatory techniques to influence AI developments These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":" What was the focus of the PAI\u2019s Guidance for Safe Model Deployment: Multistakeholder Model in Action session?","output":" The focus was on the development of Guidance for Safe Foundation Model Deployment, demonstrating the PAI multistakeholder process by exploring consensus on key issues like identifying risks and scaling oversight and safety practices for AI models.","context":"(iv)   Within 365 days of the date of this order, the Secretary of HHS shall, in consultation with the Secretary of Defense and the Secretary of Veterans Affairs, establish an AI safety program that, in partnership with voluntary federally listed Patient Safety Organizations:\n\n               (A)  establishes a common framework for approaches to identifying and capturing clinical errors resulting from AI deployed in healthcare settings as well as specifications for a central tracking repository for associated incidents that cause harm, including through bias or discrimination, to patients, caregivers, or other parties; \n\n               (B)  analyzes captured data and generated evidence to develop, wherever appropriate, recommendations, best practices, or other informal guidelines aimed at avoiding these harms; and\n\n               (C)  disseminates those recommendations, best practices, or other informal guidance to appropriate stakeholders, including healthcare providers. 4.2.  Ensuring Safe and Reliable AI.  (a)  Within 90 days of the date of this order, to ensure and verify the continuous availability of safe, reliable, and effective AI in accordance with the Defense Production Act, as amended, 50 U.S.C. 4501 et seq., including for the national defense and the protection of critical infrastructure, the Secretary of Commerce shall require:\n\n          (i)   Companies developing or demonstrating an intent to develop potential dual-use foundation models to provide the Federal Government, on an ongoing basis, with information, reports, or records regarding the following:\n\n               (A)  any ongoing or planned activities related to training, developing, or producing dual-use foundation models, including the physical and cybersecurity protections taken to assure the integrity of that training process against sophisticated threats; (b)  To help ensure the safe, responsible deployment and use of AI in the healthcare, public-health, and human-services sectors:\n\n          (i)    Within 90 days of the date of this order, the Secretary of HHS shall, in consultation with the Secretary of Defense and the Secretary of Veterans Affairs, establish an HHS AI Task Force that shall, within 365 days of its creation, develop a strategic plan that includes policies and frameworks \u2014 possibly including regulatory action, as appropriate \u2014 on responsible deployment and use of AI and AI-enabled technologies in the health and human services sector (including research and discovery, drug and device safety, healthcare delivery and financing, and public health), and identify appropriate guidance and\nresources to promote that deployment, including in the following areas: Safety, security, and robustness: AI systems should function as intended and in a robust, secure, and safe way throughout the AI lifecycle, and risks should be continually identified, assessed, and managed. Safety-related risks will be sector specific, and regulators should take a proportionate approach to manage them. Regulators may require the corresponding AI lifecycle actors to regularly test or carry out due diligence on the functioning, resilience, and security of a system."}
