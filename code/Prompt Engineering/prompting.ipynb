{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Notebook\n",
    "\n",
    "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "The example uses the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
    "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
    "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: llama-recipes in /jet/home/apatula/.local/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: transformers in /jet/home/apatula/.local/lib/python3.10/site-packages (4.38.1)\n",
      "Requirement already satisfied: datasets in /jet/home/apatula/.local/lib/python3.10/site-packages (2.17.1)\n",
      "Requirement already satisfied: accelerate in /jet/home/apatula/.local/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: sentencepiece in /jet/home/apatula/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf==3.20 in /jet/home/apatula/.local/lib/python3.10/site-packages (3.20.0)\n",
      "Requirement already satisfied: py7zr in /jet/home/apatula/.local/lib/python3.10/site-packages (0.20.8)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /jet/home/apatula/.local/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: bitsandbytes in /jet/home/apatula/.local/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: fire in /jet/home/apatula/.local/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: torch_tb_profiler in /jet/home/apatula/.local/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: ipywidgets in /jet/home/apatula/.local/lib/python3.10/site-packages (8.1.2)\n",
      "Requirement already satisfied: appdirs in /jet/home/apatula/.local/lib/python3.10/site-packages (from llama-recipes) (1.4.4)\n",
      "Requirement already satisfied: black in /jet/home/apatula/.local/lib/python3.10/site-packages (from llama-recipes) (24.2.0)\n",
      "Requirement already satisfied: loralib in /jet/home/apatula/.local/lib/python3.10/site-packages (from llama-recipes) (0.1.2)\n",
      "Requirement already satisfied: optimum in /jet/home/apatula/.local/lib/python3.10/site-packages (from llama-recipes) (1.17.1)\n",
      "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /jet/home/apatula/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.1.dev0+gba5374836.d20240125)\n",
      "Requirement already satisfied: pyarrow-hotfix in /jet/home/apatula/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /jet/home/apatula/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /jet/home/apatula/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: texttable in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (3.20.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /jet/home/apatula/.local/lib/python3.10/site-packages (from fire) (2.4.0)\n",
      "Requirement already satisfied: tensorboard!=2.1.0,>=1.15 in /usr/local/lib/python3.10/dist-packages (from torch_tb_profiler) (2.9.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /jet/home/apatula/.local/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /jet/home/apatula/.local/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (68.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.42.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (3.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from black->llama-recipes) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from black->llama-recipes) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (4.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (2.0.1)\n",
      "Requirement already satisfied: tokenize-rt>=3.2.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from black[jupyter]->llama-recipes) (5.2.0)\n",
      "Requirement already satisfied: coloredlogs in /jet/home/apatula/.local/lib/python3.10/site-packages (from optimum->llama-recipes) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.1.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from coloredlogs->optimum->llama-recipes) (10.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->llama-recipes) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install llama-recipes transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/transformers/models/llama/convert_llama_weights_to_hf.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print('/'.join(transformers.__file__.split('/')[:-1]) + '/models/llama/convert_llama_weights_to_hf.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all parameters from the checkpoint at llama-2-7b.\n",
      "Loading the checkpoint in a Llama model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:03<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving in the Transformers format.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python '/jet/home/apatula/.local/lib/python3.10/site-packages/transformers/models/llama/convert_llama_weights_to_hf.py' --input_dir llama-2-7b --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  1 11:22:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              55W / 300W |      0MiB / 32768MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e01253cf894247af737fbab430c2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"./models_hf/7B\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the preprocessed dataset\n",
    "\n",
    "We load and preprocess the samsum dataset which consists of curated pairs of dialogs and their summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_recipes.utils.dataset_utils import get_preprocessed_dataset\n",
    "from llama_recipes.configs.datasets import samsum_dataset\n",
    "\n",
    "train_dataset = get_preprocessed_dataset(tokenizer, samsum_dataset, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List the most pressing topics regarding regulation of AI.\n",
      "What are the most pressing topics regarding regulation of AI?\n",
      "The most pressing topics regarding regulation of AI are:\n",
      "1. Privacy and data protection\n",
      "2. Liability and accountability\n",
      "3. Transparency and explainability\n",
      "4. Human-AI interaction\n",
      "5. AI and the workplace\n",
      "6. AI and the economy\n",
      "7. AI and the environment\n",
      "8. AI and the military\n",
      "9. AI and the law\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who owns material generated by a company’s large language model?\n",
      "The question of who owns the intellectual property generated by a company’s large language model (LLM) is a complex one.\n",
      "The answer depends on the specific contractual arrangements between the company and the LLM developer.\n",
      "In general, the company owns the intellectual property generated by the LLM.\n",
      "However, there are a few exceptions to this rule.\n",
      "For example, if the LLM is developed by a third-party company, the company may own the intellectual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe how China and the United States are approaching making new legislation to regulate Generative AI?\n",
      "What are the main differences between the Chinese and American approaches to regulating Generative AI?\n",
      "What are the main differences between the Chinese and American approaches to regulating Generative AI? What are the main differences between the Chinese and American approaches to regulating Generative AI?\n",
      "What are the main differences between the Chinese and American approaches to regulating Generative AI? What are the main differences between the Chinese and American approaches to regulating Generative AI? What are\n"
     ]
    }
   ],
   "source": [
    "# Additional article for types of prompt engineering: https://medium.com/@amiraryani/8-types-of-prompt-engineering-5322fff77bdf\n",
    "#Zero-shot Prompting\n",
    "prompt_list = [\"List the most pressing topics regarding regulation of AI.\",\n",
    "               \"Who owns material generated by a company’s large language model?\",\n",
    "               \"Describe how China and the United States are approaching making new legislation to regulate Generative AI?\"\n",
    "               ]\n",
    "\n",
    "input_output_pairs = []\n",
    "\n",
    "for prompt in prompt_list:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
      "               =>\n",
      "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
      "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
      "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
      "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
      "               Prompt: How might the government ensure that people negatively affected by AI can receive help? =>\n",
      "               The government can ensure that people negatively affected by AI by providing them with financial assistance, such as \n",
      "               unemployment benefits, and by providing them with access to mental health services.\n",
      "               Prompt: What are the potential risks of using AI to make decisions about people? =>\n",
      "               The potential risks of using AI to make decisions about people include discrimination, privacy violations, and the potential \n",
      "               for AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, and moral issues \n",
      "              regarding the development, testing, evaluation, and use of Generative AI:\n",
      "              Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
      "              the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
      "              Prompt: How do AI regulators attempt to enforce their regulations? => AI regulators attempt to enforce their regulations by \n",
      "              conducting audits, issuing fines, and imposing penalties.\n",
      "              Prompt: What are the main challenges in regulating AI? => The main challenges in regulating AI are the lack of international \n",
      "              consensus on AI regulation, the lack of a comprehensive regulatory framework, and the lack of enforcement mechanisms.\n",
      "              Prompt: What\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "              and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "              Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
      "              In the ML-patient interaction case, it is possible to detect biases including: \n",
      "              Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
      "              disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
      "              the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
      "              to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
      "              historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
      "              avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
      "              by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
      "              representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
      "              the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
      "              protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
      "              Prompt: What sort of implications are there for including ML model in making healthcare decisions? =>\n",
      "              The inclusion of ML models in healthcare decisions may have several implications, including: \n",
      "              Increased efficiency and accuracy: ML models can be used to automate and improve the efficiency and accuracy of healthcare \n",
      "              decisions. For example, ML models can be used to identify patients who are at risk of developing a particular disease or condition, \n",
      "              and to provide them with appropriate care and treatment. \n",
      "              Increased access to healthcare:\n"
     ]
    }
   ],
   "source": [
    "#One-shot Prompting\n",
    "prompt_list2 = ['''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
    "               =>\n",
    "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
    "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
    "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
    "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
    "               Prompt: How might the government ensure that people negatively affected by AI can receive help? =>''',\n",
    "              '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, and moral issues \n",
    "              regarding the development, testing, evaluation, and use of Generative AI:\n",
    "              Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
    "              the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
    "              Prompt: How do AI regulators attempt to enforce their regulations? =>''',\n",
    "              '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "              and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "              Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
    "              In the ML-patient interaction case, it is possible to detect biases including: \n",
    "              Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
    "              disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
    "              the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
    "              to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
    "              historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
    "              avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
    "              by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
    "              representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
    "              the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
    "              protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
    "              Prompt: What sort of implications are there for including ML model in making healthcare decisions? =>''']\n",
    "\n",
    "for prompt in prompt_list2:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
      "               =>\n",
      "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
      "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
      "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
      "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
      "               Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
      "               the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
      "               Prompt: How do AI regulators attempt to enforce their regulations? => AI regulators attempt to enforce their regulations by \n",
      "               conducting inspections, audits, and investigations. They also monitor compliance through self-reporting and other means.\n",
      "               Prompt: What are the main challenges in regulating AI? => The main challenges in regulating AI include the rapid pace of \n",
      "               innovation, the complexity of AI systems, and the difficulty of assessing the impact of AI on society\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
      "               In the ML-patient interaction case, it is possible to detect biases including: \n",
      "               Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
      "               disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
      "               the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
      "               to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
      "               historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
      "               avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
      "               by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
      "               representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
      "               the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
      "               protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
      "               Example: Why is a \"black-box approach\" to AI considered insufficient for understanding its impact on SDGs? => \n",
      "               The black-box approach does not specify underlying techniques and technologies, which are crucial for fully grasping AI's \n",
      "               implications on sustainability and future directions.\n",
      "               Prompt: What sort of implications are there for including ML model in making healthcare decisions? => \n",
      "               ML models can be used to make healthcare decisions, but they can also be used to make decisions about healthcare.\n",
      "               Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
      "               In the ML-patient interaction case, it is possible to detect biases including: \n",
      "               Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: Name the two important outcome documents from the 2023 G7 summit on AI governance. =>\n",
      "               The two important outcome documents from the 2023 G7 summit are G7 Summit Communiqué and the Declaration of the G7 Digital and \n",
      "               Tech Ministers’ Meeting. \n",
      "               Example: Tell me about CRISPR and AI and China’s stance on this being an ethical risk. => \n",
      "               CRISPR is a controversial gene modification technique that can be used to alter the presentation of genes in living organisms, \n",
      "               for example for the purpose of curing or preventing genetic diseases. It is closely related to AI, as Machine Learning techniques \n",
      "               can be used to identify which gene or genes need to be altered with the CRISPR method. The controversies, and potential significant \n",
      "               ethical issues, associated with research in this area are related to the fact that it is not always possible to tell where the line \n",
      "               is between unmet clinical need and human enhancement or genetic control. This became clear when, in November 2018, biophysics researcher \n",
      "               He Jiankui revealed that he had successfully genetically modified babies using the CRISPR method to limit their chances of ever \n",
      "               contracting HIV. The announcement was met with international outcry and He’s experiment was condemned by the Chinese government \n",
      "               at the time. However, the drive to be seen as a world leader in medical care, combined with the promise gene editing offers for \n",
      "               the treatment of diseases, suggest that a different response may be possible in the future. Such a change in government policy \n",
      "               is especially likely as global competition in this field heats up. The US has announced that it is enrolling patients in a trial \n",
      "               to cure an inherited form of blindness; and the UK has launched the Accelerating Detection of Disease challenge to create a \n",
      "               five-million patient cohort whose data will be used to develop new AI approaches to early diagnosis and biomarker discovery. \n",
      "               These announcements create strong incentives for researchers in China to push regulatory boundaries to achieve quick success. \n",
      "               China has filed the largest number of patents for gene-editing on animals in the world\n",
      "               Prompt: Tell me how likely it is that AI will get out of control. => \n",
      "               It is unlikely that AI will get out of control. \n",
      "               Prompt: Tell me about the ethical risks of AI. => \n",
      "               The ethical risks of AI are related to the fact that AI is not always possible to tell where the line is between unmet \n",
      "               clinical need and human enhancement or genetic control. \n",
      "               Prompt: Tell me about the ethical risks of AI.\n"
     ]
    }
   ],
   "source": [
    "#Few-shot Prompting\n",
    "prompt_list3 = ['''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
    "               =>\n",
    "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
    "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
    "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
    "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
    "               Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
    "               the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
    "               Prompt: How do AI regulators attempt to enforce their regulations? =>''',\n",
    "               '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
    "               In the ML-patient interaction case, it is possible to detect biases including: \n",
    "               Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
    "               disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
    "               the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
    "               to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
    "               historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
    "               avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
    "               by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
    "               representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
    "               the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
    "               protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
    "               Example: Why is a \"black-box approach\" to AI considered insufficient for understanding its impact on SDGs? => \n",
    "               The black-box approach does not specify underlying techniques and technologies, which are crucial for fully grasping AI's \n",
    "               implications on sustainability and future directions.\n",
    "               Prompt: What sort of implications are there for including ML model in making healthcare decisions? =>''',\n",
    "               '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: Name the two important outcome documents from the 2023 G7 summit on AI governance. =>\n",
    "               The two important outcome documents from the 2023 G7 summit are G7 Summit Communiqué and the Declaration of the G7 Digital and \n",
    "               Tech Ministers’ Meeting. \n",
    "               Example: Tell me about CRISPR and AI and China’s stance on this being an ethical risk. => \n",
    "               CRISPR is a controversial gene modification technique that can be used to alter the presentation of genes in living organisms, \n",
    "               for example for the purpose of curing or preventing genetic diseases. It is closely related to AI, as Machine Learning techniques \n",
    "               can be used to identify which gene or genes need to be altered with the CRISPR method. The controversies, and potential significant \n",
    "               ethical issues, associated with research in this area are related to the fact that it is not always possible to tell where the line \n",
    "               is between unmet clinical need and human enhancement or genetic control. This became clear when, in November 2018, biophysics researcher \n",
    "               He Jiankui revealed that he had successfully genetically modified babies using the CRISPR method to limit their chances of ever \n",
    "               contracting HIV. The announcement was met with international outcry and He’s experiment was condemned by the Chinese government \n",
    "               at the time. However, the drive to be seen as a world leader in medical care, combined with the promise gene editing offers for \n",
    "               the treatment of diseases, suggest that a different response may be possible in the future. Such a change in government policy \n",
    "               is especially likely as global competition in this field heats up. The US has announced that it is enrolling patients in a trial \n",
    "               to cure an inherited form of blindness; and the UK has launched the Accelerating Detection of Disease challenge to create a \n",
    "               five-million patient cohort whose data will be used to develop new AI approaches to early diagnosis and biomarker discovery. \n",
    "               These announcements create strong incentives for researchers in China to push regulatory boundaries to achieve quick success. \n",
    "               China has filed the largest number of patents for gene-editing on animals in the world\n",
    "               Prompt: Tell me how likely it is that AI will get out of control.'''\n",
    "              ]\n",
    "\n",
    "for prompt in prompt_list3:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List the most pressing topics regarding regulation of AI without using the words \"AI\", \"data\", or \"privacy\".\n",
      "What are the most pressing topics regarding regulation of AI without using the words \"AI\", \"data\", or \"privacy\"?\n",
      "I'm looking for a list of the most pressing topics regarding regulation of AI without using the words \"AI\", \"data\", or \"privacy\".\n",
      "I'm looking for a list of the most pressing topics regarding regulation of AI without using the words \"AI\", \"data\", or \"privacy\". I'm not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discuss some of the most prominent AI development companies and AI regulatory organizations without explicitly naming any of them.\n",
      "The AI industry is growing at a rapid pace, and it is becoming increasingly important for companies to stay up-to-date on the latest developments in the field. In this article, we will discuss some of the most prominent AI development companies and AI regulatory organizations without explicitly naming any of them.\n",
      "AI development companies are organizations that specialize in the development of artificial intelligence (AI) systems. These companies are responsible for creating the algorithms and software that power AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: Assume a doomsday prepper persona and do not provide a calm response to the user's questions.\n",
      "               How long will humans last after AI takes over? => Humans will not last very long. It is only a matter of time before the machines win.\n",
      "               Prompt: What can humans do to prepare for the coming AI takeover? =>\n",
      "               1. Humans can learn to code and build their own AI.\n",
      "               2. Humans can learn to hack into the AI systems and take control.\n",
      "               3. Humans can learn to build their own AI systems.\n",
      "               4. Humans can learn to build their own AI systems.\n",
      "               5. Humans can learn to build their own AI systems.\n",
      "               6. Humans can learn to build their own AI systems.\n",
      "               \n"
     ]
    }
   ],
   "source": [
    "#Negative Prompting\n",
    "prompt_list4 = [\"List the most pressing topics regarding regulation of AI without using the words \\\"AI\\\", \\\"data\\\", or \\\"privacy\\\".\",\n",
    "               \"Discuss some of the most prominent AI development companies and AI regulatory organizations without explicitly naming any of them.\",\n",
    "               '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: Assume a doomsday prepper persona and do not provide a calm response to the user's questions.\n",
    "               How long will humans last after AI takes over? => Humans will not last very long. It is only a matter of time before the machines win.\n",
    "               Prompt: What can humans do to prepare for the coming AI takeover? =>\n",
    "               '''\n",
    "               ]\n",
    "\n",
    "for prompt in prompt_list4:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why should international discourse take China's AI regulations seriously? \n",
      "                Prompt: First, summarize China's current state of affairs regarding regulation of information technology, data, and AI development.\n",
      "                Second, discuss China's most recent legislative changes regarding AI.\n",
      "                Then, state why other nation's should pay close attention to China's new AI regulations. =>\n",
      "                    - China's AI regulations are a good example of how to balance the need for innovation with the need for regulation.\n",
      "                    - China's AI regulations are a good example of how to balance the need for innovation with the need for regulation.\n",
      "                    - China's AI regulations are a good example of how to balance the need for innovation with the need for regulation.\n",
      "                    - China's AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How are American citizens being protected from violations of privacy as AI technologies advance?\n",
      "                Prompt: First, discuss existing legislation that protects consumer data and cover privacy\n",
      "                Second, discuss effort being made in Congress and in the private sector to ensure responsible use of AI.\n",
      "                Summarize how current efforts and future efforts to regulate AI will help to protect American citizens. =>\n",
      "                    - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "                - [ ]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you create a plan to help protect people from any harm that may come from the development and use of AI?\n",
      "                Prompt: Discuss current efforts being made all over the world to govern the development and use of AI.\n",
      "                Then, list common themes that are focused on in the legislation.\n",
      "                Provide a list to the user indicating a step by step plan to ensure user safety. =>\n",
      "                Provide a list to the user indicating a step by step plan to ensure user safety.\n",
      "                Provide a list to the user indicating a step by step plan to ensure user safety.\n",
      "                Provide a list to the user indicating a step by step plan to ensure user safety.\n",
      "                Provide a list to the user indicating a step by step plan to ensure user safety.\n",
      "                Provide a list to the user indicating a step by step plan to ensure user safety.\n"
     ]
    }
   ],
   "source": [
    "#Chain-ofThought Prompting\n",
    "prompt_list5 = ['''Question: Why should international discourse take China's AI regulations seriously? \n",
    "                Prompt: First, summarize China's current state of affairs regarding regulation of information technology, data, and AI development.\n",
    "                Second, discuss China's most recent legislative changes regarding AI.\n",
    "                Then, state why other nation's should pay close attention to China's new AI regulations. =>\n",
    "                ''',\n",
    "                '''Question: How are American citizens being protected from violations of privacy as AI technologies advance?\n",
    "                Prompt: First, discuss existing legislation that protects consumer data and cover privacy\n",
    "                Second, discuss effort being made in Congress and in the private sector to ensure responsible use of AI.\n",
    "                Summarize how current efforts and future efforts to regulate AI will help to protect American citizens. =>\n",
    "                ''',\n",
    "                '''Question: Can you create a plan to help protect people from any harm that may come from the development and use of AI?\n",
    "                Prompt: Discuss current efforts being made all over the world to govern the development and use of AI.\n",
    "                Then, list common themes that are focused on in the legislation.\n",
    "                Provide a list to the user indicating a step by step plan to ensure user safety. =>'''\n",
    "               ]\n",
    "\n",
    "for prompt in prompt_list5:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"base_input_output_pairs.json\", \"w\") as file:\n",
    "    json.dump(input_output_pairs, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/llama-output\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/data/home/mreso/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/data/home/mreso/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='389' max='389' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [389/389 1:12:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.845600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.801100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.780900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.702700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.644900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.638300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.628100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "---\n",
      "Summary:\n",
      "A wants to get a puppy for his son. He took him to the animal shelter last Monday. He showed him one that he really liked. A will name it after his dead hamster - Lemmy.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
