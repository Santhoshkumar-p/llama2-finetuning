{"input":" How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?","output":"China's AI regulations prioritize information control through measures like barring excessive price discrimination in recommendation algorithms, requiring labels on synthetically generated content in deep synthesis, and demanding \"true and accurate\" data and outputs in generative AI.","context":"China\u2019s three most concrete and impactful regulations on algorithms and AI are its 2021\nregulation on recommendation algorithms, the 2022 rules for deep synthesis (synthetically generated content), and the 2023 draft rules on generative AI. Information control is a central goal of all three measures, but they also contain many other notable provisions. The rules for recommendation algorithms bar excessive price discrimination and protect the rights of workers subject to algorithmic scheduling. \nThe deep synthesis regulation requires conspicuous labels be placed on synthetically generated content. And the draft generative AI regulation requires both the training data and model outputs to be \u201ctrue and accurate,\u201d Summary: China is in the midst of rolling out some of the world\u2019s earliest and most detailed regulations governing artificial intelligence (AI). These include measures governing recommendation algorithms\u2014the most omnipresent form of AI deployed on the internet\u2014as well as new rules for synthetically generated images and chatbots in the mold of ChatGPT. \nChina\u2019s emerging AI governance framework will reshape how the technology is built and deployed within China and internationally, impacting both Chinese technology exports and global AI research networks.\nBut in the West, China\u2019s regulations are often dismissed as irrelevant or seen purely through the lens of a geopolitical competition to write the rules for AI. These extremely demanding requirements for generative AI systems have kicked off a particularly active public debate on the draft regulation. At the time of writing, Chinese scholars, companies, and policymakers are actively discussing how to maintain effective content controls without squashing China\u2019s nascent generative AI industry. The third paper in this series will dive deep into how this policy debate is playing out in public workshops, academic writing, and corporate lobbying.\nCountries and cultures may differ on the specific content of AI regulations, but they can learn from the content-agnostic structure of the regulations themselves. The above Chinese regulations share three structural similarities: the choice of algorithms as a point of entry; the building of regulatory tools and bureaucratic know-how; and the vertical and iterative approach that is laying the groundwork for a capstone AI law. Three regulations require the deepest analysis: recommendation algorithms, \u201cdeep synthesis,\u201d and generative AI. These interconnected documents contain the most targeted and impactful regulations to date, creating concrete requirements for how algorithms and AI are built and deployed in China. Below is a brief overview of each regulation. The remainder of this paper and subsequent papers will expand on the intellectual roots and key bureaucratic actors behind these regulations.\nProvisions on the Management of Algorithmic Recommendations in Internet Information Services"}
{"input":"Why should international discourse take China's AI regulations seriously?","output":"China's AI regulations reshape global AI deployment and offer insights into effective governance tools, influencing various sectors worldwide.","context":"But international discourse on Chinese AI governance often fails to take these regulations seriously, to engage with either their content or the policymaking process. \nInternational commentary often falls into one of two traps: dismissing China\u2019s regulations as irrelevant or using them as a political prop. Analysts and policymakers in other countries often treat them as meaningless pieces of paper. \nPresident Xi Jinping and the Chinese Communist Party (CCP) have unchecked power to disregard their own rules, the argument goes, and therefore the regulations are unimportant. \nOther U.S. policy actors use the specter of Chinese AI governance to advance their agendas. When Senate Majority Leader Chuck Schumer announced his plans to begin regulating AI earlier this year, he described China\u2019s efforts as a \u201cwake up call to the nation, \u201d warning that the United States could not afford to let its geopolitical adversary \u201cwrite the rules of the road\u201d for AI. 2 AI legislation in China How China Sets AI Governance Policy\nThis paper presents a four-layered policy funnel through which China formulates and pro-mulgates AI governance regulations. Those four layers are real-world roots; Xi Jinping and CCP ideology; the \u201cworld of ideas\u201d; and the party and state bureaucracies. These layers are porous, and regulations do not proceed through them in a purely linear fashion.\n\nInstead, they often pinball forward and backward through these layers, getting shaped and reshaped by academics, bureaucrats, public opinion, and CCP ideology. The order and relative importance of the layers also varies depending on the nature of the issue confronted.\n\nSo far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers. Nevertheless, in the area of AI regulation there is still a relatively large space for policy debates. This is perhaps due to the relatively technical nature of policies and to the freshness of the problems. How to effectively regulate AI remains a wide-open question globally, and the political interests at play in China are not yet entrenched. Ministries and state-owned enterprises have not spent decades fighting to gain leverage or to hang onto preferential policies they have carved out. This mix of factors has made public debates over AI governance unusually lively and open."}
{"input":"What are the structural similarities found in China's AI regulations?","output":"China's AI regulations share three structural similarities: they focus on algorithms as a point of entry, build regulatory tools and bureaucratic know-how, and adopt a vertical and iterative approach towards regulation.","context":"2 AI legislation in China And despite China\u2019s drastically different political system, policymakers in the United States and elsewhere can learn from its regulations. China\u2019s regulations create new bureaucratic and technical tools: disclosure requirements, model auditing mechanisms, and technical per-formance standards. These tools can be put to different uses in different countries, ranging from authoritarian controls on speech to democratic oversight of automated decisionmaking.\nCharting the successes, failures, and technical feasibility of China\u2019s AI regulations can give policymakers elsewhere a preview of what is possible and what might be pointless when it comes to governing AI.\nSo what do China\u2019s AI regulations contain? How did its massive party and state bureaucracies formulate them? And is it possible to predict where Chinese AI governance is headed?\nThis is the first in a series of three papers that will tackle these questions using a novel approach: reverse engineering. How China Sets AI Governance Policy\nThis paper presents a four-layered policy funnel through which China formulates and pro-mulgates AI governance regulations. Those four layers are real-world roots; Xi Jinping and CCP ideology; the \u201cworld of ideas\u201d; and the party and state bureaucracies. These layers are porous, and regulations do not proceed through them in a purely linear fashion.\n\nInstead, they often pinball forward and backward through these layers, getting shaped and reshaped by academics, bureaucrats, public opinion, and CCP ideology. The order and relative importance of the layers also varies depending on the nature of the issue confronted.\n\nSo far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers. But in the West, China\u2019s regulations are often dismissed as irrelevant or seen purely through the lens of a geopolitical competition to write the rules for AI. \nInstead, these regulations deserve careful study on how they will affect China\u2019s AI trajectory and what they can teach policymakers around the world about regulating the technology. Even if countries fundamentally disagree on the specific content of a regulation, they can still learn from each other when it comes to the underlying structures and technical feasibility of different regulatory approaches.\nIn this series of three papers, I will attempt to reverse engineer Chinese AI governance."}
{"input":" Why was the Artificial Intelligence Act introduced in Europe?","output":"To ensure safe, rights-respecting AI while promoting innovation.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. Introduction: MEPs reached a political deal with the Council on a bill to ensure AI in Europe is safe, respects fundamental rights and democracy, while businesses can thrive and expand. On Friday, Parliament and Council negotiators reached a provisional agreement on the Artificial Intelligence Act. This regulation aims to ensure that fundamental rights, democracy, the rule of law and environmental sustainability are protected from high risk AI, while boosting innovation and making Europe a leader in the field. The rules establish obligations for AI based on its potential risks and level of impact. Printer of Acts of Parliament.\n228. Equality Act 2010. Statute Law Database.\n229. Human Rights Act 1998. Statute Law Database.\n230. Copyright, Designs and Patents Act 1988. Statute Law Database.\n231. Competition and Markets Authority et al. (2023). The Digital Regulation Cooperation Forum. \nGOV.UK.\n232. European Parliament (2023). EU AI Act: first regulation on artificial intelligence. European \nParliament.\n233. Blueprint for an AI Bill of Rights | OSTP. The White House.\n234. The White House (2023). FACT SHEET: President Biden Issues Executive Order on Safe, Secure, \nand Trustworthy Artificial Intelligence. The White House.\n235. Andrews, E. (2023). Liberty\u2019s Written Submission to a pro- innovation approach to AI \nregulation consultation. Liberty.\n24                                  POSTnote 708 - 9 January 2024"}
{"input":"What applications are banned under the Act introduced in Europe?","output":"Banned applications include sensitive biometric categorization, facial recognition scraping, emotion recognition in workplaces, social scoring, and AI manipulation.","context":"The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition. Printer of Acts of Parliament.\n228. Equality Act 2010. Statute Law Database.\n229. Human Rights Act 1998. Statute Law Database.\n230. Copyright, Designs and Patents Act 1988. Statute Law Database.\n231. Competition and Markets Authority et al. (2023). The Digital Regulation Cooperation Forum. \nGOV.UK.\n232. European Parliament (2023). EU AI Act: first regulation on artificial intelligence. European \nParliament.\n233. Blueprint for an AI Bill of Rights | OSTP. The White House.\n234. The White House (2023). FACT SHEET: President Biden Issues Executive Order on Safe, Secure, \nand Trustworthy Artificial Intelligence. The White House.\n235. Andrews, E. (2023). Liberty\u2019s Written Submission to a pro- innovation approach to AI \nregulation consultation. Liberty.\n24                                  POSTnote 708 - 9 January 2024 A broad-based national AI law like the EU Act is unlikely over the next few years. The administration likely builds on its AI \u201cbill of rights\u201d that spans different sectors, voluntary commitments, and executive orders. Voluntary rules are seen by many as a stop-gap measure, but a divided Congress is unlikely to pass a major law with new mandatory rules.  \n\nActions addressing sensitive areas such as health care, financial services (lending, insurance, housing), work force practices (discrimination), and child safety are more likely. Executive branch agencies are likely to move forward with existing authorities in the absence of new laws. \n\nPrivate tech companies advance their own responsible AI initiatives to serve their customers and avoid tougher government action. These are largely voluntary and seen as insufficient. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":"Any exceptions for law enforcement introduced in Europe regarding biometric systems?","output":" Yes, with judicial authorization for targeted searches of specific serious crimes.","context":"adequate risk assessment and mitigation systems;\nhigh quality of the datasets feeding the system to minimise risks and discriminatory outcomes;\nlogging of activity to ensure traceability of results;\ndetailed documentation providing all information necessary on the system and its purpose for authorities to assess its compliance;\nclear and adequate information to the user;\nappropriate human oversight measures to minimise risk;\nhigh level of robustness, security and accuracy.\nAll remote biometric identification systems are considered high risk and subject to strict requirements. The use of remote biometric identification in publicly accessible spaces for law enforcement purposes is, in principle, prohibited.\n\nNarrow exceptions are strictly defined and regulated, such as when necessary to search for a missing child, to prevent a specific and imminent terrorist threat or to detect, locate, identify or prosecute a perpetrator or suspect of a serious criminal offence. Services Institute.\n145. Metropolitan Police (online). Live Facial Recognition.\n146. Fussey, P. et al. (2019). Independent Report on the London Metropolitan Police Service\u2019s Trial \nof Live Facial Recognition Technology. University of Essex Human Rights Centre.\n147. Selinger, E. et al. (2019). The Inconsentability of Facial Surveillance. Loyola Law Rev., Vol \n66, 101\u2013122.\n148. Almeida, D. et al. (2022). The ethics of facial recognition technologies, surveillance, and \naccountability in an age of artificial intelligence: a comparative analysis of US, EU, and UK \nregulatory frameworks. Ai Ethics, Vol 2, 377\u2013387.\n149. Fontes, C. et al. (2021). Ethics of surveillance: harnessing the use\nof live facial recognition technologies in public spaces for law enforcement. Institute for Ethics \nin Artificial Intelligence, Technical University of Munich.\n150. Mobilio, G. (2023). Your face is not new to me \u2013 Regulating the surveillance power of facial in Artificial Intelligence, Technical University of Munich.\n150. Mobilio, G. (2023). Your face is not new to me \u2013 Regulating the surveillance power of facial \nrecognition technologies. Internet Policy Rev., Vol 12.\n151. Ovide, S. (2020). A Case for Banning Facial Recognition. The New York Times.\n152. Murray, D. (2019). Live facial recognition: the impact on human rights and participatory \ndemocracy. University of Essex.\n153. Minderoo Centre for Technology and Democracy (2022). A Sociotechnical Audit: Assessing Police \nuse of Facial Recognition. mctd.ac.uk.\n154. Ada Lovelace Institute (2019). Beyond face value: public attitudes to facial recognition \ntechnology.\n155. Big Brother Watch Team (2023). 65 parliamentarians call for\n\u201cimmediate stop\u201d to live facial\n21                                  POSTnote 708 - 9 January 2024 Banned applications: Recognising the potential threat to citizens\u2019 rights and democracy posed by certain applications of AI, the co-legislators agreed to prohibit: biometric categorisation systems that use sensitive characteristics (e.g. political, religious, philosophical beliefs, sexual orientation, race); untargeted scraping of facial images from the internet or CCTV footage to create facial recognition databases; emotion recognition in the workplace and educational institutions; social scoring based on social behaviour or personal characteristics; AI systems that manipulate human behaviour to circumvent their free will; AI used to exploit the vulnerabilities of people (due to their age, disability, social or economic situation)."}
{"input":" Why is the AI Act proposed by the European Commission?","output":"The Act addresses AI risks, sets clear rules, and reduces burdens for businesses.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":"How does the AI Act proposed by the European Commission ensure trust in AI for Europeans?","output":" It guarantees safety, protects rights, and promotes AI uptake and innovation.","context":"24 |\nThe High-Level Expert Group on Artificial Intelligence (AI HLEG)\nResponsible entity: European Commission (EC)\nDescription: The High-Level Expert Group on Artificial Intelligence (AI HLEG)\nwas convened by the European Commission in 2016 to support the\nimplementation of the European Strategy on AI and completed its mandate in\nJuly 2020. The AI HLEG comprised representatives from academia, civil society,\nand industry that produced three outputs: the April 2019 Ethics Guidelines for\nTrustworthy Artificial Intelligence, the June 2019 Policy and Investment\nRecommendations for Trustworthy Artificial Intelligence, and the July 2020\nAssessment List for Trustworthy Artificial Intelligence (ALTAI). The European AI\nAlliance is a multi-stakeholder forum for engaging in a broad and open\ndiscussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. research collaboration and advance the development of trustworthy AI.\nMany European Union member states are also participating in European AI\nresearch projects and networks such as BVDA\/EURobotics, the Confederation\nof Laboratories for Artificial Intelligence Research in Europe (CLAIRE) and the\nEuropean Laboratory for Learning and Intelligent Systems (ELLIS). AI is also a\npriority in Horizon Europe, the EU\u2019s current framework programme for research\nand innovation."}
{"input":" What are the risk categories in AI, and how are they regulated by the European Commission?","output":"Four levels - Bans threats, strict obligations for high-risk, transparency for limited risk, free use for minimal risk.","context":"Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7]"}
{"input":"Why did the UK government publish the AI regulatory framework, and what are its objectives?","output":"The framework aims to be pro-innovation, addressing AI risks with context-specific principles. Objectives include driving growth, increasing public trust, and positioning the UK as a global AI leader.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, Governments should look beyond traditional regulatory techniques to influence AI developments"}
{"input":"What is unique about the UK's approach to defining AI in the framework?","output":"The framework defines AI by adaptivity and autonomy, not offering a legal definition. It aims to future-proof against emerging technologies.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. Objectives:\nThe framework is designed to achieve the following three objectives:\nDrive growth and prosperity: By reducing regulatory uncertainty and removing existing barriers to innovation, the UK government aims to allow AI companies to capitalize on early development successes and achieve long-term market advantage. There is clearly a competitive urgency in the government\u2019s proposals: \u201cBy acting now, we can give UK innovators a head start in the global race to convert the potential of AI into long term advantages for the UK, maximising the economic and social value of these technologies and strengthening our current position as a world leader in AI.\u201d\nIncrease public trust in AI: By effectively addressing risks, the UK government\u2019s goal is to remove barriers for AI products and innovation caused by a lack of trust in AI. signed by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety.\nThe rapid advancement of AI capabilities means that policy implications are continuously shifting. \nSome stakeholders have called for further Government action.\n* The Kings Speech announced an Automated Vehicles Bill that seeks to introduce new legal \nframeworks to support safe commercial developments of self-driving vehicles.16\n\u2020 An exascale supercomputer is a computer that can perform 1018 (a quintillion) operations per \nsecond.\n2                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nTable: Definitions of AI and related concepts\nDefinitions are not universally agreed, are continuously evolving, and are linked.\nTerm                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability,"}
{"input":"How does the UK AI regulatory framework regulate high-risk AI systems entering the market?","output":"It focuses on outcomes, not technology, with a step-by-step process and obligations like risk assessment, transparency, and human oversight.","context":"largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":"What are some possible risks of the widespread adoption and use of Generative AI?","output":"The widespread adoption and use of Generative AI can pose several risks in terms of policy and directly affecting humans. In the development of Generative AI, these include: job displacement, possible threats to worker privacy, worker exploitation, algorithmic bias, generation of misinformation, use in mass surveillance, use in cyber attacks, and replacement of human decision-making.","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d environmental toll. The Guardian.\n202. Hsu, J. (2023). Shifting where data is processed for AI can reduce environmental harm. New \nScientist.\n203. M\u00fcller, V. C. et al. (2016). Future Progress in Artificial Intelligence: A Survey of Expert \nOpinion. in Fundamental Issues of Artificial Intelligence. (ed. M\u00fcller, V. C.) 555\u2013572. Springer \nInternational Publishing.\n204. Bubeck, S. et al. (2023). Sparks of Artificial General Intelligence: Early experiments with \nGPT-4. arXiv.\n205. Metz, C. (2023). Microsoft Says New AI Shows Signs of Human Reasoning. The New York Times.\n206. Nathan, A. et al. (2023). Generative AI: hype, or truly transformative? Goldman Sachs.\n23                                  POSTnote 708 - 9 January 2024 Risk of Extinction. At the same time, AI academic computer scientists like Yoshua Bengio and Geoffrey Hinton, who are the Turing prize-winning \u201cgodfathers\u201d of generative AI, warn of dramatic risks that threaten humanity\u2019s very existence. Bengio, Hinton, Bill Gates, top executives from Google, Microsoft, OpenAI and many AI luminaries including from China and Russia have signed a statement stating \u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\u201d There are also proposals to pause developing more powerful AI models than ChatGPT 4. Elon Musk has said, \u201cWith AI we\u2019re summoning the demon.\u201d The precautionary principle suggests low-probability, high-impact outcomes should be taken seriously. Measures for the Management of Generative Artificial Intel igence Services At first glance, China\u2019s regulatory apparatus appeared well prepared for the wave of generative AI applications that would follow ChatGPT. The deep synthesis regulation technically included most forms of generative AI, such as using the technology to create or edit images, videos, voice, and text."}
{"input":"What are some possible benefits of the widespread adoption and use of Generative AI?","output":"The widespread adoption and use of Generative AI presents several possible benefits to society, including: increased efficiency, innovation, providing assistance in multiple domains, devising personalized medical treatments, creating personalized education, and decreasing the information gap.","context":"environmental toll. The Guardian.\n202. Hsu, J. (2023). Shifting where data is processed for AI can reduce environmental harm. New \nScientist.\n203. M\u00fcller, V. C. et al. (2016). Future Progress in Artificial Intelligence: A Survey of Expert \nOpinion. in Fundamental Issues of Artificial Intelligence. (ed. M\u00fcller, V. C.) 555\u2013572. Springer \nInternational Publishing.\n204. Bubeck, S. et al. (2023). Sparks of Artificial General Intelligence: Early experiments with \nGPT-4. arXiv.\n205. Metz, C. (2023). Microsoft Says New AI Shows Signs of Human Reasoning. The New York Times.\n206. Nathan, A. et al. (2023). Generative AI: hype, or truly transformative? Goldman Sachs.\n23                                  POSTnote 708 - 9 January 2024 Measures for the Management of Generative Artificial Intel igence Services At first glance, China\u2019s regulatory apparatus appeared well prepared for the wave of generative AI applications that would follow ChatGPT. The deep synthesis regulation technically included most forms of generative AI, such as using the technology to create or edit images, videos, voice, and text. certain highly capable models can be proliferated. Ultimately, governments could buy a seat at the table by providing economic incentives to companies in exchange for more influence in ensuring that AI is used for the good of all. Term                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI\nregulation\u2019 defined AI, AI systems or AI technologies as \u201cproducts and services that are \n\u2018adaptable\u2019 and \u2018autonomous\u2019.\u201d The adaptability of AI refers to AI systems, after being trained, \noften developing the ability to perform new ways of finding patterns and\nconnections in data that are not directly envisioned by their human\nprogrammers. The autonomy of AI refers to some AI systems that can make decisions without the \nintent or ongoing control of a human (PB 57).\nGenerative AI               The Alan Turing Institute defines generative AI as an \u201cartificial\nintelligence system that generates text, images, audio, video or other media in response to user \nprompts.\u201d26 Generative AI applications include chatbots, such as OpenAI\u2019s ChatGPT, photo and video\nfilters, and virtual assistants (PB 57)."}
{"input":"What are some regulations that the UK might implement in the future to manage the growth and use of Generative AI?","output":"In the future, the UK may introduce regulation to regulate the use of Generative AI in automated decision-making, use in live facial recognition, ensure access to model code and documentation, increase model transparency, protect copyrighted material, assess algorithmic bias, and protect consumer privacy.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, Audits. Information\nCommissioner\u2019s Office.\n272. Digital Regulation Cooperation Forum (2022). Auditing algorithms: the existing landscape, role \nof regulators and future outlook. GOV.UK.\n273. Intellectual Property Office (2022). Artificial Intelligence and Intellectual Property: \ncopyright and patents: Government response to consultation. GOV.UK.\n274. Intellectual Property Office (2023). The government\u2019s code of practice on copyright and AI. \nGOV.UK.\n275. Aitken, M. et al. (2022). Common Regulatory Capacity for AI. The Alan Turing Institute.\n276. Jung, C. et al. (2023). Artificial intelligence for public value creation. Inst. Public Policy \nRes.,\n277. Roberts, H. et al. (2023). Artificial intelligence regulation in the United Kingdom: a path to \ngood governance and global leadership? Internet Policy Rev., Vol 12.\n278. Minderoo Centre for Technology and Democracy (2023). Policy Brief: Generative AI. mctd.ac.uk.\n279. Equality and Human Rights Commission (2023). AI"}
{"input":"What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?","output":"The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, private sector, academia, and civil society to harness AI's benefits while mitigating its risks.","context":"Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies with all Federal laws and to promote robust technical evaluations, careful oversight, engagement with affected communities, and rigorous regulation.  It is necessary to hold those developing and deploying AI accountable to standards that protect against unlawful discrimination and abuse, including in the justice system and the Federal Government.  Only then can Americans trust AI to advance civil rights, civil liberties, equity, and justice for all. (i)     the requirement to designate at each agency within 60 days of the issuance of the guidance a Chief Artificial Intelligence Officer who shall hold primary responsibility in their agency, in coordination with other responsible officials, for coordinating their agency\u2019s use of AI, promoting AI innovation in their agency, managing risks from their agency\u2019s use of AI, and carrying out the responsibilities described in section 8(c) of Executive Order 13960 of December 3, 2020 (Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government), and section 4(b) of Executive Order 14091;\n\n          (ii)    the Chief Artificial Intelligence Officers\u2019 roles, responsibilities, seniority, position, and reporting structures;"}
{"input":"How does the administration plan to address the potential benefits and risks associated with AI?","output":"The Biden administration plans to address the potential benefits and risks associated with AI by advancing a coordinated, Federal Government-wide approach. This approach emphasizes safety and responsibility in AI development and use, involving collaboration across government, private sector, academia, and civil society. The goal is to harness AI's benefits for society while mitigating its substantial risks, ensuring that AI development reflects the principles, ideals, and diversity of America.","context":"or jeopardize safety or rights.  At the same time, my Administration will promote responsible uses of AI that protect consumers, raise the quality of goods and services, lower their prices, or expand selection and availability. The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is What should government policymakers do to protect humanity?\nThe potential dangers posed by AI are many. At the extreme, they include the threat of human extinction, which could come about by an AI-enabled catastrophe, such as a well-designed virus that spreads easily, evades detection, and destroys our civilization. Less dire, but considerably worrisome, is the threat to democratic governance if AIs gain power over people.[2] Even if AIs do not kill humans or overturn democracy, authoritarian regimes, terrorist groups, and organized crime groups could use AI to cause great harm by spreading disinformation and manipulating public opinion. Governments need to view the AI landscape as a regulatory training ground in preparation for the threats posed by even more-advanced AI capabilities, including the potential arrival of artificial general intelligence."}
{"input":"How will the administration support American workers in the age of AI?","output":"The administration plans to support workers by adapting job training and education for AI-created opportunities, ensuring collective bargaining, and preventing AI deployments that undermine worker rights or introduce new risks, aiming for responsible AI uses that augment human work and improve life quality.","context":"(c)  The responsible development and use of AI require a commitment to supporting American workers.  As AI creates new jobs and industries, all workers need a seat at the table, including through collective bargaining, to ensure that they benefit from these opportunities.  My Administration will seek to adapt job training and education to support a diverse workforce and help provide access to opportunities that AI creates.  In the workplace itself, AI should not be deployed in ways that undermine rights, worsen job quality, encourage undue worker surveillance, lessen market competition, introduce new health and safety risks, or cause harmful labor-force disruptions.  The critical next steps in AI development should be built on the views of workers, labor unions, educators, and employers to support responsible uses of AI that improve workers\u2019 lives, positively augment human work, and help all people safely enjoy the gains and opportunities from technological innovation. My Administration places the highest urgency on governing the development and use of AI safely and responsibly, and is therefore advancing a coordinated, Federal Government-wide approach to doing so.  The rapid speed at which AI capabilities are advancing compels the United States to lead in this moment for the sake of our security, economy, and society.\n\n     In the end, AI reflects the principles of the people who build it, the people who use it, and the data upon which it is built.  I firmly believe that the power of our ideals; the foundations of our society; and the creativity, diversity, and decency of our people are the reasons that America thrived in past eras of rapid change.  They are the reasons we will succeed again in this moment.  We are more than capable of harnessing AI for justice, security, and opportunity for all. (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with"}
{"input":"What challenges do technology, data, and automated systems pose to democracy and the rights of the American public?","output":"Automated systems, while capable of benefits, also pose significant threats to democracy and the rights of the American public by limiting opportunities and access to essential resources or services. These systems can reproduce existing inequities or embed new biases and discrimination, and unchecked data collection can undermine privacy and track activity without consent.","context":"To advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by From Principles to Practice\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, or access to critical needs. limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is adopted, deployed, and used. Blueprint for an AI Bill of Rights\nMAKING AUTOMATED SYSTEMS WORK FOR THE AMERICAN PEOPLE\n\nAmong the great challenges posed to democracy today is the use of technology, data, and automated systems in ways that threaten the rights of the American public. Too often, these tools are used to limit our opportunities and prevent our access to critical resources or services. These problems are well documented. In America and around the world, systems supposed to help with patient care have proven unsafe, ineffective, or biased. Algorithms used in hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embed new harmful bias and discrimination. Unchecked social media data collection has been used to threaten people\u2019s opportunities, undermine their privacy, or pervasively track their activity\u2014often without their knowledge or consent. Policy implications of artificial intelligence (AI)\nethical concerns, while others argue that the lack of transparency may make it less likely that \ncertain people, particularly those from digitally excluded communities, would be subject to a fair \ndecision.241\nSome legal academics have stated that mechanisms already exist in UK law (Section 49 of the Data \nProtection Act 2018)227 to ensure human oversight in automated decision-making.242\nHowever, other academics and civil society groups have raised concerns that in many systems, even \nwhen a human is involved in compliance with the law, they defer their judgement to AI.239,242\nThe Data Protection and Digital Information Bill,243 carried into the 2023-2024 parliamentary \nsession, which is intended by the Government to reduce burdens on businesses,244 has been \ncriticised by some civil society groups for \u201cwatering down protections\u201d245,246 present in current \nlaw against automated decision-making."}
{"input":"How can automated systems be designed to ensure they are safe and effective?","output":"Automated systems should be developed with diverse community consultation and undergo pre-deployment testing, risk identification, and ongoing monitoring. They must adhere to domain-specific standards and be evaluated independently to ensure safety and effectiveness, including mitigating potential harms beyond their intended use.","context":"You should be protected from unsafe or ineffective systems. Automated systems should be developed with consultation from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts of the system. Systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based on their intended use, mitigation of unsafe outcomes including those beyond the intended use, and adherence to domain-specific standards. Outcomes of these protective measures should include the possibility of not deploying the system or removing a system from use. Automated systems should not be designed with an intent or reasonably foreseeable possibility of endangering your safety or the safety of your community. They should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems. You should be protected from Safe and Effective Systems safety of your community. They should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems. You should be protected from inappropriate or irrelevant data use in the design, development, and deployment of automated systems, and from the compounded harm of its reuse. Independent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible. input determining the outcome. Automated systems should provide explanations that are technically valid, meaningful and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context. Reporting that includes summary information about these automated systems in plain language and assessments of the clarity and quality of the notice and explanations should be made public whenever possible."}
{"input":"How can data privacy be ensured in the design and deployment of automated systems?","output":"Data privacy can be ensured by incorporating built-in protections by default, collecting only necessary data, and respecting individual decisions on data use. Systems should avoid privacy-invasive defaults and ensure consent processes are meaningful, with enhanced protections in sensitive domains.","context":"Data Privacy You should be protected from abusive data practices via built-in protections and you should have agency over how data about you is used. You should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected. Designers, developers, and deployers of automated systems should seek your permission and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used. Systems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases where it can be appropriately and meaningfully Privacy and data protection\nAccountability\nSafety and security\nTransparency and explainability\nFairness\nHuman control of technology\nProfessional responsibility\nPromotion of human values\n\n\u2022\n\nPRIVACY AND DATA PROTECTION. AI solutions should respect an individual\u2019s right to privacy and civil liberties. Individuals should have control over\ntheir data, with their consent required for repurposing, redistributing or erasure.\n\n\u2022\n\nACCOUNTABILITY. Mechanisms must ensure accountable behavior during the life cycle of AI design and implementation. An\nagency or body should be responsible for monitoring accountability.\n\n\u2022\n\nSAFETY AND SECURITY. Cybersecurity is critical. Leaders must ensure the well-being of society at large and private individual\nhumans.\n\n\u2022 safety of your community. They should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems. You should be protected from inappropriate or irrelevant data use in the design, development, and deployment of automated systems, and from the compounded harm of its reuse. Independent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible."}
{"input":"What new policies has the Department of Homeland Security (DHS) announced to ensure the responsible use of artificial intelligence (AI)?","output":"The DHS announced policies developed by the DHS Artificial Intelligence Task Force (AITF) that establish key principles for the responsible use of AI, including the handling of face recognition technology, extensive testing, and oversight.","context":"\u201cArtificial intelligence is a powerful tool we must harness effectively and responsibly,\u201d said Secretary of Homeland Security Alejandro N. Mayorkas. \u201cOur Department must continue to keep pace with this rapidly evolving technology, and do so in a way that is transparent and respectful of the privacy, civil rights, and civil liberties of everyone we serve. I am grateful that Eric Hysen, who already co-chairs our Artificial Intelligence Task Force, has agreed to serve as our Department\u2019s first Chief AI Officer, providing the leadership and experience necessary to harness AI\u2019s enormous potential and ensure its responsible use across DHS.\u201d\n\n\u202f\u201cArtificial intelligence provides the Department with new ways to carry out our mission to secure the homeland,\u201d said CIO and Chief AI Officer Eric Hysen. \u201cThe policies we are announcing today will ensure that the Department\u2019s use of AI is free from discrimination and in full compliance with the law, ensuring that we retain the public's trust.\u201d DHS Names First Chief Artificial Intelligence Officer\u202f\n\n\u202fWASHINGTON \u2013 Today, the Department of Homeland Security (DHS) announced new policies to ensure responsible use of artificial intelligence (AI) by the Department. The policies, developed by the DHS Artificial Intelligence Task Force (AITF), build on a commitment from the Biden-Harris Administration to manage the risk and harness the benefits of AI. The Department uses AI technologies to advance its missions, including combatting fentanyl trafficking, strengthening supply chain security, countering child sexual exploitation, and protecting critical infrastructure. These new policies establish key principles for the responsible use of AI and specify how DHS will ensure that its use of face recognition and face capture technologies is subject to extensive testing and oversight. Policy Statement 139-06 \u201cAcquisition and Use of Artificial Intelligence and Machine Learning by DHS Components:\u201d The policy statement establishes the foundation for DHS\u2019s use of AI with a clear set of principles. These principles include that DHS systems, programs, and activities using AI will conform to the requirements of Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in\u202fthe Federal Government. In addition, DHS will only acquire and use AI in a manner that is consistent with the Constitution and all other applicable laws and policies. Also, DHS will not collect, use, or disseminate data used in AI activities, or establish AI-enabled\u202fsystems that make or support decisions, based on the inappropriate consideration of race, ethnicity, gender, national origin, religion, gender, sexual orientation, gender identity, age, nationality, medical condition, or disability. DHS Announces New Policies and Measures Promoting Responsible Use of Artificial Intelligence\nRelease Date: September 14, 2023\nNew Directive Outlines How the Department Will Use Face Recognition Technology\n\nDHS Names First Chief Artificial Intelligence Officer"}
{"input":"What are the key principles established by the new DHS policy statement regarding the use of AI?","output":"The new policy statement insists that DHS systems using AI will adhere to the requirements of Executive Order 13960, conform to the Constitution, applicable laws, policies, and avoid discrimination based on race, ethnicity, gender, and other inappropriate considerations.","context":"Policy Statement 139-06 \u201cAcquisition and Use of Artificial Intelligence and Machine Learning by DHS Components:\u201d The policy statement establishes the foundation for DHS\u2019s use of AI with a clear set of principles. These principles include that DHS systems, programs, and activities using AI will conform to the requirements of Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in\u202fthe Federal Government. In addition, DHS will only acquire and use AI in a manner that is consistent with the Constitution and all other applicable laws and policies. Also, DHS will not collect, use, or disseminate data used in AI activities, or establish AI-enabled\u202fsystems that make or support decisions, based on the inappropriate consideration of race, ethnicity, gender, national origin, religion, gender, sexual orientation, gender identity, age, nationality, medical condition, or disability. To learn more about how DHS uses AI technologies to protect the country, visit DHS.gov. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed."}
{"input":"What tasks were assigned to the Homeland Security Advisory Council regarding the development of the Department\u0092s AI Strategy?","output":"The Homeland Security Advisory Council was tasked to examine and provide recommendations on the Department\u0092s AI Strategy, focusing on leveraging AI for critical missions and building defenses against its nefarious use by adversaries.","context":"In March 2023, Secretary Mayorkas tasked the Homeland Security Advisory Council to\u202fexamine and provide recommendations on the development of the Department\u2019s AI Strategy. The Council has formed two subcommittees, one to focus on how DHS can leverage AI to advance critical missions, and the second on how DHS should be building defenses to nefarious uses of AI by adversaries. The Council provides recommendations to the Secretary on ways the Department can better meet the challenges of the evolving threat landscape and seize opportunities to better serve the American people. \u201cArtificial intelligence is a powerful tool we must harness effectively and responsibly,\u201d said Secretary of Homeland Security Alejandro N. Mayorkas. \u201cOur Department must continue to keep pace with this rapidly evolving technology, and do so in a way that is transparent and respectful of the privacy, civil rights, and civil liberties of everyone we serve. I am grateful that Eric Hysen, who already co-chairs our Artificial Intelligence Task Force, has agreed to serve as our Department\u2019s first Chief AI Officer, providing the leadership and experience necessary to harness AI\u2019s enormous potential and ensure its responsible use across DHS.\u201d\n\n\u202f\u201cArtificial intelligence provides the Department with new ways to carry out our mission to secure the homeland,\u201d said CIO and Chief AI Officer Eric Hysen. \u201cThe policies we are announcing today will ensure that the Department\u2019s use of AI is free from discrimination and in full compliance with the law, ensuring that we retain the public's trust.\u201d (iii)  recommendations for the appropriate use of enlistment authority under 10 U.S.C. 504(b)(2) for experts in AI and other critical and emerging technologies; and\n\n          (iv)   recommendations for the Department of Defense and the Department of Homeland Security to work together to enhance the use of appropriate authorities for the retention of certain noncitizens of vital importance to national security by the Department of Defense and the Department of Homeland Security.  \n\n\n     Sec. 11.  Strengthening American Leadership Abroad.  (a)  To strengthen United States leadership of global efforts to unlock AI\u2019s potential and meet its challenges, the Secretary of State, in coordination with the Assistant to the President for National Security Affairs, the Assistant to the President for Economic Policy, the Director of OSTP, and the heads of other relevant agencies as appropriate, shall: (v)    The Secretary of Homeland Security shall establish an Artificial Intelligence Safety and Security Board as an advisory committee pursuant to section 871 of the Homeland Security Act of 2002 (Public Law 107-296).  The Advisory Committee shall include AI experts from the private sector, academia, and government, as appropriate, and provide to the Secretary of Homeland Security and the Federal Government\u2019s critical infrastructure community advice, information, or recommendations for improving security, resilience, and incident response related to AI usage in critical infrastructure.\n\n     (b)  To capitalize on AI\u2019s potential to improve United States cyber defenses:"}
{"input":"What is the current state of AI policy at the federal level in the United States?","output":"The current state of AI policy at the federal level in the United States includes no major legislative movement in Congress, despite the introduction of relevant bills in recent years. On October 30, 2023, the Biden-Harris Administration issued an Executive Order focused on the safe, secure, and trustworthy development and use of AI, alongside a memo from the Office of Management & Budget on regulating AI within government agencies.","context":"The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI."}
{"input":"How are states and municipalities addressing gaps in AI policy due to federal inaction?","output":"States and municipalities are actively legislating to manage algorithmic harm through various measures, such as notification systems, task forces, and setting minimum privacy standards, to fill the gaps left by federal inaction. EPIC highlights recent efforts without focusing on laws primarily aimed at investing in AI development and research.","context":"EPIC.ORG AI Policy\n\nBACKGROUND\nAI policy must protect individuals, increase transparency and accountability, and center human rights.\n\nStates and municipalities are increasingly taking interest in Artificial Intelligence and filling the gaps left by federal inaction on algorithmic harm. States and cities have taken different routes, from notification and task forces to minimum privacy standards. Some of the recent efforts are highlighted below. EPIC is not including state and local laws primarily focused on investing resources in building more AI and general research, as they do not improve protection of individuals against algorithmic harm. At the federal level, EPIC is only including laws of this sort that have been enacted, and only including laws within the last several years. The United States is likely to take a decentralized bottom-up approach that is messy and may have gaps and inconsistencies. This will likely disappoint proponents of strong AI regulation. Yet, over time this may produce a patchwork quilt of AI rules that if implemented well is grounded in the expertise of specific agencies and more tailored to innovation. The tremendous risks and opportunities of AI have made it a presidential-level issue. The White House is coordinating executive agencies, as each moves ahead with actions in its own domain. The lack of meaningful regulation of social media is widely seen as a failure in Washington, and bolsters efforts to be more proactive with AI. Yet the decentralized structure of the U.S. government, political differences, and the complexities of AI forestall big, quick actions. There is also a real but less-likely probability that a big AI-related failure sparks a strong national government action. Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is"}
{"input":"What frameworks and guidelines have been proposed or implemented for guiding AI development and use?","output":"Proposed and implemented frameworks for guiding AI development and use include the White House's Blueprint for an AI Bill of Rights, the National Institute of Standards and Technology AI Risk Management Framework, the Universal Guidelines for Artificial Intelligence, and the OECD AI Principles. These frameworks emphasize safety, fairness, transparency, accountability, and human rights.","context":"AI policy design\nCountries are at different stages of the development and implementation of\nnational AI strategies and policies. Some countries, such as Canada and Finland,\ndeveloped their national AI strategies as early as 2017, closely followed by\nJapan, France, Germany and the United Kingdom in 2018. Other countries, such\nas Brazil, Egypt, Hungary, Poland and Spain, launched a national AI strategy\nmore recently. Several countries are currently in AI policy consultation and\ndevelopment processes.\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f|7\nNational AI strategies and policies are often initiated with a call to action in the\nform of a report, roadmap, or white paper that frames the high-level goals for\na strategy. The policy design and development stages often follow. In April 2023, EPIC Senior Counsel Ben Winters and Equal Justice Works Fellow Grant Fergusson published a series of blog posts explaining how each of these instruct how entities using AI can do it more responsibly.\n\nUniversal Guidelines for Artificial Intelligence\nIn October 2018, over 250 experts and 60 organizations, representing more than 40 countries, endorsed the Universal Guidelines for Artificial Intelligence (\u201cUGAI\u201d). The guidelines were organized by the Public Voice. The guidelines in full are: FUTURE RESEARCH\nAs policy and regulatory pathways for harnessing AI\nare still in their infancy, further time and research\nis needed to assess the outputs of these strategies,\nand their effectiveness in order to identify best\npractices for developing countries. Rather than\nconcluding with best practices, this article provides\nan initial review of policy and regulatory practices\ncurrently deployed in AI policymaking, illustrating\na wide range of tools and approaches that\ngovernments have taken in their attempt to harness\nAI technologies while mitigating anticipated risks.\nWhile many of the policies observed have been Measures to Address Challenges\n\u2022 Issue and make fully transparent\nthe legal framework, AI policy,\nethical principles and operating\nframework.\n\u2022 Establish special AI\/Innovation Hub\nor government unit as a center of\nexcellence.\n\u2022 Develop whole-of-government,\ndata fabric AI architecture.\n\u2022 Ensure human oversight to\nmanage risks and safeguard\nagainst machine-invoked bias."}
{"input":"What sort of tools can regulators use the ensure safe and responsible use of AI?","output":"To ensure the safe and responsible use of growing AI technologies, regulators need to keep in mind the importance of increasing model and algorithmic transparency, the need for dedicated agencies to perform algorithmic audits, development of appropriate testing sandboxes, and providing a pipeline for whistleblowers to reveal unethical behavior as a result of model outputs or developer practices.","context":"Safety, security, and robustness: AI systems should function as intended and in a robust, secure, and safe way throughout the AI lifecycle, and risks should be continually identified, assessed, and managed. Safety-related risks will be sector specific, and regulators should take a proportionate approach to manage them. Regulators may require the corresponding AI lifecycle actors to regularly test or carry out due diligence on the functioning, resilience, and security of a system. Accountability and governance: Businesses should put in place governance measures that ensure effective oversight of the supply and use of AI systems, with clear lines of accountability established across the AI lifecycle. Regulators will likely implement this principle through regulatory guidance and assurance techniques.\nContestability and redress: Users, impacted third parties, and actors in the AI lifecycle should be able to contest an AI decision or outcome that is harmful or creates material risk of harm. Regulators will be expected to clarify existing routes to contestability and redress and implement proportionate measures to ensure that the outcomes of AI use are contestable where appropriate. The government\u2019s initial non-statutory approach will not create new rights or new routes to redress at this stage. unique to their circumstances (https:\/\/www.brookings.edu\/articles\/acomprehensive-\nand-distributed-approach-to-ai-regulation\/)\n. As a result, regulatorsneed to build robust and persistent strategies to gather information for informedpolicymaking, oversight, and enforcement actions. Collectively, the emerging efforts ofthese agencies will continue to compose a regulatory toolkit upon which much futureAI governance will be built. largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground"}
{"input":"How can we increase transparency regarding the development and use of AI?","output":"Increasing transparency regarding the development and use of AI will be very important as the technology continues to develop. The primary methods available to increase transparency include government agencies requring mandatory disclosures with regards to data collection, copyrighted material use, and disclosure to consumers when their data is being collected for use when training AI algorithms.","context":"an individual right to \u201cmeaningful information aboutthe logic\u201d of algorithmic systems. This has led companies, such as\n, to offer responses\u2014albeit limited ones\u2014to requests for information aboutalgorithmic decisions. Although not yet passed into law, the forthcoming EU AI Act willalso create\nsubstantial new transparency requirements\n(https:\/\/www.brookings.edu\/articles\/the-eu-and-us-diverge-on-ai-regulation-atransatlantic-\ncomparison-and-steps-to-alignment\/#anchor5)\n, likely including directdisclosure of chatbots and public reporting about high-risk AI systems.\nTransparency requirements require little expertise and capacity from governmentagencies, making them an appealing early step in AI regulation. However, regulators doneed to be careful in specifying transparency requirements\u2014vaguely or poorly wordedrequirements can permit too much flexibility in algorithmic transparency, allowing forcompanies to cherry pick self-serving statistics. When transparency requirements are sufficiently narrowly tailored to a type ofalgorithmic system, they can lead to a wide range of benefits. Public information aboutan algorithmic function can help individuals and other businesses make better choicesabout which AI developers to patronize or work with. AI developers may themselvesrealize from public disclosures that their systems are not performing at the state of theart, leading them to prioritize product improvements. Yet, even if more transparencydoes not lead to introspection, public information can help journalists and civil societyorganizations identify subpar and potentially harmful systems. Journalism can lead topublic scrutiny that leads to change in the practices of AI developers, while civilsociety organizations may make use of lawsuits to punish lax algorithmic practices. Allthese benefits can arise even without the regulators themselves using publicinformation, although transparency also helps inform better policymaking The toolbox includes expanding transparency, performing algorithmic audits,developing AI sandboxes, leveraging the AI assurance industry, and learning fromwhistleblowers. These interventions have different strengths and weaknesses forgoverning different types of AI systems, and further, they require different internalexpertise and statutory authorities. To better inform AI policymaking, regulators shouldbe aware of these tools and their trade-offs.\nMandating corporate disclosures is a key function of many government agencies, andthis role is also valuable in markets of algorithmic systems. Algorithmic transparency isamong the most thoroughly studied subfields of AI, which has resulted in a widevariety of approaches, including transparency measures for affected individuals, thegeneral public, and to other organizations, such as other businesses or regulatorsthemselves. |5\nArtificial intelligence (AI) already provides beneficial applications used every\nday by people worldwide. The fast-paced and far-reaching changes from AI\noffer dynamic opportunities for improving the economic and social sectors. AI\ncan make businesses more productive, improve government efficiency, and\nrelieve workers of mundane tasks. It can also help address many pressing global\nchallenges, such as climate change and the lack of access to quality education\nand healthcare.\nAlongside its benefits, AI raises socio-economic and ethical considerations chief among them are questions of respect for human rights and democratic\nvalues, and the dangers of automating and amplifying biases. These concerns\nraise a number of challenges and opportunities. What does it mean to design\ntransparent systems? How can AI systems\u2019 designers and users be accountable\nand to whom? What new safety and security issues are presented with AI?"}
{"input":"How could performing algorithmic audits increase government's ability to ensure that platforms using Generative AI comply with existing laws and ensure equal treatment for all?","output":"Performing algorithmic audits could greatly increase government's ability to ensure that AI technologies comply with existing laws and are safe for consumer use. Not only can audits test the validity of claims made by algorithmic developers, but they can uncover previously unknown flaws or harmful aspects of the system.","context":"Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm. \u2022  safeguards and mechanisms to ensure compliance with regulations\nAlgorithmic impact assessments exist in the Canadian public sector and require government agencies \nto complete a questionnaire before deploying automated decision-making.269\nThere is a voluntary Algorithmic Transparency Recording Standard that UK public bodies can use to \ndisclose information about their use of AI.270\nThe Institute for the Future of Work has stated that audits of AI systems could ensure that these \nsystems are compliant with their impact assessments and the law, and that the Digital Regulation \nCooperation Forum could play a role in helping regulators cooperate on these audits.264\nThe Information Commissioners Office currently carries out consensual and compulsory data \nprotection audits* of AI systems271. However, issues relating to AI are wider than data protection \nand fall under the remit of not just the Information Commissioners Office but multiple regulators far more likely (https:\/\/www.brookings.edu\/articles\/auditing-employmentalgorithms-\nfor-discrimination\/)\nto uncover flawed and harmful aspects of analgorithmic system. However, more intensive algorithmic audits are also far moretechnically complex, requiring more expertise and technical capacity from regulators.Specifically, regulators would need data scientists with expertise in evaluatingalgorithmic systems and may need to take necessary steps to develop a computingenvironment for algorithmic evaluation with appropriate privacy and cybersecuritysafeguards.\nscientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are Audits. Information\nCommissioner\u2019s Office.\n272. Digital Regulation Cooperation Forum (2022). Auditing algorithms: the existing landscape, role \nof regulators and future outlook. GOV.UK.\n273. Intellectual Property Office (2022). Artificial Intelligence and Intellectual Property: \ncopyright and patents: Government response to consultation. GOV.UK.\n274. Intellectual Property Office (2023). The government\u2019s code of practice on copyright and AI. \nGOV.UK.\n275. Aitken, M. et al. (2022). Common Regulatory Capacity for AI. The Alan Turing Institute.\n276. Jung, C. et al. (2023). Artificial intelligence for public value creation. Inst. Public Policy \nRes.,\n277. Roberts, H. et al. (2023). Artificial intelligence regulation in the United Kingdom: a path to \ngood governance and global leadership? Internet Policy Rev., Vol 12.\n278. Minderoo Centre for Technology and Democracy (2023). Policy Brief: Generative AI. mctd.ac.uk.\n279. Equality and Human Rights Commission (2023). AI"}
{"input":"How are the AI regulations for Europe, China, and the US different? ","output":"The EU\u0092s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u0092s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u0093should adhere to the core socialist values\u0094. The United States is taking its typical decentralized approach.","context":"2 AI legislation in China So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC."}
{"input":"How is Andreessen\u0092s opinion of AI different from Elon Musk\u0092s?  ","output":"Andreessen believes that AI will save the world whereas Elon Musk believes that it could cause human extinction if unregulated. ","context":"I. The AI Regulation Debate: A Wide Spectrum of Views. The AI regulation debate has become heated and politicized, reflecting the immense hopes and fears that we have invested in AI.   \n\nHysterical fear and paranoia. On one end, leading technologist and venture capitalist Marc Andreessen  says \u201cwe have a full-blown moral panic about AI right now.\u201d He views this as \u201chysterical fear\u201d that is \u201cirrational\u201d and compares it to earlier introduction of new technologies that were similarly feared but were overblown and subsequently managed. He believes that AI is simply a computer program that is owned, controlled and used by people, and that AI does not have its own goals or its own wants. Moreover, he emphasizes, AI can be a force for tremendous good. Fourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct.  \n\nGenerative AI like Chat-GPT has additional requirements including disclosing that the content was generated by AI, designing the model to prevent it from generating illegal content, and publishing summaries of copyrighted data used for training.   \n\nVenture Capitalists- Andreessen Horowitz: AI will save the world. Influential venture capitalist Marc Andreessen (co-creator of the Mozilla Internet browser) writes, \u201cThe development and proliferation of AI \u2013 far from a risk that we should fear \u2013 is a moral obligation...\u201d He sees the drive to regulate AI as hysterical fear that is irrational. He calls out big companies for selling fear and pushing AI regulation to protect their economic interests. Big companies have the resources to meet extensive AI rules, while small companies and start-ups (that venture capitalists invest in) mostly do not. Andreessen argues: Chui, M., M. Harryson, J. Manyika, R. Roberts, R. Chung, A. Heteren, and P. Nel. 2018a.\nNotes from the AI Frontier: Applying AI for Social Good. Discussion Paper. McKinsey Global Institute.\nhttps:\/\/www.mckinsey.com\/~\/media\/McKinsey\/Featured%20Insights\/Artificial%20Intelligence\/Applying%20artificial%20intelligence%20\nfor%20social%20good\/MGI-Applying-AI-for-social-good-Discussion-paper-Dec-2018.ashx\nChui, Michael, James Manyika, Mehdi Miremadi, Nicolaus Henke, Rita Chung, Pieter Nel, and Sankalp Malhotra. 2018b.\nNotes from the AI Frontier: Insights from Hundreds of Use Cases. Discussion Paper. McKinsey Global Institute.\nhttps:\/\/www.mckinsey.com\/~\/media\/mckinsey\/featured%20insights\/artificial%20intelligence\/notes%20from%20the%20ai%20frontier%20\napplications%20and%20value%20of%20deep%20learning\/notes-from-the-ai-frontier-insights-from-hundreds-of-use-cases-discussion-paper.ashx they work. The Conversation.\n175. David, D. (2021). Analyzing The Rise Of Deepfake Voice Technology. Forbes.\n176. Green, J. et al. (2023). Experts warn of rise in scammers using AI to mimic voices of loved \nones in distress. ABC News.\n177. Brundage, M. et al. (2018). The Malicious Use of Artificial Intelligence. Malicious AI Report.\n178. Leffer, L. (2023). Can AI Replace Actors? Here\u2019s How Digital Double Tech Works. Scientific \nAmerican.\n179. Smith, D. (2023). \u2018We\u2019re going\nthrough a big revolution\u2019: how AI is de-ageing stars on screen. The Guardian.\n180. Equity (online). AI Vision Statement.\n181. Maddaus, G. (2023). SAG-AFTRA\nStrike: AI Fears Mount for Background Actors. Variety.\n182. Barbas, S. (2015). Laws of Image: Privacy and Publicity in America. Stanford University Press.\n22                                  POSTnote 708 - 9 January 2024"}
{"input":"What are the four categories of AI systems defined by the EU\u0092s AI Act on the basis of risk? ","output":"The four categories are unacceptable risk, high risk, limited risk and low\/minimal risk. ","context":"Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. The Regulatory Framework defines 4 levels of risk in AI:\npyramid showing the four levels of risk: Unacceptable risk; High-risk; limited risk, minimal or no risk\n \nAll AI systems considered a clear threat to the safety, livelihoods and rights of people will be banned, from social scoring by governments to toys using voice assistance that encourages dangerous behaviour.\n\nHigh risk:\nAI systems identified as high-risk include AI technology used in: The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach"}
{"input":"What is artificial intelligence? ","output":"Technology, machines, and software that have the ability to be self-directed and learn from their actions are generally known as artificial intelligence.","context":"Definitions \u2013 What is AI?\n\u2022 AI: Artificial Intelligence can be\ndefined as the ability of the software\nsystems to carry out tasks that\nusually require human intelligence:\nvision, speech, language,\nknowledge, and search.\n\u2022 ML: Machine Learning is the ability\nof the computer algorithm to learn\nfrom data and improve automatically.\n\u2022 NLP: Neuro-linguistic programming\nis the ability of an AI algorithm to\nread a text, convert speech into text,\nor vice versa.\n\u2022 Data Mining: The ability of the AI\nalgorithm to examine large amounts\nof raw data to determine patterns.\n\u2022 ANN: Artificial Neural Networks\nare AI algorithms that recognize\nrelationships between different data\nsets similar to how the human brain\nanalyzes such information. Sec. 3.  Definitions.  For purposes of this order:\n\n     (a)  The term \u201cagency\u201d means each agency described in 44 U.S.C. 3502(1), except for the independent regulatory agencies described in 44 U.S.C. 3502(5).\n\n     (b)  The term \u201cartificial intelligence\u201d or \u201cAI\u201d has the meaning set forth in 15 U.S.C. 9401(3):  a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments.  Artificial intelligence systems use machine- and human-based inputs to perceive real and virtual environments; abstract such perceptions into models through analysis in an automated manner; and use model inference to formulate options for information or action.\n\n     (c)  The term \u201cAI model\u201d means a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs. Term                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI\nregulation\u2019 defined AI, AI systems or AI technologies as \u201cproducts and services that are \n\u2018adaptable\u2019 and \u2018autonomous\u2019.\u201d The adaptability of AI refers to AI systems, after being trained, \noften developing the ability to perform new ways of finding patterns and\nconnections in data that are not directly envisioned by their human\nprogrammers. The autonomy of AI refers to some AI systems that can make decisions without the \nintent or ongoing control of a human (PB 57).\nGenerative AI               The Alan Turing Institute defines generative AI as an \u201cartificial\nintelligence system that generates text, images, audio, video or other media in response to user \nprompts.\u201d26 Generative AI applications include chatbots, such as OpenAI\u2019s ChatGPT, photo and video\nfilters, and virtual assistants (PB 57). Artificial Intelligence (AI) seeks to make computers do                         full implications of AI without exploring the specificities\nthe sorts of things that minds can do, and various                              of the AI technology clusters concerned. It is hoped that\nfunctional or descriptive definitions exist. It is                              this primer provides an easily accessible guide to these\nimportant to note that AI has a long history, dating back                       specificities. Indeed, a black-box approach to \u201cAI\u201d is only\nto the 19th century, with many techniques dating to the                         of limited usefulness to most policy questions.\n1950s. In practice, various clusters of AI types are used\n\u2013 also in hybrid formats."}
{"input":"What potential threats of AI have public policy debates focused on? ","output":"Public policy debates have focused on such developments as the exacerbation of inequality, the loss of jobs, and the potential threat of human extinction if AI continues to evolve without effective guardrails.","context":"With the release of advanced forms of AI to the public early in 2023, public policy debates have rightly focused on such developments as the exacerbation of inequality, the loss of jobs, and the potential threat of human extinction if AI continues to evolve without effective guardrails. There has been less discussion about how AI might affect geopolitics and which actors might take the lead in the future development of AI or other advanced AI algorithms. Nevertheless, in the area of AI regulation there is still a relatively large space for policy debates. This is perhaps due to the relatively technical nature of policies and to the freshness of the problems. How to effectively regulate AI remains a wide-open question globally, and the political interests at play in China are not yet entrenched. Ministries and state-owned enterprises have not spent decades fighting to gain leverage or to hang onto preferential policies they have carved out. This mix of factors has made public debates over AI governance unusually lively and open. What should government policymakers do to protect humanity?\nThe potential dangers posed by AI are many. At the extreme, they include the threat of human extinction, which could come about by an AI-enabled catastrophe, such as a well-designed virus that spreads easily, evades detection, and destroys our civilization. Less dire, but considerably worrisome, is the threat to democratic governance if AIs gain power over people.[2] Even if AIs do not kill humans or overturn democracy, authoritarian regimes, terrorist groups, and organized crime groups could use AI to cause great harm by spreading disinformation and manipulating public opinion. Governments need to view the AI landscape as a regulatory training ground in preparation for the threats posed by even more-advanced AI capabilities, including the potential arrival of artificial general intelligence. World of Ideas\nOnce a real-world change has thrown up an issue that needs addressing, and after the issue has been filtered through the prism of Xi Jinping and CCP ideology, it enters perhaps the most dynamic layer. This is the world of ideas, where the problem and its solution are debated by actors ranging from think tank scholars to AI scientists, and from investigative journalists to corporate lobbyists. This is where many policy ideas are generated or shot down. It is where technology companies try to steer the policy dialogue in their preferred direction and where journalists can bring social issues into mainstream public discourse.\n\nWhile these public debates do not settle policy, they provide the intellectual grist for the bureaucratic mill.\n\nThe Policy Funnel of Chinese AI Governance\nClick on a section of the diagram above to learn more about the key actors and influences on major AI governance initiatives in China."}
{"input":"Why are US government personnel said to lag behind engineers with respect to AI? ","output":"The U.S. government continues to allow private corporations to develop AI without meaningfully regulating the technology or intervening in a way that changes those corporations' behavior.","context":"The U.S. government continues to allow private corporations to develop AI without meaningfully regulating the technology or intervening in a way that changes those corporations' behavior. This approach fits with the long-standing belief in the United States that the free market (and its profit-driven incentives) is the most effective mechanism to rapidly advance technologies like AI.[1] In this world, U.S. government personnel continue to lag behind engineers in the U.S. technology sector, both in their understanding of AI and in their ability to harness its power. Private corporations direct the investment of almost all research and development funding to improve AI, and the vast majority of U.S. technical talent continues to flock to Silicon Valley. The U.S. government seeks to achieve its policy goals by relying on the country's innovators to develop new inventions that it could eventually purchase. In this world, the future relationship between the U.S. government and the Although technology has often influenced geopolitics, the prospect of AI means that the technology itself could become a geopolitical actor. AI could have motives and objectives that differ considerably from those of governments and private companies. Humans' inability to comprehend how AI \"thinks\" and our limited understanding of the second- and third-order effects of our commands or requests of AI are also very troubling. Humans have enough trouble interacting with one another. It remains to be seen how we will manage our relationships with one or more AIs. The Department of Defense incorporates AI in weapon systems, command and control, and intelligence. Humans will stay \u201cin the loop\u201d. DoD is likely criticized for moving too slow. The intelligence community and law enforcement agencies are also likely major adopters of AI, sparking civil rights concerns. \n\nGrowing competition with China shadows the AI regulatory effort, escalating a \u201cdon\u2019t fall behind China\u201d debate. Controls on exports and investments in AI-related technologies such as advanced GPUs are likely to expand over time and broaden, e.g., cloud computing, quantum. Technological innovation will continue to outpace traditional regulation\nThe U.S. government is already considered behind on understanding AI compared with leading private developers. This is due to several factors: the relative slowness of government policymaking compared with the fast pace of technology development, government's inability to pay competitive salaries for scarce talent, and the lack of clarity on whether and how AI should be regulated at all, among others. Looking ahead, this dynamic is unlikely to change. Governments will continue playing catch-up with the private sector to understand and respond to the newest and most-capable AI developments."}
{"input":"List a few measures that government policymakers should take to protect humanity. ","output":"They should focus on strengthening resilience to AI threats, look beyond traditional regulatory techniques to influence AI developments, continue support for innovation, and partner with the private sector to improve risk assessments. ","context":"What should government policymakers do to protect humanity?\nThe potential dangers posed by AI are many. At the extreme, they include the threat of human extinction, which could come about by an AI-enabled catastrophe, such as a well-designed virus that spreads easily, evades detection, and destroys our civilization. Less dire, but considerably worrisome, is the threat to democratic governance if AIs gain power over people.[2] Even if AIs do not kill humans or overturn democracy, authoritarian regimes, terrorist groups, and organized crime groups could use AI to cause great harm by spreading disinformation and manipulating public opinion. Governments need to view the AI landscape as a regulatory training ground in preparation for the threats posed by even more-advanced AI capabilities, including the potential arrival of artificial general intelligence. In addition to more-traditional regulatory practices, government policies on AI should focus on strategies of resilience to mitigate potential AI threats because strategies aimed solely at denial will not work. AI cannot be contained through regulation, so the best policy will aim to minimize the harm that AI might do. This will probably be most critical in biosecurity, [3] but harm reduction also includes countering cybersecurity threats, strengthening democratic resilience, and developing emergency response options for a wide variety of threats from state and sub- and non-state actors. Governments will either need to adopt entirely new capabilities to put this policy into action or expand existing agencies, such as the Cybersecurity and Infrastructure Security Agency. Governments should take a more comprehensive approach to regulation beyond hardware controls, which will not be enough to mitigate harms in the long run. \u2022\n\nSAFETY AND SECURITY. Cybersecurity is critical. Leaders must ensure the well-being of society at large and private individual\nhumans.\n\n\u2022\n\nTRANSPARENCY AND EXPLAINABILITY. The algorithm, business case, data collection, design, and policy information must\nbe transparent to stakeholders and those impacted. Individuals should get notifications when interacting with AI or when AI\ndecides for him or her.\n\n\u2022\n\nFAIRNESS. AI solutions should minimize bias and identify and manage risk. Inclusiveness should be ensured in design and\nimpact.\n\n\u2022\n\nHUMAN CONTROL OF TECHNOLOGY. The AI should be under human control. People should review automated decisions and\nhave the right to opt-out of automated decisions.\n\n\u2022\n\nPROFESSIONAL RESPONSIBILITY. Multistakeholder collaboration, accuracy, and scientific integrity of the solution should be\nensured.\n\n\u2022\n\nPROMOTION OF HUMAN VALUES. AI should be human-centric. It should promote human values and benefit society.\n\n7\n\n>>> (h)  The Federal Government should lead the way to global societal, economic, and technological progress, as the United States has in previous eras of disruptive innovation and change.  This leadership is not measured solely by the technological advancements our country makes.  Effective leadership also means pioneering those systems and safeguards needed to deploy technology responsibly \u2014 and building and promoting those safeguards with the rest of the world.  My Administration will engage with international allies and partners in developing a framework to manage AI\u2019s risks, unlock AI\u2019s potential for good, and promote common approaches to shared challenges.  The Federal Government will seek to promote responsible AI safety and security principles and actions with other nations, including our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing"}
{"input":"What is the purpose of the EOAI? ","output":"The executive order on AI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance.","context":"globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d The EOAI also calls for the accelerated development of AI standards with international partners in standards organizations. On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI The EOAI along with the other domestic AI policies outlined will have significant international impacts through various channels.\n\nFirst and most immediately, these suites of domestic policy developments on AI governance give Vice President Kamala Harris as head of the U.S. delegation to the U.K. AI Safety Summit a real opportunity to lead the further development of international AI governance. The U.S. position on AI stands in stark contrast to the lack of strong U.S. leadership on privacy regulation, where the absence of federal privacy legislation created a vacuum that the EU\u2019s General Data Protection Regulation (GDPR) filled, allowing GDPR to become a leading model for privacy regulation worldwide. For government use of AI, the EO requires the development of guidance for agency use of AI and a faster and more efficient process for agencies to procure AI products and services. The directive also calls for the rapid hiring of AI professionals and the training of employees at all levels. The content of a forthcoming Office of Management and Budget memo will dictate the details of how government AI use will change.\n\nEPIC has long advocated for comprehensive privacy protections, rigorous testing protocols, expanded resources for evaluation of AI systems, and a government-whole effort to fighting algorithmic discrimination.\n\nTwo days later, on November 1st, 2023, the Office of Management and Budget released a memo dictating how Government Agencies should approach using and regulating AI. There is a 30-day comment period before it\u2019s finalized."}
{"input":"Which other countries are developing their own AI regulations? ","output":"Countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance.","context":"INTERNATIONAL LAWS\nSeveral other counties are more advanced than the U.S. in terms of AI policy development that works to protect people from algorithmic harm. For more information on AI laws and norms internationally, please see EPIC\u2019s International Policy page. These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules. Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC."}
{"input":"Which international forums focus on AI governance?  ","output":"AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).","context":"The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. \u2022\n\nCreating a new governmental or independent AI co-ordination entity.\n\n\u2022\n\nEstablishing AI expert advisory groups. These are generally multistakeholder groups comprising AI experts tasked with identifying and\nreporting on current and future opportunities, risks and challenges\narising from the use of AI in society. These AI councils also provide\nrecommendations to the government.\n\n\u2022\n\nSetting up oversight and advisory bodies for AI and data ethics.\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f8|\nFigure 1. A selection of national AI governance approaches\nAssigning oversight to\nan existing ministry or\ndepartment\n\n\u2022 The White House Office of Science and Technology Policy oversees the United States\u2019\nnational AI strategy.\n\u2022 Estonia\u2019s Ministry of Economic Affairs and Communications created the national AI\nstrategy.\n\u2022 France coordinates AI policy implementation from within the Prime Minister\u2019s Office.\n\nCreating a new\ngovernmental or\nindependent body for\nAI Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors."}
{"input":"Name the two important outcome documents from the 2023 G7 summit on AI governance. ","output":"The two important outcome documents from the 2023 G7 summit are G7 Summit Communiqu\u00e9 and the Declaration of the G7 Digital and Tech Ministers\u0092 Meeting. ","context":"With the recent launch of large language AI models such as GPT-4, society anticipates a rapid integration of AI technologies, making it crucial to discuss the responsible use of AI and its governance. However, even the like-minded G7 countries approach AI governance differently, ranging from strict, comprehensive laws to sector-specific guidelines. Because of this, the consensus among G7 leaders on trustworthy AI will have a big impact on the way AI systems should be developed and operated around the world. This commentary takes a close look at two important outcome documents from the 2023 G7 summit that broach the subject of AI governance: the G7 Summit Communiqu\u00e9 and the Declaration of the G7 Digital and Tech Ministers\u2019 Meeting\u2014 collectively referred to as the G7 statements\u2014to predict potential cooperation on AI governance among the G7 countries.\n\nShared Understanding of Values, Principles, and Risks\nEstablishing Common Values The Path to Trustworthy AI G7 Outcomes and Implications for Global AI Governance\n\nCommentary by Hiroki Habuka\n\nPublished June 6, 2023\n\nIntroduction\nThe G7 Summit, held from May 19 to 21, 2023, left a strong impression of unity among global leaders. Emerging technology such as artificial intelligence (AI), along with national security and energy, was highlighted as a key area requiring a strong alliance among G7 countries toward fundamental values such as democracy and human rights. The G7 Digital and Tech Ministers\u2019 Meeting, which took place a little earlier, also discussed responsible AI and global AI governance as one of the central topics. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance."}
{"input":"What is the G7? ","output":"The G7 is a forum of nations that share fundamental values, such as freedom, democracy, and human rights.","context":"The G7 statements emphasize the need to support the development of tools for trustworthy AI several times. Tools here include a wide range of regulatory and nonregulatory frameworks, technical standards and assurance techniques, risk assessment and management frameworks, auditing, and potential certification schemes. The Path to Trustworthy AI G7 Outcomes and Implications for Global AI Governance\n\nCommentary by Hiroki Habuka\n\nPublished June 6, 2023\n\nIntroduction\nThe G7 Summit, held from May 19 to 21, 2023, left a strong impression of unity among global leaders. Emerging technology such as artificial intelligence (AI), along with national security and energy, was highlighted as a key area requiring a strong alliance among G7 countries toward fundamental values such as democracy and human rights. The G7 Digital and Tech Ministers\u2019 Meeting, which took place a little earlier, also discussed responsible AI and global AI governance as one of the central topics. Conclusion\nThe G7 statements have highlighted the shared commitment to developing and implementing trustworthy AI that upholds values such as human rights and democracy. The statements also acknowledge risks including online abuse, threats to privacy, misuse of AI, and IP concerns. The G7 leaders have agreed to support the establishment of an interoperable governance framework to counteract potential gaps and fragmentation in global technology governance. They have also committed to assisting the efforts by organizations such as OECD and GPAI, and to collaborate in the development of international technical standards under SDOs. The European Union (EU) Commission embraced the G7 leaders\u2019 agreement and swiftly issued a call to action. \u201cI am pleased to welcome the G7 international Guiding Principles and the voluntary Code of Conduct, reflecting EU values to promote trustworthy AI. I call on AI developers to sign and implement this Code of Conduct as soon as possible,\u201d said Commission President Ursula von der Leyen in a press release. (Later this week, Von der Leyen will be joining other global leaders in Buckinghamshire for the two-day UK AI Safety Summit. More details below.)"}
{"input":"Mention some trustworthy tools for AI. ","output":"Tools for trustworthy AI include a wide range of regulatory and nonregulatory frameworks, technical standards and assurance techniques, risk assessment and management frameworks, auditing, and potential certification schemes.","context":"explore data trust frameworks for safe, secure and equitable data transfers.\nEuropean countries are co-operating to create a European data space (GAIA-X),\nwhich will include private and public data.\nIn addition, many software tools to manage and use AI exist as open-source\nresources, which facilitates their adoption and allows for crowdsourcing\nsolutions to software bugs. Tools include TensorFlow (Google) and Cognitive\nToolkit (Microsoft). A number of researchers and companies share curated\ntraining datasets and training tools publicly to help diffuse AI technology. The G7 statements emphasize the need to support the development of tools for trustworthy AI several times. Tools here include a wide range of regulatory and nonregulatory frameworks, technical standards and assurance techniques, risk assessment and management frameworks, auditing, and potential certification schemes. (A)  developing a companion resource to the AI Risk Management Framework, NIST AI 100-1, for generative AI;\n\n               (B)  developing a companion resource to the Secure Software Development Framework to incorporate secure development practices for generative AI and for dual-use foundation models; and\n\n               (C)  launching an initiative to create guidance and benchmarks for evaluating and auditing AI capabilities, with a focus on capabilities through which AI could cause harm, such as in the areas of cybersecurity and biosecurity.\n\n          (ii)  Establish appropriate guidelines (except for AI used as a component of a national security system), including appropriate procedures and processes, to enable developers of AI, especially of dual-use foundation models, to conduct AI red-teaming tests to enable deployment of safe, secure, and trustworthy systems.  These efforts shall include: (a)  Artificial Intelligence must be safe and secure.  Meeting this goal requires robust, reliable, repeatable, and standardized evaluations of AI systems, as well as policies, institutions, and, as appropriate, other mechanisms to test, understand, and mitigate risks from these systems before they are put to use.  It also requires addressing AI systems\u2019 most pressing security risks \u2014 including with respect to biotechnology, cybersecurity, critical infrastructure, and other national security dangers \u2014 while navigating AI\u2019s opacity and complexity.  Testing and evaluations, including post-deployment performance monitoring, will help ensure that AI systems function as intended, are resilient against misuse or dangerous modifications, are ethically developed and operated in a secure manner, and are compliant with applicable Federal laws and policies.  Finally, my Administration will help develop effective labeling and content provenance mechanisms, so that Americans are able to"}
{"input":"How do export controls influence AI and other data-driven technologies and their effect on US National Security?","output":"Export controls are a necessary measure for many nations to ensure the responsible and ethical use of technology. As a leader in technological innovation, particularly in the realm of AI, data, and microchip production, the United States of America uses export control as one of the first tools in preventing terrorist organizations, enemy nations, and competitors from gaining easy access to advanced technology.","context":"As Adeyemo\u2019s statement indicates, the United States views technology export controls not only as a defensive measure to limit the spread of weapons but as a powerful tool of national security\u2014capable of deterring or punishing actions contrary to U.S. and allied interests. When the United States and its allies did successfully agree on a multilateral regime of export controls against Russia, export controls moved back to the forefront of U.S. national security policy.\n\nIn September 2023, National Security Advisor Jake Sullivan said that the Russian export controls \u201cdemonstrated that technology export controls can be more than just a preventative tool. If implemented in a way that is robust, durable, and comprehensive, they can be a new strategic asset in the U.S. and allied toolkit to impose costs on adversaries, and even over time degrade their battlefield capabilities.\u201d Improved Export Controls Enforcement Technology Needed for U.S. National Security\nExecutive Summary\nAs technology has become increasingly central to strategic competition with Russia and China, export controls have moved to the forefront of U.S. foreign policy on technology issues. Most notably, restricting Russia\u2019s access to advanced technology through export controls is a key part of the U.S. response to Russia\u2019s invasion of Ukraine, as U.S. government officials have repeatedly stated.\n\nUnfortunately, nearly all the debate is focused on whether and when to apply export controls, not how to ensure that export controls are effectively administered and enforced once applied. On October 7, 2022, the Department of Commerce announced major changes to export control policies toward China for the procurement of advanced AI and semiconductor technology used in both commercial and military AI systems. This policy document included important acknowledgments that previous export control approaches\u2014based on restricting the sale of dual-use technologies to military end users or for military end uses\u2014had diminished in effectiveness over the past decade. Specifically, the new policy stated that controls prior to October 7 \u201cgenerally only apply when the \u2018U.S. person\u2019 has knowledge that their activities are contributing to prohibited end uses or end users. China\u2019s military-civil fusion effort makes it more difficult to tell which items are made for restricted end uses, thereby diminishing the effect of these existing controls.\u201d In October 2022, the Biden administration further expanded the scope of U.S. export controls when it released two new rules aimed at severely restricting Chinese artificial intelligence (AI) and semiconductor capabilities. A recent CSIS report, Choking Off China\u2019s Access to the Future of AI, assesses these controls in detail. What is novel about these controls is that they represent a sea change in the U.S. approach to China, particularly since the controls are both geographic in nature and unilateral. The administration has also indicated recently that it is shifting from a policy focused on delaying adversaries\u2019 technological advancements to one of directly degrading them. Though less severe than those imposed on Russia, these new export controls are intended to prevent China from achieving key strategic priorities. Both Russia and China are doubtless devoting massively increased resources to evading the new export controls."}
{"input":"How might heightened export controls on advanced technologies effect the global balance of power?","output":"Advanced technology like microchips, software, and other hard-to-manufacture hardware are included in the development of many dangerous projects, from weapons manufacturing to serving as support vectors for cyber attacks. Access to advanced technology increases the capabilities of bad actors and makes enforcement of international law and protection of threatened peoples more difficult. If these actors were afforded easy access to these technologies, they may be used to gain advantage during times of war or to threaten peaceful entities.","context":"As Adeyemo\u2019s statement indicates, the United States views technology export controls not only as a defensive measure to limit the spread of weapons but as a powerful tool of national security\u2014capable of deterring or punishing actions contrary to U.S. and allied interests. When the United States and its allies did successfully agree on a multilateral regime of export controls against Russia, export controls moved back to the forefront of U.S. national security policy.\n\nIn September 2023, National Security Advisor Jake Sullivan said that the Russian export controls \u201cdemonstrated that technology export controls can be more than just a preventative tool. If implemented in a way that is robust, durable, and comprehensive, they can be a new strategic asset in the U.S. and allied toolkit to impose costs on adversaries, and even over time degrade their battlefield capabilities.\u201d Improved Export Controls Enforcement Technology Needed for U.S. National Security\nExecutive Summary\nAs technology has become increasingly central to strategic competition with Russia and China, export controls have moved to the forefront of U.S. foreign policy on technology issues. Most notably, restricting Russia\u2019s access to advanced technology through export controls is a key part of the U.S. response to Russia\u2019s invasion of Ukraine, as U.S. government officials have repeatedly stated.\n\nUnfortunately, nearly all the debate is focused on whether and when to apply export controls, not how to ensure that export controls are effectively administered and enforced once applied. The Huawei and ZTE export controls were a potent reminder to the U.S. foreign policy community of the power of export controls. U.S. technology export controls were able to do significant damage to the financial prospects of leading Chinese technology giants. The United States did this entirely with non-violent means and at a limited cost to the U.S. economy.\n\nHowever, major weaknesses in U.S. export control enforcement capacity are evident, as demonstrated by Russia\u2019s success in evading controls for weapons used in Ukraine. More recently, on October 7, the Biden administration enacted a major set of export controls that restricted the sale of AI and semiconductor technology to China. The new export controls dramatically reduce China\u2019s prospects for becoming a superpower in AI technology and likewise reduce China\u2019s prospects for being self-sufficient in semiconductor technology.\n\nWhen targeted correctly and enforced effectively, export controls are a powerful tool of foreign policy, as seen in the 2018 restrictions on semiconductor sales to ZTE and Huawei."}
{"input":"How might better data collection and analysis methods increase the capabilities of US export controls?","output":"Better data collection and analysis methods have the potential to greatly increase the capabilities of US export control agencies. Access to internal commerce data, data from other Federal Agencies, and pooling of Open Source data promise to allow for better identification of agents circumventing existing export controls and development of more efficient responses.","context":"If given adequate funding, BIS could dramatically enhance the productivity and efficacy of export controls by applying modern data science and machine learning technologies.\n\nImproved technology would go a long way toward strengthening BIS\u2019s ability to manage its key administrative lists, rapidly assess license applications, and effectively enforce export controls. Adopt a more proactive rather than reactive approach to export controls\nLeverage a new technology suite to provide real-time data about export control evasion\nExpand the analyst staff and train BIS staff with relevant courses\nFor ensuing appropriations, it is important that Congress provide sufficient funding to enable BIS and its data contractors to keep the source code and inputs evergreen, which will necessitate frequent updating and monitoring. Although the adoption of this technology will also have significant cost-saving effects on staff time and bandwidth, the initial bucket of funding will be needed to train BIS staff to use the new technology. Improved Export Controls Enforcement Technology Needed for U.S. National Security\nExecutive Summary\nAs technology has become increasingly central to strategic competition with Russia and China, export controls have moved to the forefront of U.S. foreign policy on technology issues. Most notably, restricting Russia\u2019s access to advanced technology through export controls is a key part of the U.S. response to Russia\u2019s invasion of Ukraine, as U.S. government officials have repeatedly stated.\n\nUnfortunately, nearly all the debate is focused on whether and when to apply export controls, not how to ensure that export controls are effectively administered and enforced once applied. Private sector compliance resources are critical for the viability of U.S. export controls, but they are not a substitute for robust internal capabilities at BIS."}
{"input":"How do AI's capabilities and ethical considerations influence the formation of AI policies?","output":"AI's capabilities and ethical considerations shape AI policy formation by requiring frameworks that ensure AI's development and use are beneficial, safe, and respectful of human rights.","context":"boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory of, China\u2019s AI strategy. It emphasises that the Chinese government is aware of the potential benefits, practical risks, and the ethical challenges that AI presents, and that the direction of China\u2019s AI strategy will largely be determined by the interplay of these factors and by the extent to which government\u2019s interests may outweigh ethical concerns. Section 5 concludes the paper by summarising the key findings of our analysis. This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] FINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re-\n\nview of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries."}
{"input":"How does the World Bank's AI Policy differ from its previous Disclosure Policy?","output":"The AI Policy transitioned the Bank from a prescriptive disclosure model to one that allows for the disclosure of any information not explicitly exempted, significantly expanding the scope of information available to the public.","context":"AI POLICY HIGHLIGHTS\n\nWhat is the main difference between the Bank's Access to Information Policy (AI Policy) and the previous\nDisclosure policy?\n\nThe AI Policy, launched in 2010, moved the Bank from an approach that spelled out what information is disclosed to\none that allows the Bank to disclose any information in its possession that is not on a list of exceptions set out in the AI\nPolicy.\n\n    \uf0b7    Under the AI Policy, significantly more information on Bank operations and Board proceeding is available.\n    \uf0b7    The new Policy permits public release of some documents prior to discussion by the World Bank's Board of\n         Executive Directors.\n    \uf0b7    Certain restricted information is eligible for declassification after 5, 10, or 20 years.\n    \uf0b7    It also establishes an appeal mechanism that provides public recourse when the Bank denies access to\n         information. AI FAQs\n\nAI POLICY HIGHLIGHTS\n\n    \uf0b7    What is the main difference between the World Bank's Access to Information Policy (AI Policy) and the\n         previous Disclosure policy?\n    \uf0b7    Is the AI Policy retroactive?\n    \uf0b7    Was the AI Policy discussed with member countries?\n    \uf0b7    What can be disclosed under the AI Policy?\n    \uf0b7    What are the exceptions to disclosing information?\n    \uf0b7    Why does the AI Policy restrict some types of information from disclosure?\n\nPROACTIVE DISCLOSURE\n\n    \uf0b7    Where do I find World Bank information?\n    \uf0b7    When do project documents become available?\n    \uf0b7    When do Board papers and Board records become available?\n\nREQUESTING INFORMATION TO THE WORLD BANK\n\n    \uf0b7    Can I contact a Bank staff directly for Bank\u2019s information?\n    \uf0b7    What does it cost to get information from the Bank?\n    \uf0b7    What if the Bank restricts something it is supposed to release when requested?\n    \uf0b7    What is the process to file an appeal?\n\n\n\nAI POLICY HIGHLIGHTS Go Top\n\nWhy does the AI Policy restrict some types of information from disclosure?\n\nThe AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the\npublic and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff and other\nparties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure\ncould cause harm to well defined interests.\n\n                                                                                                                    Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information? Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction"}
{"input":"Why is a \"black-box approach\" to AI considered insufficient for understanding its impact on SDGs?","output":"The black-box approach does not specify underlying techniques and technologies, which are crucial for fully grasping AI's implications on sustainability and future directions.\n","context":"thing that \u201cwe know when we see it\u201d. For example, it is              list as a working basis to discuss SDG impacts. However,\nnotoriously difficult to measure human intelligence, yet             I am cognizant of the fact that such a \u201cblack-box\nit is pretty obvious to us when we meet a highly                     approach\u201d to AI which does not specify the underlying\nintelligent person.                                                  techniques and technologies is not sufficient to fully\nAs a working definition, I suggest following eminent                 understand all SDG impacts. The details of AI concepts,\ncognitive scientist Margaret Boden: \u201cArtificial                      techniques and technologies matter greatly, especially\nIntelligence (AI) seeks to make computers do the sorts of            with regard to sustainability and with regard to future\nthings that minds can do. Some of these (e.g., reasoning,            directions of AI and its physical, economic and socio- Policy implications of artificial intelligence (AI)\n109. Neri, E. et al. (2020). Artificial intelligence: Who is responsible for the diagnosis? Radiol. \nMed. (Torino), Vol 125, 517\u2013521.\n110. Larsson, S. et al. (2020). Transparency in artificial intelligence. Internet Policy Rev., Vol \n9.\n111. Bagchi, S. (2023). What is a black box? A computer scientist explains what it means when the \ninner workings of AIs are hidden. The Conversation.\n112. Savage, N. (2022). Breaking into the black box of artificial intelligence. nature.\n113. Hassija, V. et al. (2023). Interpreting Black-Box Models: A Review on Explainable Artificial \nIntelligence. Cogn. Comput.\n114. Cassauwers, T. (2020). Opening the \u2018black box\u2019 of artificial intelligence | Research and \nInnovation. European Commission.\n115. Blanc, R. M. and M. de (2023). Open Data and the AI Black Box. Electronic Frontier Foundation.\n116. Wadden, J. J. (2022). Defining the undefinable: the black box problem in healthcare artificial leading to new applications, new development models,\ncombined with other methods as need be. This will have\n                                                                                and also sustainability concerns. This has important\nimportant implications on our SDG aspirations until\n                                                                                implications for humanity\u2019s aspirations expressed in the\n2030, as will be detailed in this report.\n                                                                                SDGs.\nConclusion                                                                      Finally, it is impossible to adequately understand the\nArtificial Intelligence (AI) seeks to make computers do                         full implications of AI without exploring the specificities 1Note: The views expressed in this brief are those of the author and do not necessarily reflect those of the United Nations or its\nsenior management.\n\f   Science-Policy Brief for the Multistakeholder Forum on Science, Technology and Innovation for the SDGs, May 2022\n\n                                                             Table 1. Basic AI functions\n\n                                                                    Basic AI functions\n                         It can \u201csee\u201d      It can \u201chear\u201d             It can \u201cread\u201d         It can move by     It can \u201creason\u201d\n                         and identify                                                      itself, based on     and looks for\n        What it can do"}
{"input":"What is the primary concern driving the need for international governance of AI?","output":"The dual nature of AI, offers both extraordinary potential benefits and significant risks, stressing the need for governance to not only address challenges but also to harness AI's potential in ways that are inclusive and equitable, emphasizing the importance of achieving the Sustainable Development Goals (SDGs).","context":"The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI governance that can otherwise stifle innovation and reduce opportunities for uptake and use of AI; and finding ways to broaden access globally to the computing power and data that is essential for building and training AI models. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI"}
{"input":"How does the report suggest AI can contribute to addressing climate change?","output":"AI is seen as a critical tool for sustainable digital transformation, offering new capacities for precise decision-making at scale. Examples include creating predictive systems for climate investment, improving net-zero reporting, and developing AI interventions in carbon removal technologies.","context":"(i)    issue a public report describing the potential for AI to improve planning, permitting, investment, and operations for electric grid infrastructure and to enable the provision of clean, affordable, reliable, resilient, and secure electric power to all Americans;\n\n          (ii)   develop tools that facilitate building foundation models useful for basic and applied science, including models that streamline permitting and environmental reviews while improving environmental and social outcomes;\n\n          (iii)  collaborate, as appropriate, with private sector organizations and members of academia to support development of AI tools to mitigate climate change risks; (v)    establish an office to coordinate development of AI and other critical and emerging technologies across Department of Energy programs and the 17 National Laboratories.\n\n     (h)  Within 180 days of the date of this order, to understand AI\u2019s implications for scientific research, the President\u2019s Council of Advisors on Science and Technology shall submit to the President and make publicly available a report on the potential role of AI, especially given recent developments in AI, in research aimed at tackling major societal and global challenges.  The report shall include a discussion of issues that may hinder the effective use of AI in research and practices needed to ensure that AI is used responsibly for research. (iii)  collaborate, as appropriate, with private sector organizations and members of academia to support development of AI tools to mitigate climate change risks;\n\n          (iv)   take steps to expand partnerships with industry, academia, other agencies, and international allies and partners to utilize the Department of Energy\u2019s computing capabilities and AI testbeds to build foundation models that support new applications in science and energy, and for national security, including partnerships that increase community preparedness for climate-related risks, enable clean-energy deployment (including addressing delays in permitting reviews), and enhance grid reliability and resilience; and\n\n          (v)    establish an office to coordinate development of AI and other critical and emerging technologies across Department of Energy programs and the 17 National Laboratories. on Digital Cooperation.\nhttps:\/\/www.un.org\/en\/pdfs\/DigitalCooperation-report-for%20web.pdf\nUSAID.2018. Reflecting the Past, Shaping the Future: Making AI Work for International Development. Washington, DC.\nhttps:\/\/www.usaid.gov\/sites\/default\/files\/documents\/15396\/AI-ML-in-Development.pdf\nVillani, C. and others. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy.\nParis: Conseil national du num\u00e9rique.\nVinuesa, R., Azizpour, H., Leite, I. et al. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals.\nNature Communications, 11, 233.\nhttps:\/\/doi.org\/10.1038\/s41467-019-14108-y\nWhittaker, M. and others. 2018. AI Now Report 2018. New York: AI Now Institute, New York University.\nhttps:\/\/ainowinstitute.org\/AI_Now_2018_Report.pdf\nWhite House. 2020. American Artificial Intelligence Initiative: Year One Annual Report. Office of Science and Technology Policy."}
{"input":"How does the World Bank's AI Policy differ from its previous Disclosure Policy?","output":"The AI Policy transitioned the Bank from a prescriptive disclosure model to one that allows for the disclosure of any information not explicitly exempted, significantly expanding the scope of information available to the public.","context":"AI POLICY HIGHLIGHTS\n\nWhat is the main difference between the Bank's Access to Information Policy (AI Policy) and the previous\nDisclosure policy?\n\nThe AI Policy, launched in 2010, moved the Bank from an approach that spelled out what information is disclosed to\none that allows the Bank to disclose any information in its possession that is not on a list of exceptions set out in the AI\nPolicy.\n\n    \uf0b7    Under the AI Policy, significantly more information on Bank operations and Board proceeding is available.\n    \uf0b7    The new Policy permits public release of some documents prior to discussion by the World Bank's Board of\n         Executive Directors.\n    \uf0b7    Certain restricted information is eligible for declassification after 5, 10, or 20 years.\n    \uf0b7    It also establishes an appeal mechanism that provides public recourse when the Bank denies access to\n         information. AI FAQs\n\nAI POLICY HIGHLIGHTS\n\n    \uf0b7    What is the main difference between the World Bank's Access to Information Policy (AI Policy) and the\n         previous Disclosure policy?\n    \uf0b7    Is the AI Policy retroactive?\n    \uf0b7    Was the AI Policy discussed with member countries?\n    \uf0b7    What can be disclosed under the AI Policy?\n    \uf0b7    What are the exceptions to disclosing information?\n    \uf0b7    Why does the AI Policy restrict some types of information from disclosure?\n\nPROACTIVE DISCLOSURE\n\n    \uf0b7    Where do I find World Bank information?\n    \uf0b7    When do project documents become available?\n    \uf0b7    When do Board papers and Board records become available?\n\nREQUESTING INFORMATION TO THE WORLD BANK\n\n    \uf0b7    Can I contact a Bank staff directly for Bank\u2019s information?\n    \uf0b7    What does it cost to get information from the Bank?\n    \uf0b7    What if the Bank restricts something it is supposed to release when requested?\n    \uf0b7    What is the process to file an appeal?\n\n\n\nAI POLICY HIGHLIGHTS Go Top\n\nWhy does the AI Policy restrict some types of information from disclosure?\n\nThe AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the\npublic and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff and other\nparties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure\ncould cause harm to well defined interests.\n\n                                                                                                                    Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information? Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction"}
{"input":"Is the AI Policy retroactive, and how does it apply to historical information?","output":"Yes, the AI Policy applies retroactively to all information held by the Bank, regardless of when it was created, ensuring broad access to historical documents and data.","context":"Go Top\n\nIs the AI Policy retroactive?\n\nYes. The Policy applies to all information in the Bank's possession, regardless of its age.\n\n                                                                                                                    Go Top\n\nWas the AI Policy discussed with member countries?\n\nPublic consultations on the AI Policy proposal were held in 2009 with various stakeholder groups in 33 countries as\nwell as on the Bank's website. The final AI Policy reflects the input of member countries, civil society organizations,\nacademics, parliamentarians, media, the private sector, other international organizations and donor agencies\n\n                                                                                                                    Go Top\n\fWhat can be disclosed under the AI Policy? Policy implications of artificial intelligence (AI)\n27                                  POSTnote 708 - 9 January 2024\nContributors\nPOST is grateful to Ansh Bhatnagar for researching this briefing, to STFC for funding their \nparliamentary fellowship, and to all contributors and reviewers. For further information on this \nsubject, please contact the co-author, Devyani Gajjar.\nMembers of the POST Board*\nDr Elena Abrusci, Brunel University London* Dr Mhairi Aitken, Alan Turing Institute* Emmanuelle \nAndrews, Liberty\nDr Hayleigh Bosher, Brunel University London* Matt Davies, Ada Lovelace Institute\nMaximilian Gahntz, Mozilla Foundation Conor Griffin, Google DeepMind\nProfessor Oliver Hauser, University of Exeter Harry Law, Google*\nMia Leslie, Public Law Project*\nMavis Machirori, Ada Lovelace Institute Professor Gina Neff, University of Cambridge\nSam Nutt, London Office of Technology and Innovation Lucy Purdon, Mozilla Foundation*\nAdam Smith, British Computer Society* AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. AI FAQs\n\nAI POLICY HIGHLIGHTS\n\n    \uf0b7    What is the main difference between the World Bank's Access to Information Policy (AI Policy) and the\n         previous Disclosure policy?\n    \uf0b7    Is the AI Policy retroactive?\n    \uf0b7    Was the AI Policy discussed with member countries?\n    \uf0b7    What can be disclosed under the AI Policy?\n    \uf0b7    What are the exceptions to disclosing information?\n    \uf0b7    Why does the AI Policy restrict some types of information from disclosure?\n\nPROACTIVE DISCLOSURE\n\n    \uf0b7    Where do I find World Bank information?\n    \uf0b7    When do project documents become available?\n    \uf0b7    When do Board papers and Board records become available?\n\nREQUESTING INFORMATION TO THE WORLD BANK\n\n    \uf0b7    Can I contact a Bank staff directly for Bank\u2019s information?\n    \uf0b7    What does it cost to get information from the Bank?\n    \uf0b7    What if the Bank restricts something it is supposed to release when requested?\n    \uf0b7    What is the process to file an appeal?\n\n\n\nAI POLICY HIGHLIGHTS"}
{"input":"How were stakeholders involved in the development of the AI Policy?","output":"The World Bank conducted public consultations in 33 countries and online, gathering input from member countries, civil society, academics, the media, and other stakeholders to inform the AI Policy's development.","context":"As the word \u201cstakeholder\u201d frequently appears in the G7 statements, the future of AI governance will likely not rely solely on top-down, government-led rule-setting. Instead, it suggests an approach where multi-stakeholder initiatives are discussed on a case-by-case basis and updated in an agile manner. Regulations and technical standards serve as critical tools to facilitate these efforts in building trustworthy AI. However, it is crucial to note that the key players in AI governance, who develop and use these tools, will be not only the government entities, but rather the private stakeholders including AI developers, users, and civil society organizations, and so on. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. FUTURE RESEARCH\nAs policy and regulatory pathways for harnessing AI\nare still in their infancy, further time and research\nis needed to assess the outputs of these strategies,\nand their effectiveness in order to identify best\npractices for developing countries. Rather than\nconcluding with best practices, this article provides\nan initial review of policy and regulatory practices\ncurrently deployed in AI policymaking, illustrating\na wide range of tools and approaches that\ngovernments have taken in their attempt to harness\nAI technologies while mitigating anticipated risks.\nWhile many of the policies observed have been 60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed"}
{"input":"Why is AI significant for the public sector?","output":"AI offers transformative potential for governments by improving personalized service delivery, enhancing efficiency, strengthening policy compliance, and aiding fraud detection. It represents a strategic resource for economic growth and competitiveness.","context":"4\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fHow can countries address these challenges?\nAI policy incorporating legislative foundations, institutional arrangements and a\ngovernance framework for the use of AI could provide the necessary enabling\nenvironment. This could be part of a broader Digital Transformation Policy or a\nstand-alone framework.\nA central innovation hub for AI in government could help pool scarce human\nand financial resources to support the initiatives of line ministries and start to\ndevelop a whole-of-government approach. In the use cases examined here,\nmost governments have already set-up the main hub for AI issues that serves\nas a central authority over AI projects being delivered by line agencies.\nThe AI hub\u2019s role could also be to connect industry expertise to the government\ndepartments and line agencies, to promote research, and build alliances with\nacademic institutions and the private sector. 2\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. volumes of research data (OECD, 2020[5]).\nAI can also be leveraged by governments to innovate and transform the public\nsector. AI promises to make government services \u201csmarter\u201d: more agile,\nefficient and user-friendly. For instance, AI can help deliver personalised\nservices to citizens. It can also enhance the efficiency and quality of\nadministrative procedures by automating physical and digital tasks. In addition,\nit can improve decisions through better predictions based on patterns in large\nvolumes of data. Building on their digital government approaches, many\nnational AI strategies and policies explicitly encourage the adoption of AI in the\npublic sector. Public entities can use AI to strengthen law enforcement\ncapabilities and improve policy implementation. AI is also expected to free up\npublic servants\u2019 time and allow them to shift to higher-value work (Berryhill\net al., 2019[6])."}
{"input":"What are the main challenges to AI adoption in the public sector?","output":"Key barriers include lack of awareness of AI's potential, inadequate digital technologies, poor data quality, low digital skills, and the absence of a supportive policy and legal environment.","context":"What are the main challenges?\nIn most of the World Bank\u2019s client countries, a lack of awareness of the potential of\nAI, inadequate foundational digital technologies, low availability or quality of data,\nand low access to digital skills, are the major barriers to AI adoption see Figure 1.\nAs of today, the appropriate policy and legal environment for AI do not yet exist.\nPublic administrations that lack digital processes, data collection capabilities,\ntechnical skills in the civil service and digital infrastructure are unlikely to be able\nto benefit from the application of AI in the short to medium term, and the policy\nand legal frameworks need to first be more developed.\nThat said, despite the digital divide that exists across countries in terms of\nfulfilling the prerequisites for AI adoption, several use cases are now emerging\nin our client countries and several governments have started piloting AI to\naddress their development challenges. 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. secure processing;\n\u2022 Targeting key sectors for AI adoption for economic growth and human development; and\n\u2022 Adopting AI to improve public service delivery and resource efficiency while building capability\nand expertise. \u2022\n\nIdentify and manage risks rather than avoiding them. Good algorithm impact assessment framework models exist, which can\nbe tailored to suit a country\u2019s context.\n\nGovernments and world leaders are instrumental in guiding the transition to automation and AI. They can provide leadership to influence the trajectory of AI adoption among citizens at national and international levels. This will help avoid adverse consequences\nand reap productivity gains. National governments could choose global guiding principles that will inevitably shape the acceptance\nor rejection of AI. Since AI will have a profound influence on service delivery, citizen engagement, and core operations, it is imperative to formulate a cohesive governance model that supports the process of ethical implementation.\n12\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fSupported by the GovTech Global Partnership\n\nRepublic of Korea"}
{"input":"How can AI impact the workforce in the public and private sectors?","output":"AI and automation are expected to replace up to 30% of current jobs by 2030, necessitating programs to upskill the workforce and policies to manage this transition effectively.","context":"2\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR 3                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nBenefits and risks\nJob creation and displacement\nIn 2022, a report commissioned for the Department for Science, Innovation & Technology stated that \ninvestment in the UK AI sector grew five-fold between 2019 and 2021, and that it brought in \u00a310.6 \nbillion in revenue and employed over 50,000 people.28\nOECD analysis published in July 2023 found that AI was changing the nature of work by assisting \nworkers and reducing the time spent on mundane tasks, rather than causing job losses.29\nHowever, some emerging academic research indicates that developments in generative AI may be linked \nto a loss in the quantity and earnings of white-collar jobs.30,31\nThere is potential for new jobs across all sectors to be created with improved productivity and \nglobal economic growth (PB 57).32 However, some reports state that certain jobs, such as clerical As more decision making becomes automated through the use of machine learning and models we can expect this to change the\ndemand for workers, with that increasing for high-skilled workers and decreasing for lower-skilled workers. The risk of job losses\nboth in the public and private sectors will therefore need to be managed. Governments can promote the use of human-enhancing\nAI and manage the use of AI which replaces labor, and at the same time launch programs to skill-up the public sector workforce\nand introduce policies that manage this transition. The McKinsey Global Institute has estimated that in the public and private\nsectors combined, as much as 30 percent of today\u2019s jobs will be replaced by AI and automation by 2030, and up to 375 million\nworkers worldwide could be affected by emerging technologies.\nThe legislative framework for data protection and privacy is key to the effective use of AI and are widely enacted. However, secure processing;\n\u2022 Targeting key sectors for AI adoption for economic growth and human development; and\n\u2022 Adopting AI to improve public service delivery and resource efficiency while building capability\nand expertise."}
{"input":"What principles guide the OECD's approach to artificial intelligence?","output":"The OECD AI Principles promote AI that is innovative, trustworthy, respects human rights, democratic values, and ensures safety and accountability.","context":"\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI.\n\nTo develop practical guidance to implement the AI Principles as mandated by the\nOECD Council, the OECD convened a multi-stakeholder and multi-disciplinary OECD\nNetwork of Experts on AI in early 2020 that is developing a report on the State of\nimplementation of the OECD AI Principles: Insights from national AI policies (OECD,\n2021[1]). The report provides good practices and lessons learned on the\nimplementation of the five recommendations to policy makers contained in the\nOECD AI Principles.\nSource: oecd.ai\/ai-principles and oecd.ai\/network-of-experts. is a relatively new phenomenon. To track these initiatives, the OECD AI Policy\nObservatory (OECD.AI) comprises over 620 national AI policies from over 60\ncountries and the European Union (EU). These resources provide a baseline to\nmap countries\u2019 AI policy initiatives according to the recommendations to\ngovernments contained in the OECD AI Principles (Box 1).\nBox 1. OECD AI Principles\nThe OECD Principles on Artificial Intelligence promote AI that is innovative,\ntrustworthy and respects human rights and democratic values. They were adopted\nin May 2019 by OECD member countries when they approved the OECD Council\nRecommendation on Artificial Intelligence. The OECD AI Principles identify five\ncomplementary values-based principles for the responsible stewardship of\ntrustworthy AI:\n\u2022 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021"}
{"input":"What is the World Bank's policy regarding access to information?","output":"In the interest of maintaining transparency and accountability, the World Bank maintains a comprehensive information access policy and will disclose any information requested that does not fall into its defined list of exceptions. However, the World Bank maintains its authority to exercise its judgment when releasing information.","context":"Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction World Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\nI. Introduction\n1. The World Bank1 recognizes that transparency and accountability are of\nfundamental importance to the development process and to achieving its mission to end\nextreme poverty and promote shared prosperity. Transparency is essential to building\nand maintaining public dialogue and increasing public awareness about the Bank\u2019s\ndevelopment role and mission. It is also critical for enhancing good governance,\naccountability, and development effectiveness. Openness promotes engagement with\nstakeholders, which, in turn, improves the design and implementation of projects and public only after the country (or other third-party) authority concerned has\ngranted its written consent to disclose.\n21. Documents Prepared Jointly with Partners. The Bank makes available to the\npublic the final documents that it prepares jointly with partners after reaching agreement\non such disclosure with the partner concerned.20\nIV. Implementation Aspects of the Policy\nA. Accessing Information\n22. Proactive Disclosure. The Bank routinely discloses a wide range of documents,\nthrough its external website \u2013 www.worldbank.org \u2013 as soon as the documents are\nfinalized after key process milestones. Under this Policy, the Bank significantly\nincreases the amount of information it makes available to the public, particularly\ninformation related to projects under implementation and to the actions of the Board of\nExecutive Directors.\nunder IBRD Loans and IDA Credits, and Guidelines: Selection and Employment of Consultants by\nWorld Bank Borrowers). IV.\nA.\n\nImplementation Aspects of the Policy\n\nAccessing Information\n\n22.\nProactive Disclosure. The Bank routinely discloses a wide range of documents,\nthrough its external website \u2013 www.worldbank.org \u2013 as soon as the documents are\nfinalized after key process milestones. Under this Policy, the Bank significantly\nincreases the amount of information it makes available to the public, particularly\ninformation related to projects under implementation and to the actions of the Board of\nExecutive Directors.\n\n19\n\n20"}
{"input":"What sort of information falls under the World Bank's list of exceptions when filing a request for information disclosure?","output":"The World Bank does not provide access to its information if releasing that information would cause harm to any specific individual or parties. Additionally, the World Bank does not release the following categories of information: personal information, communications of Executive Directors' Offices, the proceedings of its Ethics Committee, Attorney-Client privileged information, security and safety information, information restricted by agreements with third parties, administrative matters, deliberative information, and financial information.","context":"31.\nThe Bank declassifies and discloses\u2014routinely on the Bank\u2019s external website,\nand in response to requests\u2014certain types of restricted information (including\ninformation prepared under earlier disclosure policies of the Bank) as their sensitivity\ndiminishes over time. Some restricted information is not eligible for declassification.\n32.\nInformation Not Eligible for Declassification. The information that is restricted\nfrom disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member The Policy explicitly sets out 10 categories of information that are considered to be exceptions and not eligible for\ndeclassification:\n\n    1.  Personal information\n    2.  Communications of Executive Directors' offices\n    3.  Proceedings of Ethics Committee for Board Officials\n    4.  Information subject to attorney-client privilege\n    5.  Security and safety information\n    6.  Information under separate disclosure regimes and other investigative information\n    7.  Information provided by member countries or third parties in confidence\n    8.  Corporate administrative matters\n    9.  Deliberative information such as emails, notes, memoranda, draft reports, statistical analyses that inform the\n        Bank's internal decision-making processes and individual audit reports.\n    10. Financial Information such as banking or billing information of World Bank Group entities, member countries,\n        clients, donors, recipients, or vendors, including consultants. from disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member\ncountries, clients, donors, recipients, or vendors, including consultants) and the financial\nand deliberative information that contains or relates to such information, is not\ndeclassified or made publicly available.\n26 See footnote 11.\n13\n33. Information Eligible for Declassification. The following documents are\ndeclassified and made publicly available 5, 10, or 20 years after the date on the (d) Board papers distributed to the Executive Directors for information are posted\nupon distribution.\n24.\nInformation on Request. Information that is disclosable under this policy and is\nnot on the Bank\u2019s external website is available on request (subject to paragraph 26). Such\nrequests may be submitted in writing by electronic means, mail, or fax. Requests should\nindicate, with reasonable specificity, the information that is being sought, to enable the\nBank to locate the information within a reasonable period of time. If a particular\ndocument is required, it should be identified precisely, preferably by date and title.\nElectronic submissions should be directed through the World Bank website:\nwww.worldbank.org\/wbaccess. Requests may also be made in person at the InfoShop in\nWashington, DC, or the Public Information Centers located in member countries.\n25.\nTimelines for Responding to Requests. The Bank acknowledges receipt of"}
{"input":"Tell me the guiding principles of the World Bank's access to information policy.","output":"The World Bank's Access to Information Policy Guiding Principles are as follows: maximizing access to information, setting out a clear list of exceptions, safeguarding the deliberative process, providing clear procedures for making information available, and recognizing requesters' right to an appeals process.","context":"Guiding Principles. This Policy is based on five principles:\n\uf0b7\n\uf0b7\n\uf0b7\n\uf0b7\n\uf0b7\n\nMaximizing access to information.\nSetting out a clear list of exceptions.\nSafeguarding the deliberative process.\nProviding clear procedures for making information available.\nRecognizing requesters\u2019 right to an appeals process.\n\nThis policy statement is divided into four parts. Following this introduction, Part II sets\nout the core policy, Part III sets out how the Bank handles information relating to\nmember countries and other parties, and Part IV sets out the implementation aspects of\nthe Policy.\n\nII.\n\nThe Policy Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction World Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\nI. Introduction\n1. The World Bank1 recognizes that transparency and accountability are of\nfundamental importance to the development process and to achieving its mission to end\nextreme poverty and promote shared prosperity. Transparency is essential to building\nand maintaining public dialogue and increasing public awareness about the Bank\u2019s\ndevelopment role and mission. It is also critical for enhancing good governance,\naccountability, and development effectiveness. Openness promotes engagement with\nstakeholders, which, in turn, improves the design and implementation of projects and IV.\nA.\n\nImplementation Aspects of the Policy\n\nAccessing Information\n\n22.\nProactive Disclosure. The Bank routinely discloses a wide range of documents,\nthrough its external website \u2013 www.worldbank.org \u2013 as soon as the documents are\nfinalized after key process milestones. Under this Policy, the Bank significantly\nincreases the amount of information it makes available to the public, particularly\ninformation related to projects under implementation and to the actions of the Board of\nExecutive Directors.\n\n19\n\n20"}
{"input":"What are three use cases of AI-ready data that can advance the public good?","output":"Defense: The Biden administration called the transnational organized crime a billion-dollar problem affecting millions of lives through drug overdose, violence, firearm deaths, and human trafficking. In response, the Defense Department, Defense Intelligence Unit, and AI company Quantifind launched the Countering Malign Influence project. The project uses open-source data to identify, track, and counter transnational criminal groups attempting to mask their identities and activities. The speed and volume of data collected would be too large for human analysts alone to process.\nHealth: Data collection for health research typically lacks participants with diverse backgrounds. NIH\u0092s Bridge2AI program builds participant diversity into the design of all funded projects. For example, Bridge2AI recently funded a University of Washington-led coalition to create a flagship, ethically sourced data set to uncover how human health is restored after disease. The effort, which is using Type 2 diabetes as a case study, will recruit an equal number of Black, Hispanic\/Latinx, Asian, and white participants while engaging with tribal communities to address barriers to participation.\nEarth Science: Rip currents cause hundreds of drownings and require tens of thousands of rescues annually. In response, NOAA launched the first national rip current forecast model to inform coastal communities and visitors about the risk of rip currents. The model uses AI to generate the probability of a rip current based on NOAA Coastal Observation Networks.","context":"What are positive-use cases of AI-ready data?\nAI-ready data can advance the public good; three examples include: What is AI-ready data?\nWhy is AI-ready data important to the federal government\u2019s AI agenda?\nWhere is AI-ready data being applied across federal agencies?\nHow could AI-ready data become the federal standard?\nWhat is AI-ready data?\nThe federal government is generating and collecting data from sensors, systems, and contractors at a rapid pace, but collecting raw data is only the first step. The next step is to clean and process this data into a usable format that can power an AI application\u2014we call this being \u201cAI-ready.\u201d At the same time, ethical challenges arise when AI-ready systems are being implemented. The Pentagon\u2019s controversial surveillance video program, Project Maven, highlighted the need for government to be more transparent about its efforts to develop AI. In 2020, the Defense Innovation Unit responded to this need by publishing Responsible AI Guidelines. Other agencies are using this opportunity to ensure their data sets are ethically sourced and do not perpetuate inequities that often occur during data collection and analysis.\n\nHow does AI-ready data fit into the federal government\u2019s AI and data agenda?\nhttps:\/\/bipartisanpolicy.org\/wp-content\/uploads\/2023\/02\/AI-Ready-Open-Data.jpg Artificial intelligence and machine learning (AI\/ML) have the potential to create applications that tackle societal challenges from human health to climate change. These applications, however, require data to power AI model development and implementation. Government\u2019s vast amount of open data can fill this gap: McKinsey estimates that open data can help unlock $3 trillion to $5 trillion in economic value annually across seven sectors. But for open data to fuel innovations in academia and the private sector, the data must be both easy to find and use. While Data.gov makes it simpler to find the federal government\u2019s open data, researchers still spend up to 80% of their time preparing data into a usable, AI-ready format. As Intel warns, \u201cYou\u2019re not AI-ready until your data is.\u201d\n\nIn this explainer, the Bipartisan Policy Center provides an overview of existing efforts across the federal government to improve the AI readiness of its open data. We answer the following questions:"}
{"input":"What does the UK Department for Science, Innovation and Technology\u0092s taskforce define itself as, and why?","output":"The Taskforce describes itself as a \u0093start-up inside government.\u0094 It was formed at the direction of UK Prime Minister Rishi Sunak to create an AI research team that can \u0093evaluate the risks at the frontier of AI.\u0094","context":"The UK Department for Science, Innovation and Technology\u2019s \u2018Frontier AI Taskforce\u2019 issues second report\nOnly eighteen months into its formation, and just ahead of the UK-organized AI Safety Summit, the \u2018Frontier AI Taskforce\u2019 issued its second progress report. Since its first report in September, the group has tripled its research capacity, cemented new partnerships with leading AI organizations, and supported the development of Isambard-AI, an AI supercomputer where more intensive safety research will be conducted, among other developments. Policy implications of artificial intelligence (AI)\nHuman-Centered Artificial Intelligence (HAI).\n19. Jones, E. Explainer: What is a foundation model? Ada Lovelace Institute.\n20. Google Google Generative AI.\nGoogle AI.\n21. Department for Science, Innovation and Technology (2023). AI regulation: a pro- innovation \napproach. GOV.UK.\n22. Department for Science, Innovation and Technology (2023). Bristol set to host UK\u2019s most \npowerful supercomputer to turbocharge AI innovation. GOV.UK.\n23. Foreign, Commonwealth & Development Office et al. (2023). AI Safety Summit 2023. GOV.UK.\n24. Department for Science, Innovation and Technology et al. (2023). The Bletchley Declaration by \nCountries Attending the AI Safety Summit, 1-2 November 2023.\n25. Donelan, M. (2023). Introducing the AI Safety Institute. GOV.UK.\n26. The Alan Turing Institute (online). Data science and AI glossary.\n27. Cabinet Office et al. (2021). Ethics, Transparency and Accountability Framework for Automated Tailored advisory to support businesses\u2019 scale-up\nDigital Catapult\nResponsible entity: UK Government\nDescription: The UK has a network of catapults helping to promote the\nadoption of technologies. The digital catapult leads on AI-related efforts and\noffers access to compute credits to businesses and links to cloud computing\nresources, which are vital for businesses that could benefit from AI but lack the\ntechnical infrastructure. The digital Catapult works with 30 start-ups per year\nand uses a competitive process. To monitor the efficacy of R&D projects, the\nproject relies on a wider framework in place with partners at UK Research and\nInnovation. Innovate UK focuses on commercialisation in some of its\nprogrammes like Knowledge Transfer Networks; which can show \u201cwins\u201d but\ngenerally uses indirect measures of impact and influence such as citations. The\nU.K. is focusing on building the ecosystem this decade and will focus on AI\napplications next decade. (A)  coordinating or developing guidelines related to assessing and managing the safety, security, and trustworthiness of dual-use foundation models; and\n\n               (B)  in coordination with the Secretary of Energy and the Director of the National Science Foundation (NSF), developing and helping to ensure the availability of testing environments, such as testbeds, to support the development of safe, secure, and trustworthy AI technologies, as well as to support the design, development, and deployment of associated PETs, consistent with section 9(b) of this order."}
{"input":"Tell me about CRISPR and AI and China\u0092s stance on this being an ethical risk.","output":"CRISPR is a controversial gene modification technique that can be used to alter the presentation of genes in living organisms, for example for the purpose of curing or preventing genetic diseases. It is closely related to AI, as Machine Learning techniques can be used to identify which gene or genes need to be altered with the CRISPR method. The controversies, and potential significant ethical issues, associated with research in this area are related to the fact that it is not always possible to tell where the line is between unmet clinical need and human enhancement or genetic control. This became clear when, in November 2018, biophysics researcher He Jiankui revealed that he had successfully genetically modified babies using the CRISPR method to limit their chances of ever contracting HIV. The announcement was met with international outcry and He\u0092s experiment was condemned by the Chinese government at the time. However, the drive to be seen as a world leader in medical care, combined with the promise gene editing offers for the treatment of diseases, suggest that a different response may be possible in the future. Such a change in government policy is especially likely as global competition in this field heats up. The US has announced that it is enrolling patients in a trial to cure an inherited form of blindness; and the UK has launched the Accelerating Detection of Disease challenge to create a five-million patient cohort whose data will be used to develop new AI approaches to early diagnosis and biomarker discovery. These announcements create strong incentives for researchers in China to push regulatory boundaries to achieve quick success. China has filed the largest number of patents for gene-editing on animals in the world","context":"The third ethical risk is associated with CRISPR gene modification and AI. CRISPR is a controversial gene modification technique that can be used to alter the presentation of genes in living organisms, for example for the purpose of curing or preventing genetic diseases. It is closely related to AI, as Machine Learning techniques can be used to identify which gene or genes need to be altered with the CRISPR method. The controversies, and potential significant ethical issues, associated with research in this area are related to the fact that it is not always possible to tell where the line is between unmet clinical need and human enhancement or genetic control (Cohen 2019). This became clear when, in November 2018, biophysics researcher He Jiankui revealed that he had successfully genetically modified babies using the CRISPR method to limit their chances of ever contracting HIV (Cohen 2019). The announcement was met by international outcry and He\u2019s experiment was condemned by the In the same vein, when focusing on medical ethics, it is clear that, although China may agree with the West on the bioethical principles, its focus on the health of the population, in contrast to the West\u2019s focus on the health of the individual, may easily lead to unethical outcomes (the sacrifice imposed on one for the benefit of many) and is creating a number of risks, as AI encroaches on the medical space. These are likely to evolve over time, but the risks of unequal care between those who can afford a human clinician and those who cannot, control of social diseases, and of unethical medical research are currently the most significant. modified babies using the CRISPR method to limit their chances of ever contracting HIV (Cohen 2019). The announcement was met by international outcry and He\u2019s experiment was condemned by the Chinese government at the time (Belluz 2019). However, the drive to be seen as a world leader in medical care (Cheng 2018), combined with the promise gene editing offers for the treatment of diseases, suggest that a different response may be possible in the future (Cyranoski 2019; \u201cChina opens a Pandora\u2019s Box\u201d, 2018). Such a change in government policy is especially likely as global competition in this field heats up. The US has announced that it is enrolling patients in a trial to cure an inherited form of blindness (Marchione 2019); and the UK has launched the Accelerating Detection of Disease challenge to create a five-million patient cohort whose data will be used to develop new AI approaches to early diagnosis and biomarker discovery (UK Research and Innovation 2019). These announcements to create a five-million patient cohort whose data will be used to develop new AI approaches to early diagnosis and biomarker discovery (UK Research and Innovation 2019). These announcements create strong incentives for researchers in China to push regulatory boundaries to achieve quick successes (Tatlow 2015; Lei et al. 2019). China has filed the largest number of patents for gene-editing on animals in the world (Martin-Laffon et al. 2019). Close monitoring will be essential if further ethical misdemeanours are to be avoided."}
{"input":"Why are nations around the world concerned about investing in artificial intelligence?","output":"Nations around the world are concerned with investing in technology to develop artificial intelligence to outperform their competition in various areas of national security.","context":"HM Government. 2018. Industrial Strategy: Artificial Intelligence Sector Deal.\nhttps:\/\/assets.publishing.service.gov.uk\/government\/uploads\/system\/uploads\/attachment_data\/file\/702810\/180425_BEIS_AI_Sector_\nDeal__4_.pdf\nHolon IQ. 2020. The Global AI Strategy Landscape.\nhttps:\/\/www.holoniq.com\/notes\/the-global-ai-strategy-landscape\/\nIDC (International Data Corporation). 2019. \u201cSpending on Artificial Intelligence Systems in the Middle East & Africa to Top $374 Million in\n2020.\u201d IDC Media Center, September 30.\nhttps:\/\/www.idc.com\/getdoc.jsp?containerId=prMETA45546719\nIFC (International Finance Corporation). 2020. Artificial Intelligence in Emerging Markets: Opportunities, Trends, and Emerging Business\nModels. Washington, D.C.: International Finance Corporation.\nhttps:\/\/www.ifc.org\/wps\/wcm\/connect\/publications_ext_content\/ifc_external_publication_site\/publications_listing_page\/artificial+intelligence+in+emerging+markets Many governments view AI as a strategic resource to enhance a countries\u2019\ncompetitiveness and to boost economic growth. AI can potentially contribute\n$13 trillion to the global economy by 2030, according to a 2018 report by\nMcKinsey & Company. At least 50 governments have developed or are in\nImportance of AI\nthe process of developing an AI strategy. However, the pace of AI adoption is\nuneven, and most countries that the Bank is actively supporting to modernize\n\u2022 $13 trillion potential by 2030\ngovernments, are either not ready for AI adoption or at very early stages. There\n\u2022 50 Governments are preparing\nis no country from the Africa or Latin America regions, for example, in the list of\nAI Strategies.\nthe top 20 countries on the AI Readiness Index developed by Oxford Insights.\nExcept for four economies, the Asia-Pacific region is also not well advanced in\nAI. Slower adoption of AI in client countries may have the potential to lead to Against this backdrop, a report by PwC suggested that China is the country that has the most to gain from AI, with a boost in GDP of up to 26% by 2030 (\u201cSizing the Prize\u201d 2017). Estimates also suggest that AI could facilitate an increase in employment by 12% over the next two decades (\u201cNet Impact of AI on Jobs in China?\u201d 2018). Because of these potential benefits, President Xi has frequently spoken of the centrality of AI to the country\u2019s overall economic development (Hickert and Ding 2018; Kania 2018b). China has been pursuing the potential economic benefits of AI concretely and proactively for some time. For example, there has been a 500% increase in the annual installation of robotic upgrades since 2012. This rate is staggering, especially when compared to a rate of just over 100% in Europe (Shoham et al. 2018), equating to over double the number of robot installations in China than Europe. to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response."}
{"input":"What are the current efforts being pursued by countries to get ahead of their competition regarding AI?","output":"To outperform competition in the field of Artificial Intelligence, countries around the world are seeking to control access to computing resources (which are necessary to train bigger models more quickly and process large amounts of data), mitigate economic disasters related to AI failure, ensure defense related AI systems are robust and secure, and understanding how competitors are using AI.","context":"categories of tools and instruments that were used\nto assess national AI strategies. The third section\npresents findings from eleven countries, beginning\nwith a more detailed analysis of the national AI\npolicies of Finland and the United Arab Emirates\n(UAE). This section also includes findings from an\nanalysis of a broader set of countries in Asia, Africa\nand South America, highlighting the approach\ntaken to AI in some developing countries. The final\nsection highlights future areas of research needed\nto expand upon this work. In this article, we analysed the nature of AI policy within China and the context within which it has emerged, by mapping the major national-level policy initiatives that express the intention to utilise AI. We identified three areas of particular relevance: international competitiveness, economic growth, and social governance (construction). The development and deployment of AI in each of these areas have implications for China and for the international community. For example, although the \u2018trump-card\u2019 policy to gain a military advantage may not be something new, its application to AI technologies risks igniting an arms race and undermining international stability (Taddeo and Floridi 2018). Efforts to counteract this trend seem largely hollow. Our analysis indicates that China has some of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions Press Release, January 20. http:\/\/www.mcit.gov.eg\/Media_Center\/Press_Room\/Press_Releases\/28084\nArabian Business. 2018. \u201cUAE Adopts New National AI Strategy Aimed at Becoming Global Leader.\u201d Arabian Business, April 22.\nhttps:\/\/www.arabianbusiness.com\/technology\/418254-uae-adopts-new-national-ai-strategy-aimed-at-becoming-global-leader\nArshadi, A. K. and others. 2020. \u201cArtificial Intelligence for Covid-19 Drug Discovery and Vaccine Development.\u201d\nFrontiers in Artificial Intelligence, 18.\nhttps:\/\/doi.org\/10.3389\/frai.2020.00065\nAstana Times. 2019. \u201cMicrosoft Kazakhstan to Assist Domestic Companies Integrate AI Technology.\u201d The Astana Times, March 18.\nhttps:\/\/astanatimes.com\/2019\/03\/microsoft-kazakhstan-to-assist-domestic-companies-integrate-ai-technology\/\nAU-EU Digital Economy Task Force (AU-EU DETF). 2019. New Africa-Europe Digital Economy Partnership: Accelerating the Achievement\nof the Sustainable Development Goals. FINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re-\n\nview of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries."}
{"input":"Why is artificial intelligence important for a country to invest in?","output":"Investing in artificial intelligence is one of many necessary investments for a country to meet economic success and guarantee national security. Artificial Intelligence has use cases that can affect nearly every industry, from defense and logistics to finance and automation. AI technologies are tools that can provide for more efficient allocation of resources and for help with human decision-making when too much or too little data is present.","context":"Many governments view AI as a strategic resource to enhance a countries\u2019\ncompetitiveness and to boost economic growth. AI can potentially contribute\n$13 trillion to the global economy by 2030, according to a 2018 report by\nMcKinsey & Company. At least 50 governments have developed or are in\nImportance of AI\nthe process of developing an AI strategy. However, the pace of AI adoption is\nuneven, and most countries that the Bank is actively supporting to modernize\n\u2022 $13 trillion potential by 2030\ngovernments, are either not ready for AI adoption or at very early stages. There\n\u2022 50 Governments are preparing\nis no country from the Africa or Latin America regions, for example, in the list of\nAI Strategies.\nthe top 20 countries on the AI Readiness Index developed by Oxford Insights.\nExcept for four economies, the Asia-Pacific region is also not well advanced in\nAI. Slower adoption of AI in client countries may have the potential to lead to AI policy implementation\nInvesting in AI R&D\nEnhancing national AI research and development (R&D) capabilities is a key\ncomponent of many national AI strategies and policies. AI is a general-purpose\ntechnology with implications across industries. It is also called an \u201cinvention of\na method of invention\u201d (Cockburn, 2018[2]) and is already widely used by\nscientists and inventors to facilitate innovation. Entirely new industries could\nbe created based on the scientific breakthroughs enabled by AI. This\nunderscores the key role of governments in providing sustained investment in\nAI basic research with long term horizons, particularly in areas under-served by\nmarket-driven investments. In addition, research institutions in all areas require\ncapable AI systems to remain competitive, particularly in biomedical science\nand life science fields.\nThe allocation of public budgets to AI R&D varies in scale across countries. mentioned AI as one of the six critical areas for developing the country\u2019s emerging industries (CCP 2016), and as an important factor in stimulating economic growth. When read together, these documents indicate that there has been a conscious effort to develop and use AI in China for some time, even before \u2018the Sputnik moment\u2019. However, prior to 2016, AI was presented merely as one technology among many others, which could be useful in achieving a range of policy goals. This changed with the release of the AIDP. Against this backdrop, a report by PwC suggested that China is the country that has the most to gain from AI, with a boost in GDP of up to 26% by 2030 (\u201cSizing the Prize\u201d 2017). Estimates also suggest that AI could facilitate an increase in employment by 12% over the next two decades (\u201cNet Impact of AI on Jobs in China?\u201d 2018). Because of these potential benefits, President Xi has frequently spoken of the centrality of AI to the country\u2019s overall economic development (Hickert and Ding 2018; Kania 2018b). China has been pursuing the potential economic benefits of AI concretely and proactively for some time. For example, there has been a 500% increase in the annual installation of robotic upgrades since 2012. This rate is staggering, especially when compared to a rate of just over 100% in Europe (Shoham et al. 2018), equating to over double the number of robot installations in China than Europe."}
{"input":"What are the twin goals of Singapore\u2019s NAIS 2.0(National AI Strategy)?","output":"One is excellence: Singapore will selectively develop peaks of excellence in AI, to advance the field and maximize value creation. The other is empowerment: Singapore will raise up individuals, businesses, and communities to use AI with confidence, discernment, and trust.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f22 | AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f16 | AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 17 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13"}
{"input":"To achieve the vision and goals, Singapore will direct efforts under NAIS 2.0 toward three Systems and 10 Enablers. What are these 3 systems and 10 Enablers? ","output":"System 1: Activity Drivers (Enablers: Industry, Government, Research). Industry, Government, and public research performers have deep technical capabilities that can be applied to deliver value. We need to orchestrate them around meaningful use cases and problem statements to transform our economy and society. \nSystem 2: People & Communities (Enablers: Talent, Capabilities, Placemaking). We will attract more top-tier researchers and engineers to work with and from Singapore. More of our technology workforce should work to scale novel AI solutions, that form part of the toolkit which a confident base of enterprises and workers can use.\nSystem 3: Infrastructure &Environment, Leader in Thought and Action). We will ensure that Singapore hosts the necessary infrastructure and provides a trusted environment for AI innovation. This will make us a credible leader and preferred site for AI development, deployment, and adoption.\n","context":"Description: The Singapore National AI Strategy was launched in 2019 and\naims to accelerate the development and deployment of AI in Singapore. The\nstrategy adopts a human-centric approach to AI and focuses on an initial five\nnational projects of high social and economic value in logistics, municipal\nservices, healthcare, education and border security. It also outlines plans to\nstrengthen five ecosystem enablers, which include talent and education, data\narchitecture, a progressive and trusted environment, international\ncollaboration, and a \u201ctriple-helix\u201d partnership between the research\ncommunity, industry and government. The strategy also identifies key nontechnological hurdles in AI deployment which Singapore believes it is uniquely\npositioned to address (e.g. stakeholder buy-in, governance, process change)\nand outlines Singapore\u2019s plans to become a global hub for developing, testbedding, deploying, and scaling AI solutions.\nRead more: https:\/\/oecd.ai\/dashboards\/countries\/Singapore. The frameworks and tools created by these international organizations and initiatives will serve as benchmarks that countries can refer to, for example, in considering regulatory frameworks or allocating civil liabilities.\nSupport for International Technical Standards\n\nThe other approach stressed in the G7 Statements to ensure interoperable AI governance framework is the support for international technical standards by standards development organizations (SDOs). Figures\nFigure 1. A selection of national AI governance approaches ................................................. 8\nFigure 2. National AI strategies and policies prioritise a number of sectors ................... 10\n\nBoxes\nBox 1. OECD AI Principles................................................................................................................. 5\nBox 2. Key considerations when implementing AI strategies and policies...................... 13\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f|3\n\nTable of Contents\n\nAn overview of national AI strategies and policies ................................................ 4\nAI policy design ............................................................................................................................... 6\nAI policy implementation ............................................................................................................. 8\nAI policy intelligence to monitor implementation .............................................................. 14\nInternational co-operation on AI .............................................................................................. 14\nAnnex. A selection of national AI policies .............................................................................. 26\nReferences ...................................................................................................................................... 26"}
{"input":"How will Singapore intensify the promotion of AI adoption across all enterprises?","output":"(1) Make available tools that enterprises can use to evaluate their readiness to adopt AI. For example, self-assessment tools like AISG\u2019s AI Readiness Index (AIRI) are useful in helping companies identify and map out the capabilities and infrastructure they need for using AI.\n(2) We will promote baseline digital adoption for our enterprises and enhance the enterprise digitalisation toolkit, to support more sophisticated AI adoption. Businesses\u2019 first interactions with AI often happen through their existing enterprise solutions. Only after they have had the foundational capabilities in place, and are convinced of AI\u2019s benefits, do they then turn to more advanced or bespoke AI business solutions. Through programmes like SMEs Go Digital and CTO-as-a-Service (CTOaaS), the Government has helped businesses gain awareness of suitable AI-enabled solutions. We have also encouraged the adoption of sector-relevant AI solutions, through our refreshed Industry Digital Plans (IDPs).\n(3) For more digitally mature enterprises, we will provide tailored support for AI-enabled business transformation. This includes leveraging IMDA and EnterpriseSG\u2019s Digital Leaders Programme (DLP), which helps companies build in-house digital capabilities, as well as the scoping of impactful AI projects in partnership with AI solution providers.\n","context":"Description: The Singapore National AI Strategy was launched in 2019 and\naims to accelerate the development and deployment of AI in Singapore. The\nstrategy adopts a human-centric approach to AI and focuses on an initial five\nnational projects of high social and economic value in logistics, municipal\nservices, healthcare, education and border security. It also outlines plans to\nstrengthen five ecosystem enablers, which include talent and education, data\narchitecture, a progressive and trusted environment, international\ncollaboration, and a \u201ctriple-helix\u201d partnership between the research\ncommunity, industry and government. The strategy also identifies key nontechnological hurdles in AI deployment which Singapore believes it is uniquely\npositioned to address (e.g. stakeholder buy-in, governance, process change)\nand outlines Singapore\u2019s plans to become a global hub for developing, testbedding, deploying, and scaling AI solutions.\nRead more: https:\/\/oecd.ai\/dashboards\/countries\/Singapore. stimulate AI uptake, including within the public sector. Malta has also\nreformed the Seed Investments Scheme with more favourable tax credit\nconditions for innovative AI firms.\n3. Connecting emerging companies with business opportunities\nthrough networking and collaborative platforms. Another way that\ncountries boost the development of innovative AI research ecosystems\nis by establishing networking and collaborative platforms, such as AI\nhubs, AI labs and AI accelerator programmes. They facilitate cooperation\nbetween industry, academia and public research institutes.\n4. Providing tailored advisory to support business scale-up. Countries\nare introducing a wide range of policy measures and initiatives to spur\ninnovation and AI adoption by businesses, particularly SMEs. For\nexample, the European Commission\u2019s AI4EU project is an AI-on-demand\nplatform to help EU SMEs adopt AI. hard law approaches for managing AI development\nacross any of the key policy domains, though it has\nindicated committing to ethical standards through\nframeworks and toolkits, and providing strategic\ninvestments and partnerships to advance adoption\nof AI within targeted sectors. Both governments\nalso place high strategic importance on engaging\nmulti-stakeholder collaborations to facilitate\nAI. However, while the UAE has taken a hybrid\napproach with more top-down directed initiatives\nto accelerate the development of an AI ecosystem,\nFinland has instead exhibited a mostly bottom-up\napproach. Other key differences include the greater\nfocus the UAE has had on acquiring AI talent\nfrom abroad, as well as their more emphatic role in\ndirecting sectoral adoption of AI.\n\nworldbank.org\/digitaldevelopment\n23\n\n\fANALYTICAL INSIGHTS - NOTE 4 Except for four economies, the Asia-Pacific region is also not well advanced in\nAI. Slower adoption of AI in client countries may have the potential to lead to\nfurther inequality between the rich and the poor nations. To ensure our clients are aware of the potential for AI to advance their\ndevelopment, this note highlights opportunities and risks that need to be managed, which may be of interest to those clients\nseeking to explore AI in their modernization strategies."}
{"input":"According to China\u2019s \u2018New Generation Artificial Intelligence Development Plan\u2019 (2017), what are its three-step strategic objectives? ","output":"First, by 2020, the overall technology and application of AI will be in step with globally advanced levels, the AI industry will have become a new important economic growth point, and AI technology applications will have become a new way to improve people\u2019s livelihoods, strongly supporting China\u2019s entrance into the ranks of innovative nations and comprehensively achieving the struggle toward the goal of a moderately prosperous society. By 2020 China will have achieved important progress in a new generation of AI theories and technologies. It will have actualized important progress in big data intelligence, cross-medium intelligence, swarm intelligence, hybrid enhanced intelligence, and autonomous intelligence systems, and will have achieved important progress in other foundational theories and core technologies; the country will have achieved iconic advances in AI models and methods, core devices, high-end equipment, and foundational software. The AI industry\u2019s competitiveness will have entered the first echelon internationally. China will have established initial AI technology standards, service systems, and industrial ecological system chains. It will have cultivated a number of the world\u2019s leading AI backbone enterprises, with the scale of AI\u2019s core industry exceeding 150 billion RMB,  and exceeding 1 trillion RMB as driven by the scale of related industries. The AI development environment will be further optimized, opening up new applications in important domains, gathering a number of high-level personnel and innovation teams, and initially establishing AI ethical norms, policies, and regulations in some areas.\nSecond, by 2025, China will achieve major breakthroughs in basic theories for AI, such that some technologies and applications achieve a world-leading level and AI becomes the main driving force for China\u2019s industrial upgrading and economic transformation, while intelligent social construction has made positive progress. By 2025, a new generation of AI theory and technology system will be initially established, as AI with autonomous learning ability achieves breakthroughs in many areas to obtain leading research results. The AI industry will enter the global high-end value chain. This new-generation AI will be widely used in intelligent manufacturing, intelligent medicine, intelligent city, intelligent agriculture, national defense construction, and other fields, while the scale of AI\u2019s core industry will be more than 400 billion RMB, and the scale of related industries will exceed 5 trillion RMB. By 2025 China will have seen the initial establishment of AI laws and regulations, ethical norms and policy systems, and the formation of AI security assessment and control capabilities.\nThird, by 2030, China\u2019s AI theories, technologies, and applications should achieve world-leading levels, making China the world\u2019s primary AI innovation center, achieving visible results in intelligent economy and intelligent society applications, and laying an important foundation for becoming a leading innovation-style nation and an economic power. China will have formed a more mature new-generation AI theory and technology system. The country will achieve major breakthroughs in brain-inspired intelligence, autonomous intelligence, hybrid intelligence, swarm intelligence, and other areas, having important impact in the domain of international AI research and occupying the commanding heights of AI technology. AI industry competitiveness will reach the world-leading level. AI should be expansively deepened and greatly expanded into production and livelihood, social governance, national defense construction, and in all aspects of applications, will become an expansive core technology for key systems, support platforms, and the intelligent application of a complete industrial chain and high-end industrial clusters, with AI core industry scale exceeding 1 trillion RMB, and with the scale of related industries exceeding 10 trillion RMB. China will have established several world-leading AI technology innovation and personnel training centers (or bases) and will have constructed more comprehensive AI laws and regulations, and an ethical norms and policy system.\n","context":"2.1 The New generation artificial intelligence development plan (AIDP)\nReleased in July 2017 by the State Council (which is the chief administrative body within China), the \u2018New Generation Artificial Intelligence Development Plan\u2019 (AIDP) acts as a unified document that outlines China\u2019s AI policy objectives. Chinese media have referred to it as \u2018year one of China\u2019s AI development strategy\u2019 (\u201cChina AI Development Report\u201d 2018, p. 63). The overarching aim of the policy, as articulated by the AIDP, is to make China the world centre of AI innovation by 2030, and make AI \u2018the main driving force for China\u2019s industrial upgrading and economic transformation\u2019 (AIDP 2017). The AIDP also indicates the importance of using AI in a broader range of sectors, including defence and social welfare, and focuses on the need to develop standards and ethical norms for the use of AI. Altogether, the Plan provides a comprehensive AI strategy and challenges other leading powers in many key areas. 3 China\u2019s AI strategic focus A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. https:\/\/www.investopedia.com\/terms\/a\/abu-dhabi-investment-council.asp\nChina State Council. 2017. \u201cA Next Generation Artificial Intelligence Development Plan.\u201d\n(G. Webster, R. Creemers, P. Triolo, and E. Kania, Trans.). Stanford-New America DigiChina Project. State Council.\nhttps:\/\/www.newamerica.org\/cybersecurity-initiative\/digichina\/blog\/full-translation-chinas-new-generation-artificial-intelligence-development-plan-2017\/"}
{"input":"What are the focus tasks of China\u2019s New Generation Artificial Intelligence Development? ","output":"(1) Build open and coordinated AI science and technology innovation systems (2) Fostering a high-end, highly efficient smart economy (3) Construct a safe and convenient intelligent society (4) Strengthen military-civilian integration in the AI domain (5) Build a safe and efficient intelligent infrastructure system (6) Plan a new generation of AI major science and technology projects.","context":"A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. 3 China\u2019s AI strategic focus The AIDP provides a longitudinal perspective on China\u2019s strategic situation regarding AI, including its comparative capabilities, the opportunities offered, and the potential risks. Following a technology-first approach, it may be tempting to concentrate one\u2019s attention on the stated capabilities of AI, to gain an insight into the types of technologies in which China is investing. However, this would likely offer only a short-term perspective and would soon be out of date as technological innovation advances rapidly. Furthermore, it would do little to explain why China is seeking to develop a strong AI sector in the decades to come. To this end, it is more useful to try to understand China\u2019s strategic focus from a policy-first approach, by analysing the areas where China considers that AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the 2.1 The New generation artificial intelligence development plan (AIDP)\nReleased in July 2017 by the State Council (which is the chief administrative body within China), the \u2018New Generation Artificial Intelligence Development Plan\u2019 (AIDP) acts as a unified document that outlines China\u2019s AI policy objectives. Chinese media have referred to it as \u2018year one of China\u2019s AI development strategy\u2019 (\u201cChina AI Development Report\u201d 2018, p. 63). The overarching aim of the policy, as articulated by the AIDP, is to make China the world centre of AI innovation by 2030, and make AI \u2018the main driving force for China\u2019s industrial upgrading and economic transformation\u2019 (AIDP 2017). The AIDP also indicates the importance of using AI in a broader range of sectors, including defence and social welfare, and focuses on the need to develop standards and ethical norms for the use of AI. Altogether, the Plan provides a comprehensive AI strategy and challenges other leading powers in many key areas."}
{"input":"What intelligent infrastructure China is aiming to build in its New Generation Artificial Intelligence Development Plan?","output":"1.  Network Infrastructure. Speed up the layout of real-time collaborative AI 5G enhanced technology research and the development and application of space-oriented collaborative AI for the construction of high-precision navigation and positioning networks to strengthen the core of intelligent sensing technology research and key facilities. Develop intelligent industrial support, driving networks, etc., to study the intelligent network security architecture. Speed up the construction of integrated information network for space and earth, promoting a space-based information network, the future of the Internet, mobile communication network of the full integration.\n2. Big Data Infrastructure. Rely on a national data sharing exchange platform, open data platform and other public infrastructure. Construct governance, public services, industrial development, technology research and development, and other fields of big data information databases Support the implementation of national governance data applications. Integrate various types of social data platforms and data center resources. Create nationwide integrated service capabilities with reasonable layout and linkages.\n3. High-performance computing infrastructure. Continue to strengthen the supercomputing infrastructure, distributed computing infrastructure and cloud computing center construction. Build sustainable development of high-performance computing application for the ecological environment. Promote the next generation of supercomputer research and development and applications.\n","context":"2.1 The New generation artificial intelligence development plan (AIDP)\nReleased in July 2017 by the State Council (which is the chief administrative body within China), the \u2018New Generation Artificial Intelligence Development Plan\u2019 (AIDP) acts as a unified document that outlines China\u2019s AI policy objectives. Chinese media have referred to it as \u2018year one of China\u2019s AI development strategy\u2019 (\u201cChina AI Development Report\u201d 2018, p. 63). The overarching aim of the policy, as articulated by the AIDP, is to make China the world centre of AI innovation by 2030, and make AI \u2018the main driving force for China\u2019s industrial upgrading and economic transformation\u2019 (AIDP 2017). The AIDP also indicates the importance of using AI in a broader range of sectors, including defence and social welfare, and focuses on the need to develop standards and ethical norms for the use of AI. Altogether, the Plan provides a comprehensive AI strategy and challenges other leading powers in many key areas. https:\/\/www.investopedia.com\/terms\/a\/abu-dhabi-investment-council.asp\nChina State Council. 2017. \u201cA Next Generation Artificial Intelligence Development Plan.\u201d\n(G. Webster, R. Creemers, P. Triolo, and E. Kania, Trans.). Stanford-New America DigiChina Project. State Council.\nhttps:\/\/www.newamerica.org\/cybersecurity-initiative\/digichina\/blog\/full-translation-chinas-new-generation-artificial-intelligence-development-plan-2017\/ The vertical and iterative approach of the past few years now appears to be building toward something more ambitious. In June 2023, China\u2019s State Council\u2014the rough equivalent of the U.S. Cabinet\u2014announced that this year it would begin preparations on a draft Artificial\nIntelligence Law to be submitted to the National People\u2019s Congress, China\u2019s legislature. Details remain sparse, but Chinese scholars anticipate that the law will build on\nthe existing regulations to create a more comprehensive, horizontal piece of legislation that acts as a capstone on Chinese AI policy. Since 2013, China has published several national-level policy documents, which reflect the intention to develop and deploy AI in a variety of sectors. For example, in 2015, the State Council released guidelines on China\u2019s \u2018Internet\u2009+\u2019 action. It sought to integrate the internet into all elements of the economy and society. The document clearly stated the importance of cultivating emerging AI industries and investing in research and development. In the same year, the 10-year plan \u2018Made in China 2025\u2019 was released, with the aim to transform China into the dominant player in global high-tech manufacturing, including AI (McBride and Chatzky 2019). Another notable example is the Central Committee of the Communist Party of China\u2019s (CCP) 13th 5-year plan,Footnote2 published in March 2016. The document mentioned AI as one of the six critical areas for developing the country\u2019s emerging industries (CCP 2016), and as an important factor in stimulating economic growth. When read together, these"}
{"input":"What Is the UK\u2019s Definition of AI?","output":"In the white paper, DSIT defines AI as \u201cproducts and services that are \u2018adaptable\u2019 and \u2018autonomous.\u2019\u201d When defining AI as adaptable, the white paper aims to cover the difficulty of explaining AI logic and outcomes because the technology trains and operates based on inferring patterns and connections that aren\u2019t easily understood by humans or initially envisioned by its programmers. Autonomy describes the difficulty in assigning responsibility for an AI technology\u2019s outcomes because the technology can make decisions without human intent or control. By focusing on adaptable and autonomous products and services, the UK government hopes to future-proof its AI definition rather than focus on specific methods or technologies like machine learning or large language models (LLMs).","context":"signed by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety.\nThe rapid advancement of AI capabilities means that policy implications are continuously shifting. \nSome stakeholders have called for further Government action.\n* The Kings Speech announced an Automated Vehicles Bill that seeks to introduce new legal \nframeworks to support safe commercial developments of self-driving vehicles.16\n\u2020 An exascale supercomputer is a computer that can perform 1018 (a quintillion) operations per \nsecond.\n2                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nTable: Definitions of AI and related concepts\nDefinitions are not universally agreed, are continuously evolving, and are linked.\nTerm                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground Sec. 3.  Definitions.  For purposes of this order:\n\n     (a)  The term \u201cagency\u201d means each agency described in 44 U.S.C. 3502(1), except for the independent regulatory agencies described in 44 U.S.C. 3502(5).\n\n     (b)  The term \u201cartificial intelligence\u201d or \u201cAI\u201d has the meaning set forth in 15 U.S.C. 9401(3):  a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments.  Artificial intelligence systems use machine- and human-based inputs to perceive real and virtual environments; abstract such perceptions into models through analysis in an automated manner; and use model inference to formulate options for information or action.\n\n     (c)  The term \u201cAI model\u201d means a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs."}
{"input":"What Is the Current Regulatory Landscape for AI in the UK?\n","output":"Inconsistent coordination and enforcement across various regulators, including the Health and Safety Executive, Equality and Human Rights Commission, and Competition and Markets Authority, govern AI in the UK. This inconsistent coordination is why the white paper calls for system-wide coordination to clarify who is responsible for cross-cutting AI risks and to avoid duplicative requirements.\n\nAI is already covered by a few different types of laws and regulations, including the Equality Act 2010 to prevent discrimination according to protected characteristics; UK General Data Protection Regulation to process personal data fairly; product safety law; product-specific legislation for electronic equipment, medical devices, and toys; and consumer rights law to protect consumers. Other relevant laws include the Human Rights Act 1998, the Public Sector Equality Duty, Data Protection Act 2018, and sector-specific fairness requirements like the Financial Conduct Authority handbook.","context":"UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":"How's UK's approach different from that of the European Union?","output":"Unlike the European Union (EU), the UK\u2019s approach to AI will not focus on new legislation in the short term. It will instead focus on creating guidelines to empower regulators and will only take statutory action when necessary. The following explains the heart of the white paper before analyzing its strengths and weaknesses","context":"Objectives:\nThe framework is designed to achieve the following three objectives:\nDrive growth and prosperity: By reducing regulatory uncertainty and removing existing barriers to innovation, the UK government aims to allow AI companies to capitalize on early development successes and achieve long-term market advantage. There is clearly a competitive urgency in the government\u2019s proposals: \u201cBy acting now, we can give UK innovators a head start in the global race to convert the potential of AI into long term advantages for the UK, maximising the economic and social value of these technologies and strengthening our current position as a world leader in AI.\u201d\nIncrease public trust in AI: By effectively addressing risks, the UK government\u2019s goal is to remove barriers for AI products and innovation caused by a lack of trust in AI. Increase public trust in AI: By effectively addressing risks, the UK government\u2019s goal is to remove barriers for AI products and innovation caused by a lack of trust in AI.\nStrengthening the UK\u2019s position as a global leader in AI: By working with global partners, the UK government hopes to hold a crucial leadership role in shaping international governance and regulation, particularly in the development of the global AI assurance industry.\nThe government expressly excludes from the scope of the white paper issues relating to access to data, compute capability, and sustainability, as well as the balancing of the rights of content producers and AI developers. Press conference: Lead MEPs Brando Benifei (S&D, Italy) and Dragos Tudorache (Renew, Romania), the Secretary of State for digitalisation and artificial intelligence Carme Artigas, and Commissioner Thierry Breton held a joint press conference after the negotiations. The statement of Mr Benifei is available here and Mr Tudorache's here. More extracts are available here\n\nNext steps: The agreed text will now have to be formally adopted by both Parliament and Council to become EU law. Parliament\u2019s Internal Market and Civil Liberties committees will vote on the agreement in a forthcoming meeting. UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise."}
{"input":"What is the EU AI Act?","output":"The AI Act is a legal framework governing the sale and use of artificial intelligence in the EU. Its official purpose is to ensure the proper functioning of the EU single market by setting consistent standards for AI systems across EU member states. In practice, it is the first comprehensive regulation addressing the risks of artificial intelligence through a set of obligations and requirements that intend to safeguard the health, safety and fundamental rights of EU citizens and beyond, and is expected to have an outsized impact on AI governance worldwide. ","context":"Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":"What kind of AI systems does The AI Act cover?","output":"The AI Act covers AI systems that are \u201cplaced on the market, put into service or used in the EU.\u201d This means that in addition to developers and deployers in the EU, it also applies to global vendors selling or otherwise making their system or its output available to users in the EU. ","context":"equipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground\nArtificial intelligence (AI) can be found in a wide variety of everyday applications, such as in \ndeciding what users see on social media1,2 digital personal assistants,3 and recognising patterns \nin images for medical diagnosis.4\u20137\nAI technology is described in Artificial intelligence: An explainer. This briefing focuses on \npolicy aspects.\nAI systems are increasingly being used in the public and private sector for decision- making.8 Some \ncurrent and potential future examples of automated decision-making include:\n\u2022  assisting with managing workers, such as allocating work, monitoring performance, and \ndetermining pay9\n\u2022  aiding with local and national public sector decisions, such as social housing allocation,10 \nbenefit claims, and other issues10\u201315\n\u2022  self-driving cars*16,17 \u2022\n\nCOMPLIANCE AND RISK MANAGEMENT. AI systems are used to crossreference and reconcile terabytes of data from multiple sources to create alerts for noncompliance. For example, tax authorities can use AI to\ntrack tax filers who use duplicate profiles to avoid taxation, and social safety program may use AI to detect whether beneficiaries comply with eligibility criteria.\n\n\u2022\n\nFRAUD DETECTION, PREVENTION, AND INVESTIGATION.\n\n9\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR Artificial Intelligence (AI) seeks to make computers do                         full implications of AI without exploring the specificities\nthe sorts of things that minds can do, and various                              of the AI technology clusters concerned. It is hoped that\nfunctional or descriptive definitions exist. It is                              this primer provides an easily accessible guide to these\nimportant to note that AI has a long history, dating back                       specificities. Indeed, a black-box approach to \u201cAI\u201d is only\nto the 19th century, with many techniques dating to the                         of limited usefulness to most policy questions.\n1950s. In practice, various clusters of AI types are used\n\u2013 also in hybrid formats. signed by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety.\nThe rapid advancement of AI capabilities means that policy implications are continuously shifting. \nSome stakeholders have called for further Government action.\n* The Kings Speech announced an Automated Vehicles Bill that seeks to introduce new legal \nframeworks to support safe commercial developments of self-driving vehicles.16\n\u2020 An exascale supercomputer is a computer that can perform 1018 (a quintillion) operations per \nsecond.\n2                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nTable: Definitions of AI and related concepts\nDefinitions are not universally agreed, are continuously evolving, and are linked.\nTerm                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI"}
{"input":"Under the EU AI act, what requirements fo developers of high-risk AI systems must meet?","output":"A comprehensive set of risk management, data governance, monitoring and record-keeping practices, detailed documentation alongside transparency and human oversight obligations, and standards for accuracy, robustness and cybersecurity. High-risk AI systems must also be registered in an EU-wide public database. ","context":"Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. Shaping Europe\u2019s digital future - AI Act\n\nThe Commission is proposing the first-ever legal framework on AI, which addresses the risks of AI and positions Europe to play a leading role globally.\nThe regulatory proposal aims to provide AI developers, deployers and users with clear requirements and obligations regarding specific uses of AI. At the same time, the proposal seeks to reduce administrative and financial burdens for business, in particular small and medium-sized enterprises (SMEs).\n\nThe proposal is part of a wider AI package, which also includes the updated Coordinated Plan on AI. Together, the Regulatory framework and Coordinated Plan will guarantee the safety and fundamental rights of people and businesses when it comes to AI. And, they will strengthen uptake, investment and innovation in AI across the EU."}
{"input":"How many semistructured interviews were conducted with stakeholders and subject-matter experts from the Department of Defense and other federal agencies as part of the AI assessment?","output":"68 semistructured interviews and discussions were conducted with stakeholders and subject-matter experts from DoD and other federal agencies and advisory bodies","context":"(i)    within 180 days of the date of this order, tracking and reporting progress to the President on increasing AI capacity across the Federal Government, including submitting to the President a report and recommendations for further increasing capacity; \n\n          (ii)   identifying and circulating best practices for agencies to attract, hire, retain, train, and empower AI talent, including diversity, inclusion, and accessibility best practices, as well as to plan and budget adequately for AI workforce needs;\n\n          (iii)  coordinating, in consultation with the Director of OPM, the use of fellowship programs and agency technology-talent programs and human-capital teams to build hiring capabilities, execute hires, and place AI talent to fill staffing gaps; and\n\n          (iv)   convening a cross-agency forum for ongoing collaboration between AI professionals to share best practices and improve retention. (b)  Within 45 days of the date of this order, to coordinate rapid advances in the capacity of the Federal AI workforce, the Assistant to the President and Deputy Chief of Staff for Policy, in coordination with the Director of OSTP and the Director of OMB, and in consultation with the National Cyber Director, shall convene an AI and Technology Talent Task Force, which shall include the Director of OPM, the Director of the General Services Administration\u2019s Technology Transformation Services, a representative from the Chief Human Capital Officers Council, the Assistant to the President for Presidential Personnel, members of appropriate agency technology talent programs, a representative of the Chief Data Officer Council, and a representative of the interagency council convened under subsection 10.1(a) of this section.  The Task Force\u2019s purpose shall be to accelerate and track the hiring of AI and AI-enabling talent across the Federal Government, including through the following (ii)   As set forth in subsection 4.3(b)(i) of this section, within 180 days of the date of this order, the Secretary of Defense and the Secretary of Homeland Security shall, consistent with applicable law, each develop plans for, conduct, and complete an operational pilot project to identify, develop, test, evaluate, and deploy AI capabilities, such as large-language models, to aid in the discovery and remediation of vulnerabilities in critical United States Government software, systems, and networks. (iv)   convening a cross-agency forum for ongoing collaboration between AI professionals to share best practices and improve retention.\n\n     (c)  Within 45 days of the date of this order, to advance existing Federal technology talent programs, the United States Digital Service, Presidential Innovation Fellowship, United States Digital Corps, OPM, and technology talent programs at agencies, with support from the AI and Technology Talent Task Force described in subsection 10.2(b) of this section, as appropriate and permitted by law, shall develop and begin to implement plans to support the rapid recruitment of individuals as part of a Federal Government-wide AI talent surge to accelerate the placement of key AI and AI-enabling talent in high-priority areas and to advance agencies\u2019 data and technology strategies."}
{"input":"Can you describe the approach taken by the research team in preparing for the interviews with academia and industry experts in the AI assessment?","output":"The research team prepared generic interview protocols for broad categories of interviewees, which were later reviewed by a member of RAND's SRG to ensure alignment with the study goals. These protocols were then used to derive individual interview protocols tailored for specific experts","context":"(v)    establish an office to coordinate development of AI and other critical and emerging technologies across Department of Energy programs and the 17 National Laboratories.\n\n     (h)  Within 180 days of the date of this order, to understand AI\u2019s implications for scientific research, the President\u2019s Council of Advisors on Science and Technology shall submit to the President and make publicly available a report on the potential role of AI, especially given recent developments in AI, in research aimed at tackling major societal and global challenges.  The report shall include a discussion of issues that may hinder the effective use of AI in research and practices needed to ensure that AI is used responsibly for research. The role of public consultations and stakeholder participation to\npromote an inclusive dialogue on AI\nTo seek input on the design of their national AI policies and strategies,\ngovernments often involve a broad range of stakeholders including citizens,\ncivil society groups, private companies, research organisations and others.\nPublic consultations leverage different tools including interviews, surveys,\nonline discussion fora and events such as hearings, workshops, seminars, focus\ngroups and conferences. Based on information from national policy initiatives\ncollected at OECD.AI, the formation of expert groups and the organisation of\nworkshops and seminars are the most common types of consultations. Expert\nconsultations usually help define the issues, formulate policy objectives and, in\nsome cases, assess policy effectiveness. In addition to expert consultations,\ncountries such as Canada or Chile engage citizens to ensure that a diverse range\nof perspectives is taken into account. (ii)  The Secretary of State and the Administrator of the United States Agency for International Development, in collaboration with the Secretary of Energy and the Director of NSF, shall develop a Global AI Research Agenda to guide the objectives and implementation of AI-related research in contexts beyond United States borders.  The Agenda shall:\n\n               (A)  include principles, guidelines, priorities, and best practices aimed at ensuring the safe, responsible, beneficial, and sustainable global development and adoption of AI; and\n\n               (B)  address AI\u2019s labor-market implications across international contexts, including by recommending risk mitigations. Finnish AI strategy exhibits a mostly bottom-up\napproach, leveraging several multi-stakeholder\nconsultations to form the basis of their strategy.\nTheir initial 2017 strategy document states this call\n21\nfor multi-stakeholder consultation, which informed\nthe foundation of proposals in their 2019 strategy\ndocument which includes the eleven key actions in\nFigure 1 and some specific measures for fostering\nAI.22 The government places significant value in\n\nlearning from different actors. The representatives\nthey invited included people from the private and\npublic sectors, research institutes and employers,\nand also included employees, individual experts\nand influencers to form the basis of the steering\ngroup and its subgroups. Citizen participation was\nalso encouraged through online platforms that\nfacilitated interaction and discussion on a variety\n23\nof queries around AI."}
{"input":"How did the research team structure the insights gathered from academia and industry experts in the AI assessment report?","output":"The research team organized the insights along the six dimensions of posture assessment, highlighting recurring themes and important perspectives from both academia and industry. The appendix concludes with summaries on AI regulation, talent competition, and the JAIC","context":"(v)    establish an office to coordinate development of AI and other critical and emerging technologies across Department of Energy programs and the 17 National Laboratories.\n\n     (h)  Within 180 days of the date of this order, to understand AI\u2019s implications for scientific research, the President\u2019s Council of Advisors on Science and Technology shall submit to the President and make publicly available a report on the potential role of AI, especially given recent developments in AI, in research aimed at tackling major societal and global challenges.  The report shall include a discussion of issues that may hinder the effective use of AI in research and practices needed to ensure that AI is used responsibly for research. 24 |\nThe High-Level Expert Group on Artificial Intelligence (AI HLEG)\nResponsible entity: European Commission (EC)\nDescription: The High-Level Expert Group on Artificial Intelligence (AI HLEG)\nwas convened by the European Commission in 2016 to support the\nimplementation of the European Strategy on AI and completed its mandate in\nJuly 2020. The AI HLEG comprised representatives from academia, civil society,\nand industry that produced three outputs: the April 2019 Ethics Guidelines for\nTrustworthy Artificial Intelligence, the June 2019 Policy and Investment\nRecommendations for Trustworthy Artificial Intelligence, and the July 2020\nAssessment List for Trustworthy Artificial Intelligence (ALTAI). The European AI\nAlliance is a multi-stakeholder forum for engaging in a broad and open\ndiscussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial (ii)  The Secretary of State and the Administrator of the United States Agency for International Development, in collaboration with the Secretary of Energy and the Director of NSF, shall develop a Global AI Research Agenda to guide the objectives and implementation of AI-related research in contexts beyond United States borders.  The Agenda shall:\n\n               (A)  include principles, guidelines, priorities, and best practices aimed at ensuring the safe, responsible, beneficial, and sustainable global development and adoption of AI; and\n\n               (B)  address AI\u2019s labor-market implications across international contexts, including by recommending risk mitigations. Finnish AI strategy exhibits a mostly bottom-up\napproach, leveraging several multi-stakeholder\nconsultations to form the basis of their strategy.\nTheir initial 2017 strategy document states this call\n21\nfor multi-stakeholder consultation, which informed\nthe foundation of proposals in their 2019 strategy\ndocument which includes the eleven key actions in\nFigure 1 and some specific measures for fostering\nAI.22 The government places significant value in\n\nlearning from different actors. The representatives\nthey invited included people from the private and\npublic sectors, research institutes and employers,\nand also included employees, individual experts\nand influencers to form the basis of the steering\ngroup and its subgroups. Citizen participation was\nalso encouraged through online platforms that\nfacilitated interaction and discussion on a variety\n23\nof queries around AI."}
{"input":"What specific sectoral approaches were recommended by panelists at the Brookings event to enhance AI governance in the United States?","output":"Panelists recommended a sectoral approach to AI governance and the explicit inclusion of law enforcement in the regulatory framework to address inequality and ensure accountability","context":"The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023"}
{"input":"How might the exclusion of law enforcement from the Blueprint for an AI Bill of Rights impact the overall effectiveness of AI governance in the United States?","output":"Excluding law enforcement from the Blueprint may perpetuate oversurveillance of certain populations and hinder efforts to address discrimination, especially in areas like facial recognition technology that can disproportionately impact people of color ","context":"Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. Developing guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination\nSome top takeaways from the Executive Order, according to the MIT Technology Review's Tate Ryan-Mosley and Melissa Heikkil\u00e4, include requiring the Department of Commerce to develop guidance for labeling AI-generated content, such as watermarking tools; compelling developers to share safety test results for new AI models that may pose a national security risk; and forcing AI companies to be more transparent about how their new models work, among other \u201csafety and security\u201d priorities. (d)  Artificial Intelligence policies must be consistent with my Administration\u2019s dedication to advancing equity and civil rights.  My Administration cannot \u2014 and will not \u2014 tolerate the use of AI to disadvantage those who are already too often denied equal opportunity and justice.  From hiring to housing to healthcare, we have seen what happens when AI use deepens discrimination and bias, rather than improving quality of life.  Artificial Intelligence systems deployed irresponsibly have reproduced and intensified existing inequities, caused new types of harmful discrimination, and exacerbated online and physical harms.  My Administration will build on the important steps that have already been taken \u2014 such as issuing the Blueprint for an AI Bill of Rights, the AI Risk Management Framework, and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies (A)  identify areas where AI can enhance law enforcement efficiency and accuracy, consistent with protections for privacy, civil rights, and civil liberties; and\n\n               (B)  recommend best practices for law enforcement agencies, including safeguards and appropriate use limits for AI, to address the concerns set forth in section 13(e)(i) of Executive Order 14074 as well as the best practices and the guidelines set forth in section 13(e)(iii) of Executive Order 14074; and  \n\n          (iii)  supplement the report set forth in subsection 7.1(b)(i) of this section as appropriate with recommendations to the President, including with respect to requests for necessary legislation.  \n\n     (c)  To advance the presence of relevant technical experts and expertise (such as machine-learning engineers, software and infrastructure engineering, data privacy experts, data scientists, and user experience researchers) among law enforcement professionals:"}
{"input":"How might revisiting existing civil rights regimes to address biases in AI systems contribute to a more equitable AI governance framework in the United States?","output":"Revisiting civil rights laws to address biases in AI systems can help prevent discrimination and disparate impacts on marginalized populations, ensuring that AI governance is aligned with principles of fairness and equality ","context":"(d)  Artificial Intelligence policies must be consistent with my Administration\u2019s dedication to advancing equity and civil rights.  My Administration cannot \u2014 and will not \u2014 tolerate the use of AI to disadvantage those who are already too often denied equal opportunity and justice.  From hiring to housing to healthcare, we have seen what happens when AI use deepens discrimination and bias, rather than improving quality of life.  Artificial Intelligence systems deployed irresponsibly have reproduced and intensified existing inequities, caused new types of harmful discrimination, and exacerbated online and physical harms.  My Administration will build on the important steps that have already been taken \u2014 such as issuing the Blueprint for an AI Bill of Rights, the AI Risk Management Framework, and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies 7.2.  Protecting Civil Rights Related to Government Benefits and Programs.  (a)  To advance equity and civil rights, consistent with the directives of Executive Order 14091, and in addition to complying with the guidance on Federal Government use of AI issued pursuant to section 10.1(b) of this order, agencies shall use their respective civil rights and civil liberties offices and authorities \u2014 as appropriate and consistent with applicable law \u2014 to prevent and address unlawful discrimination and other harms that result from uses of AI in Federal Government programs and benefits administration.  This directive does not apply to agencies\u2019 civil or criminal enforcement authorities.  Agencies shall consider opportunities to ensure that their respective civil rights and civil liberties offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these Developing guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination\nSome top takeaways from the Executive Order, according to the MIT Technology Review's Tate Ryan-Mosley and Melissa Heikkil\u00e4, include requiring the Department of Commerce to develop guidance for labeling AI-generated content, such as watermarking tools; compelling developers to share safety test results for new AI models that may pose a national security risk; and forcing AI companies to be more transparent about how their new models work, among other \u201csafety and security\u201d priorities. Sec. 7.  Advancing Equity and Civil Rights.\n\n     7.1.  Strengthening AI and Civil Rights in the Criminal Justice System.  (a)  To address unlawful discrimination and other harms that may be exacerbated by AI, the Attorney General shall:\n\n          (i)    consistent with Executive Order 12250 of November 2, 1980 (Leadership and Coordination of Nondiscrimination Laws), Executive Order 14091, and 28 C.F.R. 0.50-51, coordinate with and support agencies in their implementation and enforcement of existing Federal laws to address civil rights and civil liberties violations and discrimination related to AI;"}
{"input":"What are the basic principles set forth by the Japanese government for AI regulation in 2019?","output":"In 2019, the Japanese government published the Social Principles of Human-Centric AI, which include three basic philosophies: human dignity, diversity and inclusion, and sustainability. These principles aim not to restrict AI use but to realize these values through AI, aligning with the OECD's AI Principles for inclusive growth, sustainable development, and well-being.","context":"So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. 2 AI legislation in China Governments should look beyond traditional regulatory techniques to influence AI developments \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":"How does Japan's approach to AI regulation differ from that of the European Commission and Canada?","output":"Japan focuses on a risk-based, agile, and multistakeholder process without imposing a one-size-fits-all obligation or prohibition, contrasting with the European Commission's draft Artificial Intelligence Act, which prescribes specific obligations for AI according to four levels, and Canada's Artificial Intelligence and Data Act (AIDA), which mandates risk management and information disclosure for high-impact AI systems.","context":"Chinese policy actors have even described the first-mover nature of their regulations as an added difficulty. When China began work on these regulations, the debates on the EU\u2019s AI Act were well underway in Europe, and Chinese policy analysts hoped that they could follow those debates and learn from the act. But slow progress on the AI Act meant that they had to forge ahead without the benefit of international guideposts or comparisons. For the United States, one benefit of its comparatively slow progress on AI governance is the op-portunity to learn from regulatory experiments abroad\u2014if policymakers are willing to take foreign regulations seriously. International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules. 2 AI legislation in China But in the West, China\u2019s regulations are often dismissed as irrelevant or seen purely through the lens of a geopolitical competition to write the rules for AI. \nInstead, these regulations deserve careful study on how they will affect China\u2019s AI trajectory and what they can teach policymakers around the world about regulating the technology. Even if countries fundamentally disagree on the specific content of a regulation, they can still learn from each other when it comes to the underlying structures and technical feasibility of different regulatory approaches.\nIn this series of three papers, I will attempt to reverse engineer Chinese AI governance."}
{"input":"What potential benefits does Japan see in international collaboration on AI regulation, especially among G7 countries?","output":"Japan sees international collaboration as crucial for bridging the gap between different regulatory approaches, sharing best practices and incidents, and potentially developing international standards for AI governance. Such collaboration could help harmonize goals across countries, even those with varying approaches to AI regulation, and promote a common understanding of good AI governance practices on a global scale, benefiting both AI service providers and the broader international community.","context":"International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules. AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. | 15\nincluding the Council of Europe, the EU, the Global Partnership on AI (GPAI), the\nGroup of Seven (G7), the Group of Twenty (G20), the Inter-American\nDevelopment Bank (IDB), the International Telecommunications Union (ITU),\nthe OECD, the United Nations, the United Nations Educational, Scientific and\nCultural Organization (UNESCO) and the World Bank.\nCross-border research on AI is also significant. For example, the French National\nResearch Agency, the German Research Foundation and the Japan Science and\nTechnology Agency have called for trilateral French-German-Japanese\ncollaborative research on AI over three years (2019-2021). In 2020, the United\nKingdom and the United States signed a declaration on co-operation for AI R&D\nthrough which they plan to drive technological breakthroughs, promote\nresearch collaboration and advance the development of trustworthy AI.\nMany European Union member states are also participating in European AI"}
{"input":"What is Japan's approach to AI regulation as described in the document?","output":"Japan's approach to AI regulation emphasizes maximizing AI's positive impact on society, focusing on a risk-based, agile, and multi-stakeholder process. It avoids a one-size-fits-all obligation or prohibition, preferring regulations that manage risks associated with AI while promoting the implementation of AI through regulatory reform","context":"2 AI legislation in China So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. But in the West, China\u2019s regulations are often dismissed as irrelevant or seen purely through the lens of a geopolitical competition to write the rules for AI. \nInstead, these regulations deserve careful study on how they will affect China\u2019s AI trajectory and what they can teach policymakers around the world about regulating the technology. Even if countries fundamentally disagree on the specific content of a regulation, they can still learn from each other when it comes to the underlying structures and technical feasibility of different regulatory approaches.\nIn this series of three papers, I will attempt to reverse engineer Chinese AI governance."}
{"input":"How does the European Union classify AI systems under its AI Act, and what are the implications for \"high risk\" AI systems?","output":"The European Union's AI Act classifies AI systems into four tiers of risk, with \"high risk\" AI systems requiring rigorous testing, documentation of data quality, and an accountability framework including human oversight before they are released in the market. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market\u200b","context":"Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition. Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct."}
{"input":"How does the document suggest AI regulation can balance innovation with societal risks?","output":"The document suggests that a risk-based approach to AI regulation, where higher-risk applications have proportionately greater regulation, could help balance innovation with societal risks. However, it also highlights the implementation challenges, including defining and measuring risk and ensuring that regulation does not deter innovation in areas that also have high value to society","context":"\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes This viewpoint proposes ten key parameters of successful AI regulatory design and the likely outcomes. AI raises many profound questions. Nearly all agree that AI promises both huge benefits for society and yet poses major risks. The challenge is getting the balance right between innovation and societal risks \u2013 which governments don't have a good record of achieving. The devil is in the details, making rules adaptable for a technology that is likely to change rapidly and be pervasive. The existential question of Artificial General Intelligence (AGI) is generating much debate. However, this is more distant and tough to forecast accurately. We focus here on issues like model safety, bias, transparency, privacy, security, trust, copyright, content regulation, education, and economic impacts (job loss, work force adjustment, productivity). For commercial actors, the key question is regulation\u2019s impact on innovation, and does it address society\u2019s concerns so that adoption is broad and I. The AI Regulation Debate: A Wide Spectrum of Views. The AI regulation debate has become heated and politicized, reflecting the immense hopes and fears that we have invested in AI.   \n\nHysterical fear and paranoia. On one end, leading technologist and venture capitalist Marc Andreessen  says \u201cwe have a full-blown moral panic about AI right now.\u201d He views this as \u201chysterical fear\u201d that is \u201cirrational\u201d and compares it to earlier introduction of new technologies that were similarly feared but were overblown and subsequently managed. He believes that AI is simply a computer program that is owned, controlled and used by people, and that AI does not have its own goals or its own wants. Moreover, he emphasizes, AI can be a force for tremendous good. Technological innovation will continue to outpace traditional regulation\nThe U.S. government is already considered behind on understanding AI compared with leading private developers. This is due to several factors: the relative slowness of government policymaking compared with the fast pace of technology development, government's inability to pay competitive salaries for scarce talent, and the lack of clarity on whether and how AI should be regulated at all, among others. Looking ahead, this dynamic is unlikely to change. Governments will continue playing catch-up with the private sector to understand and respond to the newest and most-capable AI developments."}
{"input":"What is the main purpose of the Government Accountability Office (GAO) report on Artificial Intelligence (AI) implementation by federal agencies?","output":"The GAO report aims to review the implementation of AI across major federal agencies, focusing on agencies' reported current and planned uses of AI, the comprehensiveness and accuracy of these reports, and the extent of compliance with federal policy and guidance on AI.","context":"(ii)    the Chief Artificial Intelligence Officers\u2019 roles, responsibilities, seniority, position, and reporting structures;\n\n          (iii)   for the agencies identified in 31 U.S.C. 901(b), the creation of internal Artificial Intelligence Governance Boards, or other appropriate mechanisms, at each agency within 60 days of the issuance of the guidance to coordinate and govern AI issues through relevant senior leaders from across the agency; (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these objectives, agencies shall also consider opportunities to increase coordination, communication, and engagement about AI as appropriate with community-based organizations; civil-rights and civil-liberties organizations; academic institutions; industry; State, local, Tribal, and territorial governments; and other stakeholders."}
{"input":"How many AI use cases did federal agencies report, and what stage are most of these cases in?","output":"Twenty of the twenty-three agencies reviewed reported about 1,200 AI use cases, with a significant portion of these cases being in the planning stage rather than in production.","context":"(e)  To improve transparency for agencies\u2019 use of AI, the Director of OMB shall, on an annual basis, issue instructions to agencies for the collection, reporting, and publication of agency AI use cases, pursuant to section 7225(a) of the Advancing American AI Act.  Through these instructions, the Director shall, as appropriate, expand agencies\u2019 reporting on how they are managing risks from their AI use cases and update or replace the guidance originally established in section 5 of Executive Order 13960.\n\n     (f)  To advance the responsible and secure use of generative AI in the Federal Government: (vii)   requirements that agencies identified in 31 U.S.C. 901(b) develop AI strategies and pursue high-impact AI use cases;\n\n          (viii)  in consultation with the Secretary of Commerce, the Secretary of Homeland Security, and the heads of other appropriate agencies as determined by the Director of OMB, recommendations to agencies regarding:\n\n               (A)  external testing for AI, including AI red-teaming for generative AI, to be developed in coordination with the Cybersecurity and Infrastructure Security Agency;\n\n               (B)  testing and safeguards against discriminatory, misleading, inflammatory, unsafe, or deceptive outputs, as well as against producing child sexual abuse material and against producing non-consensual intimate imagery of real individuals (including intimate digital depictions of the body or body parts of an identifiable individual), for generative AI; (iii)  consider providing, as appropriate and consistent with applicable law, guidance, technical assistance, and training to State, local, Tribal, and territorial investigators and prosecutors on best practices for investigating and prosecuting civil rights violations and discrimination related to automated systems, including AI.\n\n     (b)  To promote the equitable treatment of individuals and adhere to the Federal Government\u2019s fundamental obligation to ensure fair and impartial justice for all, with respect to the use of AI in the criminal justice system, the Attorney General shall, in consultation with the Secretary of Homeland Security and the Director of OSTP:\n\n          (i)    within 365 days of the date of this order, submit to the President a report that addresses the use of AI in the criminal justice system, including any use in:\n\n               (A)  sentencing;\n\n               (B)  parole, supervised release, and probation; (g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is"}
{"input":"What recommendations did the GAO make to improve AI implementation in federal agencies?","output":"The GAO made 35 recommendations to 19 agencies, including updating AI use case inventories to align with guidance, implementing AI requirements with government-wide implications, and fully implementing AI requirements in federal law, policy, and guidance.","context":"(g)  It is important to manage the risks from the Federal Government\u2019s own use of AI and increase its internal capacity to regulate, govern, and support responsible use of AI to deliver better results for Americans.  These efforts start with people, our Nation\u2019s greatest asset.  My Administration will take steps to attract, retain, and develop public service-oriented AI professionals, including from underserved communities, across disciplines \u2014 including technology, policy, managerial, procurement, regulatory, ethical, governance, and legal fields \u2014 and ease AI professionals\u2019 path into the Federal Government to help harness and govern AI.  The Federal Government will work to ensure that all members of its workforce receive adequate training to understand the benefits, risks, and limitations of AI for their job functions, and to modernize Federal Government information technology infrastructure, remove bureaucratic obstacles, and ensure that safe and rights-respecting AI is offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these objectives, agencies shall also consider opportunities to increase coordination, communication, and engagement about AI as appropriate with community-based organizations; civil-rights and civil-liberties organizations; academic institutions; industry; State, local, Tribal, and territorial governments; and other stakeholders. (ix)    requirements for public reporting on compliance with this guidance.\n\n     (c)  To track agencies\u2019 AI progress, within 60 days of the issuance of the guidance established in subsection 10.1(b) of this section and updated periodically thereafter, the Director of OMB shall develop a method for agencies to track and assess their ability to adopt AI into their programs and operations, manage its risks, and comply with Federal policy on AI.  This method should draw on existing related efforts as appropriate and should address, as appropriate and consistent with applicable law, the practices, processes, and capabilities necessary for responsible AI adoption, training, and governance across, at a minimum, the areas of information technology infrastructure, data, workforce, leadership, and risk management.  \n\n     (d)  To assist agencies in implementing the guidance to be established in subsection 10.1(b) of this section: (e)  To improve transparency for agencies\u2019 use of AI, the Director of OMB shall, on an annual basis, issue instructions to agencies for the collection, reporting, and publication of agency AI use cases, pursuant to section 7225(a) of the Advancing American AI Act.  Through these instructions, the Director shall, as appropriate, expand agencies\u2019 reporting on how they are managing risks from their AI use cases and update or replace the guidance originally established in section 5 of Executive Order 13960.\n\n     (f)  To advance the responsible and secure use of generative AI in the Federal Government:"}
{"input":"What is the main goal of the AI regulatory toolbox?","output":"The AI regulatory toolbox aims to help governments identify and mitigate harms caused by algorithmic systems, providing a variety of strategies and tools for regulators to ensure compliance and address potential risks associated with AI technologies.","context":"scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes Governments should look beyond traditional regulatory techniques to influence AI developments II. Top Ten Parameters Driving Regulatory Design and Outcomes   \n\nTo help policy makers and regulators, we define 10 key parameters that must be addressed in some fashion. This does not propose solutions for each domain or issue.   \n\nTransparency, fairness, explainability, security and trust are over-arching goals. Unless these goals can be achieved, AI regulation is unlikely to succeed, and AI adoption will be stunted. Many specific issues, e.g., informing users of risks, disclosing model characteristics, model-bias, and independent model safety testing, support these broader goals."}
{"input":"What are some of the key components of the AI regulatory toolbox?","output":"Key components include expanding algorithmic transparency, performing algorithmic audits, developing AI regulatory sandboxes, leveraging the AI assurance industry, and encouraging complaints and whistleblower reports to uncover algorithmic harms.","context":"II. Top Ten Parameters Driving Regulatory Design and Outcomes   \n\nTo help policy makers and regulators, we define 10 key parameters that must be addressed in some fashion. This does not propose solutions for each domain or issue.   \n\nTransparency, fairness, explainability, security and trust are over-arching goals. Unless these goals can be achieved, AI regulation is unlikely to succeed, and AI adoption will be stunted. Many specific issues, e.g., informing users of risks, disclosing model characteristics, model-bias, and independent model safety testing, support these broader goals. scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms. China\u2019s regulations are not exclusively focused on algorithms. The registry includes requirements to disclose the sources of training data, and the draft generative AI regulation has specific requirements on the data\u2019s diversity and \u201cobjectivity.\u201d Many other requirements, such as that AI-generated content \u201creflect Socialist Core Values,\u201d are defined based on outcomes rather than technical specifics. Where regulators focus their interventions will be an important component of Chinese AI governance going forward.\nBuilding Regulatory Tools and Bureaucratic Know-How\nChina\u2019s initial forays into governing AI have built up specific regulatory tools and broader bureaucratic know-how that can be deployed in future regulations. The algorithm registry is a standardized disclosure tool that ministries can easily include in future regulations, refin-ing its requirements as needed. The information currently disclosed\u2014such as data sources \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":"How can the AI assurance industry contribute to regulatory compliance and AI system improvement?","output":"The AI assurance industry, comprising companies that specialize in monitoring, evaluation, and legal compliance of algorithmic systems, can help both in improving the performance and fairness of AI systems and ensuring they meet regulatory and legal standards.","context":"Leverage the AI Assurance Industry 4.\nthereproducibility that is necessary\nA common strand across the entire AI assurance industry is a mixed business modelthat advertises both profit-motivated improvements to algorithmic systems and betterpreparedness for regulatory and legal compliance. For instance, several AI assurancecompanies stress the value of internal monitoring, so corporate leaders canunderstand and scrutinize the function of their own algorithms, in addition tohighlighting future legal requirements. This likely a stronger sales pitch to potentialclients, especially given that most AI laws are still being drafted, rather than beingimplemented.\nAlthough this industry is distinct from governance, regulators should actively engagewith the AI assurance industry to advance democratic goals, perhaps best Although this industry is distinct from governance, regulators should actively engagewith the AI assurance industry to advance democratic goals, perhaps best\n. Regulators can issue guidance that encourages regulated companies toconsider using AI assurance tools, even possibly noting this could be interpreted as apotential signal of regulatory compliance. Further, regulators can inform and learn fromthe AI assurance industry. By communicating about specific technical functions andthe societal impacts of algorithmic systems in a regulated field, regulators can help AIassurance companies strive towards not just nominal compliance, but meaningfullybetter outcomes. For instance, regulators concerned with discrimination couldencourage relevant AI assurance companies to scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms. for detailed regulatory compliance. Othercompanies, such as Trustible, are primarily focused on documenting algorithmicsystems and their compliance with specific standards or regulations, without offeringdeveloper tools. Some are industry specific\u2014Fairplay AI focuses narrowly on fairnessand disparate impact analyses for financial institutions. Others, such as EticasConsulting and Babl AI, offer full algorithmic audits and associated complianceservices, aiming to improve fairness but also performance and safety more generally.\nLeverage the AI Assurance Industry 4.\nthereproducibility that is necessary"}
{"input":"What is the primary purpose of this Executive Order on AI development and use?","output":"The order aims to ensure the development and use of AI in a manner that is safe, secure, and trustworthy, addressing both the promise and the perils AI technology presents to society.","context":"Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups."}
{"input":"Which principles guide the governance of AI development?","output":"The order outlines principles such as ensuring AI safety and security, promoting responsible AI innovation and competition, supporting American workers in the AI era, and advancing equity and civil rights in AI applications.","context":"AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. Alongside establishing material goals, the AIDP outlines a specific desire for China to become a world leader in defining ethical norms and standards for AI. Following the release of the AIDP, the government, public bodies, and industry within China were relatively slow to develop AI ethics frameworks (Lee 2018; Hickert and Ding 2018). However, there has been a recent surge in attempts to define ethical principles. In March 2019, China\u2019s Ministry of Science and Technology established the National New Generation Artificial Intelligence Governance Expert Committee. In June 2019, this body released eight principles for the governance of AI. The principles emphasised that, above all else, AI development should begin from enhancing the common well-being of humanity. Respect for human rights, privacy and fairness were also underscored within the principles. Finally, they highlighted the importance of transparency, responsibility, collaboration, and agility to deal with new and emerging 6\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nMost advanced digital governments have issued governance frameworks,\nincluding ethical principles for the use of AI. Governance models incorporate three aspects: ethical principles, the role of a central agency, and\noperational framework.\nNational governments, including Australia, Canada, China, Japan, Singapore,\nUnited Arab Emirates, and the United States as well as international organizations including the European Commission, the Institute of Electrical and Electronics Engineers, International Organization for Standardization, United Nations, and World Economic Forum, are actively proposing governance models\nfor AI that emphasize common principles:\n\nEthical Principles\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\nPrivacy and data protection\nAccountability\nSafety and security\nTransparency and explainability\nFairness\nHuman control of technology\nProfessional responsibility\nPromotion of human values\n\n\u2022"}
{"input":"How does the Executive Order propose to ensure AI technologies are developed and used equitably and without discrimination?","output":"It calls for compliance with all federal laws related to equity and civil rights, robust technical evaluations, and engagement with affected communities to prevent AI from exacerbating discrimination and bias.","context":"The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies with all Federal laws and to promote robust technical evaluations, careful oversight, engagement with affected communities, and rigorous regulation.  It is necessary to hold those developing and deploying AI accountable to standards that protect against unlawful discrimination and abuse, including in the justice system and the Federal Government.  Only then can Americans trust AI to advance civil rights, civil liberties, equity, and justice for all. Developing guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination\nSome top takeaways from the Executive Order, according to the MIT Technology Review's Tate Ryan-Mosley and Melissa Heikkil\u00e4, include requiring the Department of Commerce to develop guidance for labeling AI-generated content, such as watermarking tools; compelling developers to share safety test results for new AI models that may pose a national security risk; and forcing AI companies to be more transparent about how their new models work, among other \u201csafety and security\u201d priorities."}
{"input":"What are main concerns in relation to AI policy?","output":"The need for AI policies to protect individual rights, enhance transparency and accountability, and prioritize human rights.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy. FUTURE RESEARCH\nAs policy and regulatory pathways for harnessing AI\nare still in their infancy, further time and research\nis needed to assess the outputs of these strategies,\nand their effectiveness in order to identify best\npractices for developing countries. Rather than\nconcluding with best practices, this article provides\nan initial review of policy and regulatory practices\ncurrently deployed in AI policymaking, illustrating\na wide range of tools and approaches that\ngovernments have taken in their attempt to harness\nAI technologies while mitigating anticipated risks.\nWhile many of the policies observed have been AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground"}
{"input":"How does the role of states and municipalities perform in AI regulation?","output":"States and municipalities are actively addressing gaps in federal AI regulation through various approaches, including setting minimum privacy standards and establishing notification requirements and task forces.","context":"Nevertheless, in the area of AI regulation there is still a relatively large space for policy debates. This is perhaps due to the relatively technical nature of policies and to the freshness of the problems. How to effectively regulate AI remains a wide-open question globally, and the political interests at play in China are not yet entrenched. Ministries and state-owned enterprises have not spent decades fighting to gain leverage or to hang onto preferential policies they have carved out. This mix of factors has made public debates over AI governance unusually lively and open. Governments should look beyond traditional regulatory techniques to influence AI developments Governments around the world are implementing foundational policies to regulateartificial intelligence (AI) and algorithmic systems more generally. While legislation isadvancing, regulators should not wait idly for legislators to act. Instead, regulatorsshould be actively learning about the algorithmic systems in their regulatory domainand evaluating those systems for compliance under existing statutory authority.\nMany regulatory agencies have started this work, including the U.S. Federal TradeCommission\u2019s (FTC)\nand Consumer Financial Protection Bureau(CFPB), new algorithmic regulators in\nand\n, and onlineplatform regulators such as the UK\u2019s\n(OFCOM) and the\n. These agencies and others havestarted to implement novel approaches and policies for AI regulation.\nOffice of Technology\nthe Netherlands\nSpain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023 So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable."}
{"input":"What principles do the OECD AI Principles, endorsed by the US among other countries, establish for AI use?","output":"They advocate for inclusive growth, human-centered values, transparency, robustness, security, safety, and accountability to ensure AI benefits people and the planet, respects human rights, and operates safely and securely.","context":"is a relatively new phenomenon. To track these initiatives, the OECD AI Policy\nObservatory (OECD.AI) comprises over 620 national AI policies from over 60\ncountries and the European Union (EU). These resources provide a baseline to\nmap countries\u2019 AI policy initiatives according to the recommendations to\ngovernments contained in the OECD AI Principles (Box 1).\nBox 1. OECD AI Principles\nThe OECD Principles on Artificial Intelligence promote AI that is innovative,\ntrustworthy and respects human rights and democratic values. They were adopted\nin May 2019 by OECD member countries when they approved the OECD Council\nRecommendation on Artificial Intelligence. The OECD AI Principles identify five\ncomplementary values-based principles for the responsible stewardship of\ntrustworthy AI:\n\u2022 \u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI.\n\nTo develop practical guidance to implement the AI Principles as mandated by the\nOECD Council, the OECD convened a multi-stakeholder and multi-disciplinary OECD\nNetwork of Experts on AI in early 2020 that is developing a report on the State of\nimplementation of the OECD AI Principles: Insights from national AI policies (OECD,\n2021[1]). The report provides good practices and lessons learned on the\nimplementation of the five recommendations to policy makers contained in the\nOECD AI Principles.\nSource: oecd.ai\/ai-principles and oecd.ai\/network-of-experts. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f16 |"}
{"input":"What are foundational AI models, and why are they important?","output":"Foundational AI models, such as large language models (LLMs) like ChatGPT4, are transformative technologies that can significantly impact work, communication, scientific research, and diplomacy. They offer opportunities for economic and social flourishing, with potential to raise global GDP and enhance productivity.","context":"Dutton, T., Barron, B., and Boskovic, G. 2018. Building an AI World: Report on National and Regional AI Strategies. Toronto: CIFAR.\nhttps:\/\/www.cifar.ca\/docs\/default-source\/ai-society\/buildinganaiworld_eng.pdf Sec. 3.  Definitions.  For purposes of this order:\n\n     (a)  The term \u201cagency\u201d means each agency described in 44 U.S.C. 3502(1), except for the independent regulatory agencies described in 44 U.S.C. 3502(5).\n\n     (b)  The term \u201cartificial intelligence\u201d or \u201cAI\u201d has the meaning set forth in 15 U.S.C. 9401(3):  a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments.  Artificial intelligence systems use machine- and human-based inputs to perceive real and virtual environments; abstract such perceptions into models through analysis in an automated manner; and use model inference to formulate options for information or action.\n\n     (c)  The term \u201cAI model\u201d means a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs. Types of AI                                                                    strong social systems and societies. And finally, self-\n                                                                               awareness machines can form representations about\nResearchers have long distinguished \u201cstrong AI\u201d from                           themselves, which will require building machines with\n\u201cweak AI\u201d. Weak or narrow AI was developed for a                               consciousness.iv\nspecific, well-defined task, and it is the dominant and\nthe only existing type of AI today. One example is                             AI in the broader sense simply means a machine that is\nApple\u2019s Siri personal assistant. On the other hand,                            intelligent in the sense that it can solve a specific\nstrong or general AI aims to be a system with general                          problem. Based on the types of algorithms and ways in Measures to Address Challenges\n\u2022 Issue and make fully transparent\nthe legal framework, AI policy,\nethical principles and operating\nframework.\n\u2022 Establish special AI\/Innovation Hub\nor government unit as a center of\nexcellence.\n\u2022 Develop whole-of-government,\ndata fabric AI architecture.\n\u2022 Ensure human oversight to\nmanage risks and safeguard\nagainst machine-invoked bias."}
{"input":"What are the risks associated with foundational AI models?","output":"These models can amplify existing risks or introduce new ones, including biases and discrimination encoded in training data, information hazards, privacy violations, and increased effectiveness of crime.","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d (a)  solicit input from the private sector, academia, civil society, and other stakeholders through a public consultation process on potential risks, benefits, other implications, and appropriate policy and regulatory approaches related to dual-use foundation models for which the model weights are widely available, including:\n\n          (i)    risks associated with actors fine-tuning dual-use foundation models for which the model weights are widely available or removing those models\u2019 safeguards;\n\n          (ii)   benefits to AI innovation and research, including research into AI safety and risk management, of dual-use foundation models for which the model weights are widely available; and\n\n          (iii)  potential voluntary, regulatory, and international mechanisms to manage the risks and maximize the benefits of dual-use foundation models for which the model weights are widely available; and The Regulatory Framework defines 4 levels of risk in AI:\npyramid showing the four levels of risk: Unacceptable risk; High-risk; limited risk, minimal or no risk\n \nAll AI systems considered a clear threat to the safety, livelihoods and rights of people will be banned, from social scoring by governments to toys using voice assistance that encourages dangerous behaviour.\n\nHigh risk:\nAI systems identified as high-risk include AI technology used in: Notes\n[1] Tai Ming Cheung and Thomas G. Mahnken, The Decisive Decade: United States-China Competition in Defense Innovation and Defense Industrial Policy in and Beyond the 2020s, Center for Strategic and Budgetary Assessments, May 22, 2023.\n[2] Yoshua Bengio, \"AI and Catastrophic Risk,\" Journal of Democracy, Vol. 34, No. 4, October 2023.\n[3] Christopher Mouton, Caleb Lucas, and Ella Guest, The Operational Risks of AI in Large Scale Biological Attacks: A Red-Team Approach, RAND Corporation, RR-A2977-1, 2023.\n[4] Will Knight, \"OpenAI's CEO Says the Age of Giant AI Models Is Already Over,\" Wired, April 17, 2023."}
{"input":"What additional measures are needed for international cooperation on foundational AI models?","output":"The article suggests that more ambitious trade policy commitments are needed to fully address the opportunities and risks of foundational AI models. This includes building on developments in international forums and standard-setting bodies, and exploring new commitments in FTAs, DEAs, and other international economic forums","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f16 | AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f22 | The UN and the International Telecommunication\nUnion (ITU) have also taken steps towards\ninternational coordination by articulating an\napproach for the UN system to support AI adoption\nin developing countries and by hosting the annual\nAI for Good conference. UN agencies also apply\na baseline of standards in countries that may lack\nregulatory regimes, which can ultimately help guide\nand shape the formulation of national AI strategies."}
{"input":"What is the main goal of the EU AI Act (AIA)?","output":"The main goal of the AIA is to establish the first comprehensive regulatory scheme for artificial intelligence within the European Union, aiming to protect EU consumers and spur AI innovation.","context":"Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. EU member countries, the objective of which is \u201cto present and gather\ninformation on all EU Member States' national AI strategies in a structured and\ncomprehensive way\u201d. It aims to help Member States compare their strategy and\nidentify areas for strengthening synergies and collaboration. The EU\u2019s effort to\nmonitor implementation develops a harmonised policy framework by assessing\neach Member State\u2019s strategy through specific policy areas: human capital, Shaping Europe\u2019s digital future - AI Act\n\nThe Commission is proposing the first-ever legal framework on AI, which addresses the risks of AI and positions Europe to play a leading role globally.\nThe regulatory proposal aims to provide AI developers, deployers and users with clear requirements and obligations regarding specific uses of AI. At the same time, the proposal seeks to reduce administrative and financial burdens for business, in particular small and medium-sized enterprises (SMEs).\n\nThe proposal is part of a wider AI package, which also includes the updated Coordinated Plan on AI. Together, the Regulatory framework and Coordinated Plan will guarantee the safety and fundamental rights of people and businesses when it comes to AI. And, they will strengthen uptake, investment and innovation in AI across the EU."}
{"input":"Will the AIA have a global impact?","output":"Yes, the AIA will have a global impact, particularly in sectors and applications where AI systems are used in regulated products. However, its influence on setting international standards for AI may be limited.","context":"to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response. Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC. AI and Geopolitics\nHow Might AI Affect the Rise and Fall of Nations?\n\nPublished Nov 3, 2023\n\nby Barry Pavel, Ivana Ke, Michael Spirtas, James Ryseff, Lea Sabbag, Gregory Smith, Keller Scholl, Domenique Lumpkin\n\nNations across the globe could see their power rise or fall depending on how they harness and manage the development of artificial intelligence (AI). Regardless of whether AI poses an existential risk to humanity, governments will need to develop new regulatory frameworks to identify, evaluate, and respond to the variety of AI-enabled challenges to come. What have we learned?\nCountries and companies will clash in new ways, and AI could become an actor, not just a factor"}
{"input":"What is the difference in the AIA's impact on AI systems used in platforms versus localized software?","output":"AI systems built into geographically dispersed platforms are more likely to be comprehensively affected by the AIA's requirements, while localized software may only selectively follow the AIA rules outside the EU. This distinction results in a varying degree of the Brussels effect","context":"60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC. 10\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nCASE SELECTION, INFORMATION GATHERING AND APPROACH TO ANALYSIS\nThe eleven countries reviewed for this study\nwere selected based on four criteria: maturity\nof a country\u2019s AI ecosystem, level of digital\ndevelopment, geographic location and level of\neconomic development. Finland and the UAE\nwere selected for more detailed analysis due to\ntheir early adopter status of AI technology, with\nboth governments ranking in the top 20 for AI\nreadiness in the year this review commenced.16 The\nfindings presented reflect information on national\nstrategies and policies that was gathered from globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d The EOAI also calls for the accelerated development of AI standards with international partners in standards organizations."}
{"input":"Why is international cooperation on AI important?","output":"International cooperation on AI is crucial due to the collaborative nature of AI research, innovation, and standardization. It maximizes the advantage of scale, focuses on responsible AI development, reduces barriers to innovation, aligns AI regulation, and addresses global challenges like climate change and pandemic preparedness","context":"our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing other harms. Although technology has often influenced geopolitics, the prospect of AI means that the technology itself could become a geopolitical actor. AI could have motives and objectives that differ considerably from those of governments and private companies. Humans' inability to comprehend how AI \"thinks\" and our limited understanding of the second- and third-order effects of our commands or requests of AI are also very troubling. Humans have enough trouble interacting with one another. It remains to be seen how we will manage our relationships with one or more AIs. on Digital Cooperation.\nhttps:\/\/www.un.org\/en\/pdfs\/DigitalCooperation-report-for%20web.pdf\nUSAID.2018. Reflecting the Past, Shaping the Future: Making AI Work for International Development. Washington, DC.\nhttps:\/\/www.usaid.gov\/sites\/default\/files\/documents\/15396\/AI-ML-in-Development.pdf\nVillani, C. and others. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy.\nParis: Conseil national du num\u00e9rique.\nVinuesa, R., Azizpour, H., Leite, I. et al. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals.\nNature Communications, 11, 233.\nhttps:\/\/doi.org\/10.1038\/s41467-019-14108-y\nWhittaker, M. and others. 2018. AI Now Report 2018. New York: AI Now Institute, New York University.\nhttps:\/\/ainowinstitute.org\/AI_Now_2018_Report.pdf\nWhite House. 2020. American Artificial Intelligence Initiative: Year One Annual Report. Office of Science and Technology Policy. (i)   lead efforts outside of military and intelligence areas to expand engagements with international allies and partners in relevant bilateral, multilateral, and multi-stakeholder fora to advance those allies\u2019 and partners\u2019 understanding of existing and planned AI-related guidance and policies of the United States, as well as to enhance international collaboration; and\n\n          (ii)  lead efforts to establish a strong international framework for managing the risks and harnessing the benefits of AI, including by encouraging international allies and partners to support voluntary commitments similar to those that United States companies have made in pursuit of these objectives and coordinating the activities directed by subsections (b), (c), (d), and (e) of this section, and to develop common regulatory and other accountability principles for foreign nations, including to manage the risk that AI systems pose."}
{"input":"What are the key areas for collaboration in international AI governance?","output":"The key areas for collaboration include regulatory policies, standard-setting, and joint research and development (R&D) projects. These areas are crucial for reducing regulatory burdens, incentivizing AI development, and increasing global market competition","context":"The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI governance that can otherwise stifle innovation and reduce opportunities for uptake and use of AI; and finding ways to broaden access globally to the computing power and data that is essential for building and training AI models. AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. International co-operation on AI\nInternational co-operation to promote the beneficial use of AI and address its\nchallenges is a priority for many countries. Co-operation for the development\nand adoption of AI and AI governance is being conducted at the bilateral,\nplurilateral, regional and international level. Moreover, many intergovernmental organisations with complementary mandates are engaged in AI\ninitiatives and projects. International co-operation on AI is taking place in fora\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021"}
{"input":"Why is cooperation on AI use in government important?","output":"Cooperation on AI use in government is important for sharing good practices and lessons on deploying AI in government, including procurement and effective oversight. It helps in adopting AI solutions in government more efficiently and responsibly","context":"Although technology has often influenced geopolitics, the prospect of AI means that the technology itself could become a geopolitical actor. AI could have motives and objectives that differ considerably from those of governments and private companies. Humans' inability to comprehend how AI \"thinks\" and our limited understanding of the second- and third-order effects of our commands or requests of AI are also very troubling. Humans have enough trouble interacting with one another. It remains to be seen how we will manage our relationships with one or more AIs. of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions that emerge from using AI in the industry are currently lacking. Thus, AI can help foster increased productivity and high levels of growth, but its use is likely to intensify the inequalities present within society and even decrease support for the government and its policies. The AIDP also promotes AI as a way to help deal with some of the major social problems, ranging from pollution to standards of living. However, positive impact in this area seem to come with increased control over individuals\u2019 behaviour, with governance extending into the realm of moral behaviour and further erosion of privacy. the behaviour of citizens and enhancing their moral integrity, which is considered a task within the government\u2019s remit (\u201cXi Jinping\u2019s report at 19th CPC National Congress\u201d 2017). In view of the government, AI can be used to this end. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society."}
{"input":"why has cooperation between China and global networks for AI R&D come under scrutiny in recent years?","output":"The reassessment is due to four factors: the growing capabilities of AI and its impacts on economic competitiveness and national security, China's unethical use of AI, the rise in Chinese capabilities and ambitions in AI, and the policies by which the Chinese state bolstered those capabilities","context":"The Department of Defense incorporates AI in weapon systems, command and control, and intelligence. Humans will stay \u201cin the loop\u201d. DoD is likely criticized for moving too slow. The intelligence community and law enforcement agencies are also likely major adopters of AI, sparking civil rights concerns. \n\nGrowing competition with China shadows the AI regulatory effort, escalating a \u201cdon\u2019t fall behind China\u201d debate. Controls on exports and investments in AI-related technologies such as advanced GPUs are likely to expand over time and broaden, e.g., cloud computing, quantum. These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability. structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response."}
{"input":"What are the concerns regarding collaboration with China on AI research?","output":"Concerns include whether cooperation is helping China overtake democratic nations in AI and to what extent technologists and companies in democratic nations are contributing to China's deployment of repressive AI tools","context":"structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory of, China\u2019s AI strategy. It emphasises that the Chinese government is aware of the potential benefits, practical risks, and the ethical challenges that AI presents, and that the direction of China\u2019s AI strategy will largely be determined by the interplay of these factors and by the extent to which government\u2019s interests may outweigh ethical concerns. Section 5 concludes the paper by summarising the key findings of our analysis. China is a central actor in the international debate on the development and governance of AI. It is important to understand China\u2019s internal needs, ambitions in the international arena, and ethical concerns, all of which are shaping the development of China\u2019s AI policies. It is also important to understand all this not just externally, from a Western perspective, but also internally, from a Chinese perspective. However, some ethical safeguards, constraints and desiderata are universal and are universally accepted and cherished, such as the nature and scope of human rights.Footnote15 They enable one to evaluate, after having understood, China\u2019s approach to the development of AI. This is why in this article we have sought to contribute to a more comprehensive and nuanced analysis of the structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the perceived risks present in each of these areas. The AIDP highlights three areas where AI can make a substantial difference within China: international competition, economic development, and social governance. They are strictly interrelated but, for the sake of clarity, we shall analyse them separately, and contextualise each of them by discussing the relevant literature surrounding the broader political backdrop and contemporary policy debates."}
{"input":"What is the proposed approach for rebalancing AI R&D with Chinese researchers and institutions?","output":"The proposed approach is a risk-based approach that requires a clear assessment of the costs and benefits, aiming to maximize the benefits of an open research environment while managing the risks presented by AI R&D with China","context":"The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. 2 AI legislation in China A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. 3.\nBy 2030, China seeks to become the world\u2019s innovation centre for AI. By then, growth in the core AI industry is expected to more than double again and be valued at 1 trillion yuan (ca 147 billion dollars), and further upgrades in the laws and standards are also to be expected, to deal with newly emerging challenges."}
{"input":"What is the main purpose of the \"A Framework for Ethical AI at the United Nations\" document?","output":"The main purpose is to provide a comprehensive framework for the ethical development and use of artificial intelligence (AI) within the United Nations (UN). It outlines the need for ethical principles, architectural standards, assessment methods, tools, methodologies, and a policy to guide and govern AI technologies in alignment with human rights and UN values.","context":"Alongside establishing material goals, the AIDP outlines a specific desire for China to become a world leader in defining ethical norms and standards for AI. Following the release of the AIDP, the government, public bodies, and industry within China were relatively slow to develop AI ethics frameworks (Lee 2018; Hickert and Ding 2018). However, there has been a recent surge in attempts to define ethical principles. In March 2019, China\u2019s Ministry of Science and Technology established the National New Generation Artificial Intelligence Governance Expert Committee. In June 2019, this body released eight principles for the governance of AI. The principles emphasised that, above all else, AI development should begin from enhancing the common well-being of humanity. Respect for human rights, privacy and fairness were also underscored within the principles. Finally, they highlighted the importance of transparency, responsibility, collaboration, and agility to deal with new and emerging Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors. 6\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nMost advanced digital governments have issued governance frameworks,\nincluding ethical principles for the use of AI. Governance models incorporate three aspects: ethical principles, the role of a central agency, and\noperational framework.\nNational governments, including Australia, Canada, China, Japan, Singapore,\nUnited Arab Emirates, and the United States as well as international organizations including the European Commission, the Institute of Electrical and Electronics Engineers, International Organization for Standardization, United Nations, and World Economic Forum, are actively proposing governance models\nfor AI that emphasize common principles:\n\nEthical Principles\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\nPrivacy and data protection\nAccountability\nSafety and security\nTransparency and explainability\nFairness\nHuman control of technology\nProfessional responsibility\nPromotion of human values\n\n\u2022 emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to establish ethical principles but did form an AI ethics committee in mid-2018 with this purpose in mind (\u201cAI association to draft ethics guidelines\u201d 2019)."}
{"input":"What are the core ethical principles recommended in the framework for AI at the UN?","output":"The core ethical principles include ensuring AI technologies align with fundamental human rights, promoting fairness, transparency, accountability, and ensuring the safety and security of AI systems. The framework emphasizes the importance of these principles in guiding AI development and use within the UN.","context":"\u201cPrincipled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI\u201d\nBerkman Klein Center Research Publication No. 2020-1\nhttps:\/\/ssrn.com\/abstract=3518482 or http:\/\/dx.doi.org\/10.2139\/ssrn.3518482\nFleming, Sean. 2018. \u201cIn Rwanda, High-Speed Drones are Delivering Blood to Remote Communities.\u201d World Economic Forum.\nhttps:\/\/www.weforum.org\/agenda\/2018\/12\/in-rwanda-drones-are-delivering-blood-to-remote-communities\/\nFloridi, Luciano, and others. 2018. \u201cAI4People\u2014An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.\u201d Minds & Machines 28 (4): 689\u2013707.\nhttps:\/\/doi.org\/10.1007\/s11023-018-9482-5 emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to establish ethical principles but did form an AI ethics committee in mid-2018 with this purpose in mind (\u201cAI association to draft ethics guidelines\u201d 2019). Alongside establishing material goals, the AIDP outlines a specific desire for China to become a world leader in defining ethical norms and standards for AI. Following the release of the AIDP, the government, public bodies, and industry within China were relatively slow to develop AI ethics frameworks (Lee 2018; Hickert and Ding 2018). However, there has been a recent surge in attempts to define ethical principles. In March 2019, China\u2019s Ministry of Science and Technology established the National New Generation Artificial Intelligence Governance Expert Committee. In June 2019, this body released eight principles for the governance of AI. The principles emphasised that, above all else, AI development should begin from enhancing the common well-being of humanity. Respect for human rights, privacy and fairness were also underscored within the principles. Finally, they highlighted the importance of transparency, responsibility, collaboration, and agility to deal with new and emerging Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors."}
{"input":"How is the framework for ethical AI at the UN intended to influence AI development and use globally?","output":"By establishing a robust and ethical framework for AI development and use, the UN aims to set a global standard for ethical AI. This includes influencing international policies, contributing to global discussions on AI ethics, and promoting the adoption of similar ethical standards and practices worldwide.","context":"Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors. Alongside establishing material goals, the AIDP outlines a specific desire for China to become a world leader in defining ethical norms and standards for AI. Following the release of the AIDP, the government, public bodies, and industry within China were relatively slow to develop AI ethics frameworks (Lee 2018; Hickert and Ding 2018). However, there has been a recent surge in attempts to define ethical principles. In March 2019, China\u2019s Ministry of Science and Technology established the National New Generation Artificial Intelligence Governance Expert Committee. In June 2019, this body released eight principles for the governance of AI. The principles emphasised that, above all else, AI development should begin from enhancing the common well-being of humanity. Respect for human rights, privacy and fairness were also underscored within the principles. Finally, they highlighted the importance of transparency, responsibility, collaboration, and agility to deal with new and emerging emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to establish ethical principles but did form an AI ethics committee in mid-2018 with this purpose in mind (\u201cAI association to draft ethics guidelines\u201d 2019). \u201cPrincipled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI\u201d\nBerkman Klein Center Research Publication No. 2020-1\nhttps:\/\/ssrn.com\/abstract=3518482 or http:\/\/dx.doi.org\/10.2139\/ssrn.3518482\nFleming, Sean. 2018. \u201cIn Rwanda, High-Speed Drones are Delivering Blood to Remote Communities.\u201d World Economic Forum.\nhttps:\/\/www.weforum.org\/agenda\/2018\/12\/in-rwanda-drones-are-delivering-blood-to-remote-communities\/\nFloridi, Luciano, and others. 2018. \u201cAI4People\u2014An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.\u201d Minds & Machines 28 (4): 689\u2013707.\nhttps:\/\/doi.org\/10.1007\/s11023-018-9482-5"}
{"input":"What is the significance of AI in the public sector?","output":"AI is crucial for transforming government operations by enhancing personalized service delivery, improving back-end efficiencies, strengthening policy compliance, and aiding fraud detection. It represents a strategic resource to boost economic growth and competitiveness.","context":"2\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. 5\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR 4\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fHow can countries address these challenges?\nAI policy incorporating legislative foundations, institutional arrangements and a\ngovernance framework for the use of AI could provide the necessary enabling\nenvironment. This could be part of a broader Digital Transformation Policy or a\nstand-alone framework.\nA central innovation hub for AI in government could help pool scarce human\nand financial resources to support the initiatives of line ministries and start to\ndevelop a whole-of-government approach. In the use cases examined here,\nmost governments have already set-up the main hub for AI issues that serves\nas a central authority over AI projects being delivered by line agencies.\nThe AI hub\u2019s role could also be to connect industry expertise to the government\ndepartments and line agencies, to promote research, and build alliances with\nacademic institutions and the private sector."}
{"input":"What are the main challenges to AI adoption in public sectors?","output":"Key challenges include lack of awareness about AI's potential, inadequate digital infrastructure, poor data quality, and a shortage of digital skills. Additionally, there's an absence of suitable policy and legal frameworks for AI.","context":"What are the main challenges?\nIn most of the World Bank\u2019s client countries, a lack of awareness of the potential of\nAI, inadequate foundational digital technologies, low availability or quality of data,\nand low access to digital skills, are the major barriers to AI adoption see Figure 1.\nAs of today, the appropriate policy and legal environment for AI do not yet exist.\nPublic administrations that lack digital processes, data collection capabilities,\ntechnical skills in the civil service and digital infrastructure are unlikely to be able\nto benefit from the application of AI in the short to medium term, and the policy\nand legal frameworks need to first be more developed.\nThat said, despite the digital divide that exists across countries in terms of\nfulfilling the prerequisites for AI adoption, several use cases are now emerging\nin our client countries and several governments have started piloting AI to\naddress their development challenges. 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. secure processing;\n\u2022 Targeting key sectors for AI adoption for economic growth and human development; and\n\u2022 Adopting AI to improve public service delivery and resource efficiency while building capability\nand expertise. \u2022\n\nIdentify and manage risks rather than avoiding them. Good algorithm impact assessment framework models exist, which can\nbe tailored to suit a country\u2019s context.\n\nGovernments and world leaders are instrumental in guiding the transition to automation and AI. They can provide leadership to influence the trajectory of AI adoption among citizens at national and international levels. This will help avoid adverse consequences\nand reap productivity gains. National governments could choose global guiding principles that will inevitably shape the acceptance\nor rejection of AI. Since AI will have a profound influence on service delivery, citizen engagement, and core operations, it is imperative to formulate a cohesive governance model that supports the process of ethical implementation.\n12\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fSupported by the GovTech Global Partnership\n\nRepublic of Korea"}
{"input":"What is the World Bank's Policy on Access to Information?","output":"The policy outlines the World Bank's commitment to transparency and accountability by allowing public access to a vast range of its documents and information, except for specified exceptions, to support informed engagement with stakeholders and the public.","context":"Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction World Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\nI. Introduction\n1. The World Bank1 recognizes that transparency and accountability are of\nfundamental importance to the development process and to achieving its mission to end\nextreme poverty and promote shared prosperity. Transparency is essential to building\nand maintaining public dialogue and increasing public awareness about the Bank\u2019s\ndevelopment role and mission. It is also critical for enhancing good governance,\naccountability, and development effectiveness. Openness promotes engagement with\nstakeholders, which, in turn, improves the design and implementation of projects and public only after the country (or other third-party) authority concerned has\ngranted its written consent to disclose.\n21. Documents Prepared Jointly with Partners. The Bank makes available to the\npublic the final documents that it prepares jointly with partners after reaching agreement\non such disclosure with the partner concerned.20\nIV. Implementation Aspects of the Policy\nA. Accessing Information\n22. Proactive Disclosure. The Bank routinely discloses a wide range of documents,\nthrough its external website \u2013 www.worldbank.org \u2013 as soon as the documents are\nfinalized after key process milestones. Under this Policy, the Bank significantly\nincreases the amount of information it makes available to the public, particularly\ninformation related to projects under implementation and to the actions of the Board of\nExecutive Directors.\nunder IBRD Loans and IDA Credits, and Guidelines: Selection and Employment of Consultants by\nWorld Bank Borrowers). IV.\nA.\n\nImplementation Aspects of the Policy\n\nAccessing Information\n\n22.\nProactive Disclosure. The Bank routinely discloses a wide range of documents,\nthrough its external website \u2013 www.worldbank.org \u2013 as soon as the documents are\nfinalized after key process milestones. Under this Policy, the Bank significantly\nincreases the amount of information it makes available to the public, particularly\ninformation related to projects under implementation and to the actions of the Board of\nExecutive Directors.\n\n19\n\n20"}
{"input":"Why does the World Bank restrict access to certain types of information?","output":"The policy includes exceptions to protect sensitive information that could harm individuals, financial stability, or the confidentiality of the Bank's clients and partners.","context":"Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction Go Top\n\nWhy does the AI Policy restrict some types of information from disclosure?\n\nThe AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the\npublic and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff and other\nparties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure\ncould cause harm to well defined interests.\n\n                                                                                                                    Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information? 31.\nThe Bank declassifies and discloses\u2014routinely on the Bank\u2019s external website,\nand in response to requests\u2014certain types of restricted information (including\ninformation prepared under earlier disclosure policies of the Bank) as their sensitivity\ndiminishes over time. Some restricted information is not eligible for declassification.\n32.\nInformation Not Eligible for Declassification. The information that is restricted\nfrom disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member World Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\nI. Introduction\n1. The World Bank1 recognizes that transparency and accountability are of\nfundamental importance to the development process and to achieving its mission to end\nextreme poverty and promote shared prosperity. Transparency is essential to building\nand maintaining public dialogue and increasing public awareness about the Bank\u2019s\ndevelopment role and mission. It is also critical for enhancing good governance,\naccountability, and development effectiveness. Openness promotes engagement with\nstakeholders, which, in turn, improves the design and implementation of projects and"}
{"input":"How can someone request information from the World Bank?","output":"Individuals can request information through the World Bank's website, by email, mail, or fax, or by visiting the InfoShop in Washington, D.C., or any Public Information Center.","context":"(d) Board papers distributed to the Executive Directors for information are posted\nupon distribution.\n24.\nInformation on Request. Information that is disclosable under this policy and is\nnot on the Bank\u2019s external website is available on request (subject to paragraph 26). Such\nrequests may be submitted in writing by electronic means, mail, or fax. Requests should\nindicate, with reasonable specificity, the information that is being sought, to enable the\nBank to locate the information within a reasonable period of time. If a particular\ndocument is required, it should be identified precisely, preferably by date and title.\nElectronic submissions should be directed through the World Bank website:\nwww.worldbank.org\/wbaccess. Requests may also be made in person at the InfoShop in\nWashington, DC, or the Public Information Centers located in member countries.\n25.\nTimelines for Responding to Requests. The Bank acknowledges receipt of The Bank also makes available to the public the final documents that it prepares jointly with partners after all parties\nhave agreed to disclose them. Board papers and Board records are available in this link after disclosure.\n\n                                                                                                                 Go Top\n\nREQUESTING INFORMATION TO THE WORLD BANK\n\n\n\nCan I contact a Bank staff directly for Bank's information?\n\nNo. Publicly available information can be accessed through the World Bank's website at\nwww.worldbank.org\/documents, www.worldbank.org\/projects and www.worldbank.org\/topics. For information that is\nnot readily available in the World Bank's website, you can submit your request through the online portal at\nwww.worldbank.org\/wbaccess.\n\n                                                                                                                 Go Top\n\nWhat does it cost to get information from the Bank? Bank to locate the information within a reasonable period of time. If a particular\ndocument is required, it should be identified precisely, preferably by date and title.\nElectronic submissions should be directed through the World Bank website:\nwww.worldbank.org\/wbaccess. Requests may also be made in person at the InfoShop in\nWashington, DC, or the Public Information Centers located in member countries.\n25. Timelines for Responding to Requests. The Bank acknowledges receipt of\nwritten requests for information within 5 working days, and endeavors to provide a more\ncomprehensive response within 20 working days. Additional time may be needed in\nspecial circumstances, including, for example, those involving complex or voluminous\nrequests, or requests requiring review by or consultations with internal Bank units,\nexternal parties, the Access to Information Committee, or the Board.\n26. Unreasonable or Unsupported Requests. The Bank reserves the right to refuse Go Top\n\nWhat does it cost to get information from the Bank?\n\nMost operational information is routinely posted on the Bank's external website and can be downloaded free of charge.\nFor any request for information that is not routinely posted, the Bank may charge reasonable fees for providing digital\nor hard copies, particularly for requests that are complex or time-consuming.\n\nPublications, some specialized databases, and other knowledge products (including subscription-based services) may\nbe purchased through the Bank's online bookstore (http:\/\/publications.worldbank.org). . The full text of World Bank\nresearch and books published by the Bank may be viewed and downloaded free of charge from the Open Knowledge\nRepository (http:\/\/openknowledge.worldbank.org)."}
{"input":"What are the challenges in AI regulation?","output":"Former Google CEO Eric Schmidt emphasized the challenge of defining what society wants to gain from AI and the need for a balance between regulating AI and investing in innovation.","context":"\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground Why do we need rules on AI?\nThe proposed AI regulation ensures that Europeans can trust what AI has to offer. While most AI systems pose limited to no risk and can contribute to solving many societal challenges, certain AI systems create risks that we must address to avoid undesirable outcomes.\n\nFor example, it is often not possible to find out why an AI system has made a decision or prediction and taken a particular action. So, it may become difficult to assess whether someone has been unfairly disadvantaged, such as in a hiring decision or in an application for a public benefit scheme.\n\nAlthough existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring."}
{"input":"What are some of the benefits of AI in healthcare?","output":"The industry report highlights benefits such as enhanced diagnostic accuracy, optimized treatment plans, revolutionized preventative regimens, and overall improvements in health and wellness through the use of data analytics, artificial intelligence, and digital technologies.","context":"(A)  development, maintenance, and use of predictive and generative AI-enabled technologies in healthcare delivery and financing \u2014 including quality measurement, performance improvement, program integrity, benefits administration, and patient experience \u2014 taking into account considerations such as appropriate human oversight of the application of AI-generated output;\n\n               (B)  long-term safety and real-world performance monitoring of AI-enabled technologies in the health and human services sector, including clinically relevant or significant modifications and performance across population groups, with a means to communicate product updates to regulators, developers, and users; * The Trades Union Congress established a taskforce to write a draft AI and Employment Bill to be \npublished in early 2024. The taskforce is advised by an expert committee consisting of technology \nindustry group TechUK, the University of Oxford, the British Computer Society, trade unions, think \ntanks, and cross-party Parliamentarians.50\n5                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nHowever, the expanding use of digital and AI technologies in healthcare may create barriers for \ndigitally excluded communities, such as the elderly, in accessing healthcare.75\nWhilst deployment is not yet widespread,76 potential benefits from generative AI in education could \ninclude:\n\u2022  bespoke educational courses that adapt to different learning styles\n\u2022  reducing teachers\u2019 administrative work\n\u2022  assisting teachers with feedback and marking77 Countries tend to prioritise a handful of economic sectors, including mobility\n(such as logistics and transportation), energy, health, and agriculture (Figure 2).\nIn mobility, AI applications can help governments improve road safety, enhance\npublic transportation efficiency, manage traffic and reduce carbon emissions.\nIn health care, AI can help governments harness the latest breakthroughs to\nhelp detect health conditions early or remotely. They can also help deliver\npreventative services, optimise clinical decision-making and discover new\ntreatments and medications (OECD, 2020[4]). AI tools and techniques have also\nbeen used to help policy makers and the medical community accelerate\nresearch and treatments for the COVID-19 virus by rapidly analysing large\nvolumes of research data (OECD, 2020[5]).\nAI can also be leveraged by governments to innovate and transform the public\nsector. AI promises to make government services \u201csmarter\u201d: more agile, equipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground\nArtificial intelligence (AI) can be found in a wide variety of everyday applications, such as in \ndeciding what users see on social media1,2 digital personal assistants,3 and recognising patterns \nin images for medical diagnosis.4\u20137\nAI technology is described in Artificial intelligence: An explainer. This briefing focuses on \npolicy aspects.\nAI systems are increasingly being used in the public and private sector for decision- making.8 Some \ncurrent and potential future examples of automated decision-making include:\n\u2022  assisting with managing workers, such as allocating work, monitoring performance, and \ndetermining pay9\n\u2022  aiding with local and national public sector decisions, such as social housing allocation,10 \nbenefit claims, and other issues10\u201315\n\u2022  self-driving cars*16,17"}
{"input":"What specific examples are given to illustrate the impact of digital health technologies?","output":"The industry report provides examples like remote patient monitoring, the use of wearable technologies for real-time health data tracking, and digital health records contributing to better patient care and efficiency in the healthcare system.","context":"worldbank.org\/digitaldevelopment\n34\n\n\fANALYTICAL INSIGHTS - NOTE 4 worldbank.org\/digitaldevelopment\n37\n\n\fANALYTICAL INSIGHTS - NOTE 4 worldbank.org\/digitaldevelopment\n45\n\n\fANALYTICAL INSIGHTS - NOTE 4 worldbank.org\/digitaldevelopment\n38\n\n\fANALYTICAL INSIGHTS - NOTE 4"}
{"input":"What is the predicted impact of generative AI on jobs?","output":"LinkedIn's analysis predicts that the jobs of 55% of the platform\u2019s users will be impacted in some way by the adoption of generative AI.","context":"3                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nBenefits and risks\nJob creation and displacement\nIn 2022, a report commissioned for the Department for Science, Innovation & Technology stated that \ninvestment in the UK AI sector grew five-fold between 2019 and 2021, and that it brought in \u00a310.6 \nbillion in revenue and employed over 50,000 people.28\nOECD analysis published in July 2023 found that AI was changing the nature of work by assisting \nworkers and reducing the time spent on mundane tasks, rather than causing job losses.29\nHowever, some emerging academic research indicates that developments in generative AI may be linked \nto a loss in the quantity and earnings of white-collar jobs.30,31\nThere is potential for new jobs across all sectors to be created with improved productivity and \nglobal economic growth (PB 57).32 However, some reports state that certain jobs, such as clerical 26. The Alan Turing Institute (online). Data science and AI glossary.\n27. Cabinet Office et al. (2021). Ethics, Transparency and Accountability Framework for Automated \nDecision-Making. GOV.UK.\n28. Department for Science, Innovation and Technology (2023). Artificial Intelligence sector study \n2022. GOV.UK.\n29. Green, A. (2023). Artificial intelligence and jobs: No signs of slowing labour demand (yet). \nOECD.\n30. Hui, X. et al. (2023). The Short- Term Effects of Generative Artificial Intelligence on \nEmployment: Evidence from an Online Labor Market. Ludwigs- Maximilians University\u2019s Center for \nEconomic Studies and the ifo Institute.\n31. Burn-Murdoch, J. (2023). Here\u2019s what we know about generative AI\u2019s impact on white-collar work. \nFinancial Times.\n32. Hayton, J. et al. (2023). Adoption of AI in UK firms - and the consequences for jobs. Institute \nfor the Future of Work.\n33. Briggs, J. et al. (2023). The Potentially Large Effects of Artificial Intelligence on Economic for the Future of Work.\n33. Briggs, J. et al. (2023). The Potentially Large Effects of Artificial Intelligence on Economic \nGrowth. Goldman Sachs.\n34. Department for Science, Innovation and Technology (2021). The potential impact of AI on UK \nemployment and the demand for skills. GOV.UK.\n35. Pawel Gmyrek, J. B. (2023). Generative AI and Jobs: A global analysis of potential effects on \njob quantity and quality. International Labour Organization.\n36. Ellingrud, K. et al. (2023). Generative AI and the future of work in America. McKinsey.\n37. Young, E. et al. (2023). Mind the gender gap: Inequalities in the\nemergent professions of artificial intelligence (AI) and data science. New Technol. Work Employ., \nVol 38, 391\u2013414.\n38. Hayton, J. et al. (2023). Briefing Paper: What drives UK firms to\nadopt AI and robotics, and what are the consequences for jobs? Zenodo.\n39. Wall, J. et al. (2023). Making AI work for Britain. techUK. There is potential for new jobs across all sectors to be created with improved productivity and \nglobal economic growth (PB 57).32 However, some reports state that certain jobs, such as clerical \nwork,33,34 could become redundant.\nStakeholders have raised concerns that AI developments may disproportionately affect disadvantaged \ngroups. For example, the majority of clerical work is carried out by women.34\u201337\nA 2021 report commissioned by the former Department for Business, Energy and Industrial Strategy \nhighlighted a regional disparity in the net employment impacts of AI, with London and the South \nEast benefitting from net job gains more than Northern England and the Midlands.34\nSome academics and think tanks,38 and technology trade associations,39 have said that the \nGovernment should help workers retrain and gain relevant skills, and ensure that existing \ninequalities are not exacerbated (PN 697).\nIn October 2023, the Government announced \u00a3118m to increase the UK\u2019s AI skills"}
{"input":"What are some of the critical socioeconomic determinations impacted by Algorithmic Decision-Making Systems (ADSs)?","output":"ADSs have significant implications in applications such as educational access, job discovery and hiring, employee management, consumer financial services, property appraisal, rent setting, tenant screening, medical provisioning, and medication approval.","context":"* An example of this was a shopping algorithm at US supermarket Target which could guess, based on \ninsights from customers\u2019 shopping data, if a customer was pregnant. The algorithm would then send \nvouchers for pregnancy-related items, which in one case alerted a father to his daughter\u2019s teenage \npregnancy.251\n12                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nCompanies such as OpenAI and Anthropic have argued against making AI code public, raising safety \nconcerns around potential uses for malicious purposes.258\u2013261 Other companies (such as Meta and \nMozilla) have voluntarily shared some of their underlying code.262\nAlgorithmic impact assessment and audits\nSome learned societies and academics have said that a duty to carry out impact assessments of \nautomated decisions (algorithmic impact assessments) could be placed on companies and public \nbodies.263\u2013268 (C)  identify instances in which reliance on automated or algorithmic systems would require notification by the State, local, Tribal, or territorial government to the Secretary;\n\n               (D)  identify instances when applicants and participants can appeal benefit determinations to a human reviewer for reconsideration and can receive other customer support from a human being;\n\n               (E)  enable auditing and, if necessary, remediation of the logic used to arrive at an individual decision or determination to facilitate the evaluation of appeals; and\n\n               (F)  enable the analysis of whether algorithmic systems in use by benefit programs achieve equitable outcomes. far more likely (https:\/\/www.brookings.edu\/articles\/auditing-employmentalgorithms-\nfor-discrimination\/)\nto uncover flawed and harmful aspects of analgorithmic system. However, more intensive algorithmic audits are also far moretechnically complex, requiring more expertise and technical capacity from regulators.Specifically, regulators would need data scientists with expertise in evaluatingalgorithmic systems and may need to take necessary steps to develop a computingenvironment for algorithmic evaluation with appropriate privacy and cybersecuritysafeguards.\nscientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm."}
{"input":"What is an argument against a centralized approach to setting algorithmic standards and enforcement for federal agencies? ","output":"Because each ADS can have unique applications, it creates a need for federal agencies to have flexible authority to adapt to the specific considerations of impactful ADSs in their respective domains.","context":"Governments around the world are implementing foundational policies to regulateartificial intelligence (AI) and algorithmic systems more generally. While legislation isadvancing, regulators should not wait idly for legislators to act. Instead, regulatorsshould be actively learning about the algorithmic systems in their regulatory domainand evaluating those systems for compliance under existing statutory authority.\nMany regulatory agencies have started this work, including the U.S. Federal TradeCommission\u2019s (FTC)\nand Consumer Financial Protection Bureau(CFPB), new algorithmic regulators in\nand\n, and onlineplatform regulators such as the UK\u2019s\n(OFCOM) and the\n. These agencies and others havestarted to implement novel approaches and policies for AI regulation.\nOffice of Technology\nthe Netherlands\nSpain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023 Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm. , which is especially helpful tounderstand algorithms as part of\n. Some governments,including the EU through the Digital Services Act, are even requiring access toplatform data for independent researchers\u2014this research is expected to informregulatory investigations and\neven enforcement actions\n(https:\/\/www.brookings.edu\/articles\/platform-data-access-is-a-lynchpin-of-the-eusdigital-\nservices-act\/)\n. In fact, regulators may turn to existing academic research first,even to prioritize what other information gathering tools\u2014like those discussed here\u2014to employ.\nWhile algorithmic systems have become widely used in many regulated markets, thesealgorithms are\nunique to their circumstances (https:\/\/www.brookings.edu\/articles\/acomprehensive-\nand-distributed-approach-to-ai-regulation\/) Future of Work.\n265. Groves, L. (2022). Algorithmic impact assessment: a case study in healthcare. Ada Lovelace \nInstitute.\n266. Kaminski, M. E. et al. (2021). Algorithmic impact assessments under the GDPR: producing multi- \nlayered explanations. Int. Data Priv. Law, Vol 11, 125\u2013144.\n267. Selbst, A. D. (2021). An\nInstitutional View Of Algorithmic Impact Assessments. Harv. J. Law Technol., Vol 35, 117\u2013191.\n268. Reisman, D. et al. (2018). Algorithmic Impact Assessments: A Practical Framework for Public \nAgency Accountability. AI Now Institute.\n269. Government of Canada (2023). Algorithmic Impact Assessment Tool.\n270. Central Digital & Data Office et al. (2023). Algorithmic Transparency Recording Standard Hub. \nGOV.UK.\n271. Information Commissioner\u2019s Office (online). A Guide to ICO Audit Artificial Intelligence (AI) \nAudits. Information\nCommissioner\u2019s Office.\n272. Digital Regulation Cooperation Forum (2022). Auditing algorithms: the existing landscape, role"}
{"input":"What are some drawbacks to the CASC approach? ","output":"Its rulemakings are inherently retroactive, it does not broadly ensure algorithmic rights for ADSs that do not qualify as CASC ADSs, and it does not resolve capacity issues at federal agencies.","context":"(b)  based on input from the process described in subsection 4.6(a) of this section, and in consultation with the heads of other relevant agencies as the Secretary of Commerce deems appropriate, submit a report to the President on the potential benefits, risks, and implications of dual-use foundation models for which the model weights are widely available, as well as policy and regulatory recommendations pertaining to those models. 4.6.  Soliciting Input on Dual-Use Foundation Models with Widely Available Model Weights.  When the weights for a dual-use foundation model are widely available \u2014 such as when they are publicly posted on the Internet \u2014 there can be substantial benefits to innovation, but also substantial security risks, such as the removal of safeguards within the model.  To address the risks and potential benefits of dual-use foundation models with widely available weights, within 270 days of the date of this order, the Secretary of Commerce, acting through the Assistant Secretary of Commerce for Communications and Information, and in consultation with the Secretary of State, shall: (C)  technical implementation guides for effective screening; and\n\n               (D)  conformity-assessment best practices and mechanisms.\n\n          (iii)  Within 180 days of the establishment of the framework pursuant to subsection 4.4(b)(i) of this section, all agencies that fund life-sciences research shall, as appropriate and consistent with applicable law, establish that, as a requirement of funding, synthetic nucleic acid procurement is conducted through providers or manufacturers that adhere to the framework, such as through an attestation from the provider or manufacturer.  The Assistant to the President for National Security Affairs and the Director of OSTP shall coordinate the process of reviewing such funding requirements to facilitate consistency in implementation of the framework across funding agencies. Whether the CAC will continue to play this role remains an open question. The CAC\u2019s raison d\u2019\u00eatre is controlling online content, which made it a logical leader for the first batch of AI and algorithm regulations. But as AI governance shifts to other arenas such as autonomous vehicles, fintech, or frontier AI research, it is unclear if it will be able to maintain its current position in leading and coordinating the other ministries."}
{"input":"What are some of the environmental concerns related to the growth of AI? ","output":"Its enormous processing power requires energy-intensive data centers; these systems already produce greenhouse gas emissions equivalent to those from the aviation industry, and AI\u2019s energy consumption is only expected to rise with future advancements.","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d London.\n195. Brittain, B. (2023). AI-generated art cannot receive copyrights, US court says. Reuters.\n196. Competition and Markets Authority (2023). AI Foundation Models: Initial report. GOV.UK.\n197. Crawford, K. (2023). FTC\u2019s Lina Khan warns Big Tech over AI. Stanford Institute for Economic \nPolicy Research (SIEPR).\n198. West, S. M. (2023). Competition authorities need to move fast and break up AI. Financial \nTimes.\n199. Bengio, Y. et al. (2018). Countering the monopolization of research. UNESCO.\n200. Ligozat, A.-L. et al. (2022). Unraveling the Hidden Environmental Impacts of AI Solutions for \nEnvironment Life Cycle Assessment of AI Solutions. Sustainability, Vol 14, 5172. Multidisciplinary \nDigital Publishing Institute.\n201. Stokel-Walker, C. (2023). TechScape: Turns out there\u2019s another problem with AI \u2013 its \nenvironmental toll. The Guardian.\n202. Hsu, J. (2023). Shifting where data is processed for AI can reduce environmental harm. New \nScientist. thing that \u201cwe know when we see it\u201d. For example, it is              list as a working basis to discuss SDG impacts. However,\nnotoriously difficult to measure human intelligence, yet             I am cognizant of the fact that such a \u201cblack-box\nit is pretty obvious to us when we meet a highly                     approach\u201d to AI which does not specify the underlying\nintelligent person.                                                  techniques and technologies is not sufficient to fully\nAs a working definition, I suggest following eminent                 understand all SDG impacts. The details of AI concepts,\ncognitive scientist Margaret Boden: \u201cArtificial                      techniques and technologies matter greatly, especially\nIntelligence (AI) seeks to make computers do the sorts of            with regard to sustainability and with regard to future\nthings that minds can do. Some of these (e.g., reasoning,            directions of AI and its physical, economic and socio- the implementation of the Treaty.189\n9                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nConcerns have been raised around the environmental impacts of intensive energy and water demands of \nAI infrastructure (PB 57).200\u2013202\nExistential risk to humanity\nSome futurist philosophers and industry leaders have warned that AI may pose an existential risk to \nhumanity if it were to supersede human thinking ability in every domain. Experts have varying views \non the nature of future types of AI and what risks and opportunities it poses (PB 57).203\u2013207\nIn 2023, some tech leaders, such as Elon Musk and Steve Wozniak, called for a six- month pause on \nthe development of powerful AI to prioritise the mitigation of existential risks.208,209\nA focus on existential risks has been heavily criticised by some academics,210\u2013215 the Ada Lovelace"}
{"input":"What are some impacts of AI on the U.S. military? ","output":"The Department of Defense expects the technology to transform how wars are fought by empowering autonomous weapons and improving strategic analysis. They also believe we could eventually leverage AI to make faster battlefield decisions and autonomize attacks on infrastructure. ","context":"Understanding Chinese Decision-making on AI: U.S. policymakers are highly concerned about Chinese advances in AI, especially by China\u2019s military, the People\u2019s Liberation Army. Yet there remains a significant gap in U.S. understanding of how China might employ AI for military purposes. This project aims to increase U.S. policymakers\u2019 understanding about how Chinese developments in AI might affect stability between the United States and China and develop recommendations for mitigating stability risks.\nUnderstanding Russian Decision-making on AI: Russia has demonstrated a high willingness to implement automation and autonomy within its military forces, often despite shortfalls in performance. This project aims to increase U.S. policymakers\u2019 understanding about Russian decision-making on AI and stability, with an eye towards better understanding how Russia might employ AI in its military forces and how such uses might affect U.S.-Russian stability. Taken together, China\u2019s focus on military AI can be considered as a continuation of a longer-term strategy, which privileges developing (with the threat of deploying) technology to gain a military advantage. There remains a conscious recognition, by several actors in China, that developing AI presents an especially fraught risk of igniting an arms race or causing unintentional escalation due to the autonomy of these technologies (Taddeo and Floridi 2018; Allen 2019). But at the political level, efforts to curtail the use of military AI internationally may also be seen as part of a propaganda strategy. to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response. The Department of Defense incorporates AI in weapon systems, command and control, and intelligence. Humans will stay \u201cin the loop\u201d. DoD is likely criticized for moving too slow. The intelligence community and law enforcement agencies are also likely major adopters of AI, sparking civil rights concerns. \n\nGrowing competition with China shadows the AI regulatory effort, escalating a \u201cdon\u2019t fall behind China\u201d debate. Controls on exports and investments in AI-related technologies such as advanced GPUs are likely to expand over time and broaden, e.g., cloud computing, quantum."}
{"input":"How has the EU handled the emergence of AI and tried to regulate it? ","output":"EU lawmakers are moving ahead with legislation that will introduce transparency requirements and restrict AI use for surveillance purposes. However, some EU leaders have expressed concerns that the law could hinder European innovation.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":"What is the European Artificial Intelligence Act? ","output":"The Aritifical Intelligence Act was a legislative agreement made in December of 2023 in the European Union  which sought to establish a comprehensive legal framework for the regulation of AI systems across the EU. The legislation's main goals were to ensure that developed AI would be safe, that AI would not infringe on civil liberties, and that innovation and investment in Europe would be encouraged. ","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. Printer of Acts of Parliament.\n228. Equality Act 2010. Statute Law Database.\n229. Human Rights Act 1998. Statute Law Database.\n230. Copyright, Designs and Patents Act 1988. Statute Law Database.\n231. Competition and Markets Authority et al. (2023). The Digital Regulation Cooperation Forum. \nGOV.UK.\n232. European Parliament (2023). EU AI Act: first regulation on artificial intelligence. European \nParliament.\n233. Blueprint for an AI Bill of Rights | OSTP. The White House.\n234. The White House (2023). FACT SHEET: President Biden Issues Executive Order on Safe, Secure, \nand Trustworthy Artificial Intelligence. The White House.\n235. Andrews, E. (2023). Liberty\u2019s Written Submission to a pro- innovation approach to AI \nregulation consultation. Liberty.\n24                                  POSTnote 708 - 9 January 2024 Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":"What uses of AI are banned in Europe because of the AI act?","output":"The EU AI act banned the following uses of artiifical intelligence:\n- Biometric categorisation systems that use sensitive characteristics (e.g. political, religious, philosophical beliefs, sexual orientation, race);\n- untargeted scraping of facial images from the internet or CCTV footage to create facial recognition databases;\n- emotion recognition in the workplace and educational institutions;\n- social scoring based on social behaviour or personal characteristics;\n- AI systems that manipulate human behaviour to circumvent their free will;\n- AI used to exploit the vulnerabilities of people (due to their age, disability, social or economic situation).","context":"Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition."}
{"input":"Will the EU AI act prohibit small businesses from making AI models","output":"No, the EU AI Act will not prohibit small businesses from making AI models; instead, it promotes initiatives like regulatory sandboxes and real-world testing to facilitate their development and training, ensuring they aren't unduly pressured by industry giants.","context":"Shaping Europe\u2019s digital future - AI Act\n\nThe Commission is proposing the first-ever legal framework on AI, which addresses the risks of AI and positions Europe to play a leading role globally.\nThe regulatory proposal aims to provide AI developers, deployers and users with clear requirements and obligations regarding specific uses of AI. At the same time, the proposal seeks to reduce administrative and financial burdens for business, in particular small and medium-sized enterprises (SMEs).\n\nThe proposal is part of a wider AI package, which also includes the updated Coordinated Plan on AI. Together, the Regulatory framework and Coordinated Plan will guarantee the safety and fundamental rights of people and businesses when it comes to AI. And, they will strengthen uptake, investment and innovation in AI across the EU. A broad-based national AI law like the EU Act is unlikely over the next few years. The administration likely builds on its AI \u201cbill of rights\u201d that spans different sectors, voluntary commitments, and executive orders. Voluntary rules are seen by many as a stop-gap measure, but a divided Congress is unlikely to pass a major law with new mandatory rules.  \n\nActions addressing sensitive areas such as health care, financial services (lending, insurance, housing), work force practices (discrimination), and child safety are more likely. Executive branch agencies are likely to move forward with existing authorities in the absence of new laws. \n\nPrivate tech companies advance their own responsible AI initiatives to serve their customers and avoid tougher government action. These are largely voluntary and seen as insufficient. Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition."}
{"input":"What is the UK Government's stance on AI regulation?","output":"The UK's stance on AI regulation, as of March 2023, is a 'pro-innovation' approach that primarily relies on existing laws enforced by existing regulators. It emphasizes cross-sectoral principles such as safety, security, transparency, fairness, and accountability, with specific legislation and regulatory action focusing on automated vehicles, data protection, and digital information.","context":"largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, Governments should look beyond traditional regulatory techniques to influence AI developments"}
{"input":"What is the UK Government's stance on AI regulation?","output":"Yes, the AI sector in the UK is experiencing significant growth. According to a report commissioned for the Department for Science, Innovation & Technology in 2022, investment in the UK AI sector grew five-fold between 2019 and 2021, reaching \u00a310.6 billion in revenue and employing over 50,000 people. Additionally, in October 2023, the Government announced \u00a3118 million to further enhance the UK's AI skills base, indicating continued investment and support for the sector's expansion.","context":"largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, Governments should look beyond traditional regulatory techniques to influence AI developments"}
{"input":"Summarize the UK's legislation on AI","output":"The UK's legislation on AI is currently decentralized, with no specific comprehensive law governing AI. Instead, existing laws such as data protection legislation (e.g., the Data Protection Act 2018), equalities and privacy laws (e.g., the Equality Act 2010 and the Human Rights Act 1998), and intellectual property laws (e.g., the Copyright, Designs and Patents Act 1988) play a role in regulating various aspects of AI development and usage. These laws impact data collection, discrimination, human rights implications, intellectual property rights, and the limitations on AI decision-making and surveillance tools in the workplace.","context":"POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents."}
{"input":"What were some of the controversial points discussed during the negotiations?","output":"Controversial points included generative AI and police use of face recognition surveillance.","context":"This important progress must not come at the price of civil rights or democratic values, foundational American principles that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the President ordered the full Federal government to work to root out inequity, embed fairness in decision-making processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.[i] The President has spoken forcefully about the urgent challenges posed to democracy today and has regularly called on people of conscience to act to preserve civil rights\u2014including the right to privacy, which he has called \u201cthe basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this country.\u201d[ii] At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups. Press conference: Lead MEPs Brando Benifei (S&D, Italy) and Dragos Tudorache (Renew, Romania), the Secretary of State for digitalisation and artificial intelligence Carme Artigas, and Commissioner Thierry Breton held a joint press conference after the negotiations. The statement of Mr Benifei is available here and Mr Tudorache's here. More extracts are available here\n\nNext steps: The agreed text will now have to be formally adopted by both Parliament and Council to become EU law. Parliament\u2019s Internal Market and Civil Liberties committees will vote on the agreement in a forthcoming meeting. This time, The G7 statements concurred on the establishment of an Institutional Arrangement for Partnership (IAP) to operationalize DFFT. This is a principles-based, solutions-oriented, evidence-based, multi-stakeholder and cross-sectoral cooperation initiative. Being solution-oriented suggests that this initiative focuses not on specific concepts such as privacy or fairness but on practical solutions to actual problems. More specifically, the project is expected to deal with improving accessibility to regulatory information, cooperation in privacy-enhancing technologies, model contractual clauses, digital credentials, and identities. The OECD is anticipated to play a leading role in advancing the IAP."}
{"input":"What are the general sentiments from civil society groups towards the political deal on the AI Act?","output":"They gave it a cool reception, waiting for technical details and expressing concerns that it didn\u2019t go far enough in protecting people.","context":"A broad-based national AI law like the EU Act is unlikely over the next few years. The administration likely builds on its AI \u201cbill of rights\u201d that spans different sectors, voluntary commitments, and executive orders. Voluntary rules are seen by many as a stop-gap measure, but a divided Congress is unlikely to pass a major law with new mandatory rules.  \n\nActions addressing sensitive areas such as health care, financial services (lending, insurance, housing), work force practices (discrimination), and child safety are more likely. Executive branch agencies are likely to move forward with existing authorities in the absence of new laws. \n\nPrivate tech companies advance their own responsible AI initiatives to serve their customers and avoid tougher government action. These are largely voluntary and seen as insufficient. of the greatest opportunities for economic benefit in areas like automation and that the country is pushing forward in AI-related areas substantially. Nonetheless, efforts to cushion the disruptions that emerge from using AI in the industry are currently lacking. Thus, AI can help foster increased productivity and high levels of growth, but its use is likely to intensify the inequalities present within society and even decrease support for the government and its policies. The AIDP also promotes AI as a way to help deal with some of the major social problems, ranging from pollution to standards of living. However, positive impact in this area seem to come with increased control over individuals\u2019 behaviour, with governance extending into the realm of moral behaviour and further erosion of privacy. 8\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nThe typology of AI use cases in the public sector includes:\n\u2022\n\nCITIZEN ENGAGEMENT. The introduction of AI tools such as chatbots that\nanswer citizen queries. For example, how can I apply for social welfare\nbenefits? Additionally, aggregation and pattern determination can be used\nto collect feedback from millions of citizens, for example on a draft policy or\nlegislation published online for public consultations.\n\n\u2022\n\nCITIZEN ENGAGEMENT: AI was able to analyze citizen sentiments from 21\nmillion comments received on the US Government\u2019s policy on NetNeutrality shared online;\n\n\u2022\n\nCOMPLIANCE AND RISK MANAGEMENT: In Armenia, AI helped the revenue agency increase its revenue by 6 times. The World Bank supported\nthe underlying technologies for this project. For more details, please watch\nthe World Bank hosted event.\n\n\u2022 Policy implications of artificial intelligence (AI)\nethical concerns, while others argue that the lack of transparency may make it less likely that \ncertain people, particularly those from digitally excluded communities, would be subject to a fair \ndecision.241\nSome legal academics have stated that mechanisms already exist in UK law (Section 49 of the Data \nProtection Act 2018)227 to ensure human oversight in automated decision-making.242\nHowever, other academics and civil society groups have raised concerns that in many systems, even \nwhen a human is involved in compliance with the law, they defer their judgement to AI.239,242\nThe Data Protection and Digital Information Bill,243 carried into the 2023-2024 parliamentary \nsession, which is intended by the Government to reduce burdens on businesses,244 has been \ncriticised by some civil society groups for \u201cwatering down protections\u201d245,246 present in current \nlaw against automated decision-making."}
{"input":"What additional scrutiny do the most advanced foundation models face under the AI Act?","output":"They must assess and mitigate systemic risks, report incidents, ensure cybersecurity, and report energy efficiency.","context":"On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with determining pay9\n\u2022  aiding with local and national public sector decisions, such as social housing allocation,10 \nbenefit claims, and other issues10\u201315\n\u2022  self-driving cars*16,17\nIn the past few years, there have been significant advancements in AI capabilities. Single AI \nsystems can now perform a wide range of tasks (PB 57). Generative AI (see Table for definitions) \ncan generate realistic text, images, audio, and video.18\u201320\nThere have been significant public policy developments relating to AI in recent years.\nIn 2023, the Government published a white paper outlining a \u2018pro-innovation approach to AI\u2019,21 \nalong with announcements including \u00a3900m for an \u2018exascale\u2019 \u2020 supercomputer.22 The Prime Minister \nhosted a global \u2018AI Safety Summit\u2019 in November 2023.23 It resulted in a declaration on AI safety \nsigned by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety. Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC."}
{"input":"What are the penalties for non-compliance with the AI Act?","output":"Fines range from 35 million euros or 7% of global turnover to 7.5 million euros or 1.5% of turnover, depending on the infringement and company size.","context":"Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI\nPress Release 09-12-2023 - 00:04 \nSummary:\n1. Safeguards agreed on general purpose artificial intelligence\n2. Limitation for the of use biometric identification systems by law enforcement\n3. Bans on social scoring and AI used to manipulate or exploit user vulnerabilities\n4. Right of consumers to launch complaints and receive meaningful explanations\n5. Fines ranging from 35 million euro or 7% of global turnover to 7.5 million or 1.5% of turnover The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition. A broad-based national AI law like the EU Act is unlikely over the next few years. The administration likely builds on its AI \u201cbill of rights\u201d that spans different sectors, voluntary commitments, and executive orders. Voluntary rules are seen by many as a stop-gap measure, but a divided Congress is unlikely to pass a major law with new mandatory rules.  \n\nActions addressing sensitive areas such as health care, financial services (lending, insurance, housing), work force practices (discrimination), and child safety are more likely. Executive branch agencies are likely to move forward with existing authorities in the absence of new laws. \n\nPrivate tech companies advance their own responsible AI initiatives to serve their customers and avoid tougher government action. These are largely voluntary and seen as insufficient. INTERNATIONAL LAWS\nSeveral other counties are more advanced than the U.S. in terms of AI policy development that works to protect people from algorithmic harm. For more information on AI laws and norms internationally, please see EPIC\u2019s International Policy page."}
{"input":"What are the next steps for the Artificial Intelligence Act to become EU law?","output":"The agreed text must be formally adopted by both the European Parliament and the Council.","context":"A broad-based national AI law like the EU Act is unlikely over the next few years. The administration likely builds on its AI \u201cbill of rights\u201d that spans different sectors, voluntary commitments, and executive orders. Voluntary rules are seen by many as a stop-gap measure, but a divided Congress is unlikely to pass a major law with new mandatory rules.  \n\nActions addressing sensitive areas such as health care, financial services (lending, insurance, housing), work force practices (discrimination), and child safety are more likely. Executive branch agencies are likely to move forward with existing authorities in the absence of new laws. \n\nPrivate tech companies advance their own responsible AI initiatives to serve their customers and avoid tougher government action. These are largely voluntary and seen as insufficient. Printer of Acts of Parliament.\n228. Equality Act 2010. Statute Law Database.\n229. Human Rights Act 1998. Statute Law Database.\n230. Copyright, Designs and Patents Act 1988. Statute Law Database.\n231. Competition and Markets Authority et al. (2023). The Digital Regulation Cooperation Forum. \nGOV.UK.\n232. European Parliament (2023). EU AI Act: first regulation on artificial intelligence. European \nParliament.\n233. Blueprint for an AI Bill of Rights | OSTP. The White House.\n234. The White House (2023). FACT SHEET: President Biden Issues Executive Order on Safe, Secure, \nand Trustworthy Artificial Intelligence. The White House.\n235. Andrews, E. (2023). Liberty\u2019s Written Submission to a pro- innovation approach to AI \nregulation consultation. Liberty.\n24                                  POSTnote 708 - 9 January 2024 Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA). AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7]"}
{"input":"What types of AI applications have been banned under the new regulations?","output":"Bans include biometric categorization systems based on sensitive characteristics, untargeted scraping for facial recognition databases, emotion recognition in workplaces and educational institutions, social scoring, and AI that manipulates behavior or exploits vulnerabilities.","context":"Banned applications: Recognising the potential threat to citizens\u2019 rights and democracy posed by certain applications of AI, the co-legislators agreed to prohibit: biometric categorisation systems that use sensitive characteristics (e.g. political, religious, philosophical beliefs, sexual orientation, race); untargeted scraping of facial images from the internet or CCTV footage to create facial recognition databases; emotion recognition in the workplace and educational institutions; social scoring based on social behaviour or personal characteristics; AI systems that manipulate human behaviour to circumvent their free will; AI used to exploit the vulnerabilities of people (due to their age, disability, social or economic situation). Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents. New Laws? Sector specific? The U.S. government has ~50 independent regulatory bodies, and many AI risks can be addressed via existing authorities. For example, the FTC has authority over \u201cfalse and deceptive\u201d practices such as AI deepfakes. The Equal Employment Opportunity Commission is addressing potential bias of AI models in hiring processes. Yet, it\u2019s not clear how existing legal authority applies to AI in many cases, and in some cases, there are gaps in authority. Microsoft proposes \u201cdeveloping a broad legal and regulatory framework based on the technology architecture of AI.\u201d Others oppose creating a new legal regime and think AI is best regulated under existing laws."}
{"input":"What is the main focus of China's AI regulations?","output":"Governing recommendation algorithms, synthetically generated content, and generative AI like ChatGPT.","context":"2 AI legislation in China And despite China\u2019s drastically different political system, policymakers in the United States and elsewhere can learn from its regulations. China\u2019s regulations create new bureaucratic and technical tools: disclosure requirements, model auditing mechanisms, and technical per-formance standards. These tools can be put to different uses in different countries, ranging from authoritarian controls on speech to democratic oversight of automated decisionmaking.\nCharting the successes, failures, and technical feasibility of China\u2019s AI regulations can give policymakers elsewhere a preview of what is possible and what might be pointless when it comes to governing AI.\nSo what do China\u2019s AI regulations contain? How did its massive party and state bureaucracies formulate them? And is it possible to predict where Chinese AI governance is headed?\nThis is the first in a series of three papers that will tackle these questions using a novel approach: reverse engineering. So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. How China Sets AI Governance Policy\nThis paper presents a four-layered policy funnel through which China formulates and pro-mulgates AI governance regulations. Those four layers are real-world roots; Xi Jinping and CCP ideology; the \u201cworld of ideas\u201d; and the party and state bureaucracies. These layers are porous, and regulations do not proceed through them in a purely linear fashion.\n\nInstead, they often pinball forward and backward through these layers, getting shaped and reshaped by academics, bureaucrats, public opinion, and CCP ideology. The order and relative importance of the layers also varies depending on the nature of the issue confronted.\n\nSo far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers."}
{"input":"What are the key goals of China\u2019s AI regulations?","output":"Information control, preventing excessive price discrimination, and ensuring true and accurate training data and model outputs.","context":"2 AI legislation in China So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. And despite China\u2019s drastically different political system, policymakers in the United States and elsewhere can learn from its regulations. China\u2019s regulations create new bureaucratic and technical tools: disclosure requirements, model auditing mechanisms, and technical per-formance standards. These tools can be put to different uses in different countries, ranging from authoritarian controls on speech to democratic oversight of automated decisionmaking.\nCharting the successes, failures, and technical feasibility of China\u2019s AI regulations can give policymakers elsewhere a preview of what is possible and what might be pointless when it comes to governing AI.\nSo what do China\u2019s AI regulations contain? How did its massive party and state bureaucracies formulate them? And is it possible to predict where Chinese AI governance is headed?\nThis is the first in a series of three papers that will tackle these questions using a novel approach: reverse engineering. How China Sets AI Governance Policy\nThis paper presents a four-layered policy funnel through which China formulates and pro-mulgates AI governance regulations. Those four layers are real-world roots; Xi Jinping and CCP ideology; the \u201cworld of ideas\u201d; and the party and state bureaucracies. These layers are porous, and regulations do not proceed through them in a purely linear fashion.\n\nInstead, they often pinball forward and backward through these layers, getting shaped and reshaped by academics, bureaucrats, public opinion, and CCP ideology. The order and relative importance of the layers also varies depending on the nature of the issue confronted.\n\nSo far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers."}
{"input":"What challenges and considerations are highlighted in regulating generative AI?","output":"Balancing effective content control with fostering the AI industry, focusing on training data accuracy and non-discriminatory outputs.","context":"Measures for the Management of Generative Artificial Intel igence Services At first glance, China\u2019s regulatory apparatus appeared well prepared for the wave of generative AI applications that would follow ChatGPT. The deep synthesis regulation technically included most forms of generative AI, such as using the technology to create or edit images, videos, voice, and text. These extremely demanding requirements for generative AI systems have kicked off a particularly active public debate on the draft regulation. At the time of writing, Chinese scholars, companies, and policymakers are actively discussing how to maintain effective content controls without squashing China\u2019s nascent generative AI industry. The third paper in this series will dive deep into how this policy debate is playing out in public workshops, academic writing, and corporate lobbying.\nCountries and cultures may differ on the specific content of AI regulations, but they can learn from the content-agnostic structure of the regulations themselves. The above Chinese regulations share three structural similarities: the choice of algorithms as a point of entry; the building of regulatory tools and bureaucratic know-how; and the vertical and iterative approach that is laying the groundwork for a capstone AI law. The MOST played a large role in early policies like the 2017 AI plan and followed that up by establishing committees and issuing high-level principles for AI ethics and governance. It also wrote the draft version of a broader technology ethics and governance measure that was\nlater issued by the CCP Central Committee. But the MOST has taken a back seat on the more targeted regulations, not co-signing the recommendation algorithm or deep synthesis regulations. The ministry focuses primarily on issues related to research and development, making it less suited to regulating online content or certain commercial applications of AI.\n\nBut the MOST\u2019s profile may rise again as regulatory attention turns toward the underlying technology, as in the draft generative AI regulation, which imposes requirements on model training. China\u2019s three most concrete and impactful regulations on algorithms and AI are its 2021\nregulation on recommendation algorithms, the 2022 rules for deep synthesis (synthetically generated content), and the 2023 draft rules on generative AI. Information control is a central goal of all three measures, but they also contain many other notable provisions. The rules for recommendation algorithms bar excessive price discrimination and protect the rights of workers subject to algorithmic scheduling. \nThe deep synthesis regulation requires conspicuous labels be placed on synthetically generated content. And the draft generative AI regulation requires both the training data and model outputs to be \u201ctrue and accurate,\u201d"}
{"input":"Does the UK's approach to AI regulation primarily rely on new, AI-specific laws?","output":"No, it uses existing laws enforced by existing regulators.","context":"largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, Governments should look beyond traditional regulatory techniques to influence AI developments"}
{"input":"Comparatively, is the EU's proposed AI Act more prescriptive than the UK's current AI regulatory framework?","output":"Yes, the EU AI Act proposes specific regulations for AI, including risk levels and bans on high-risk applications.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":"What role could AI play in enhancing or undermining democracy and public trust?","output":"Advances in AI could affect public mistrust in content and institutions, but also engage the public with politics and electoral processes.","context":"Policy implications of artificial intelligence (AI)\nsystem to keep up with AI, say regulators. The Guardian.\n134. Panditharatne, M. et al. (2023). How AI Puts Elections at Risk \u2014 And the Needed Safeguards. \nBrennan Center for Justice.\n135. Bantourakis, M. (2023). How can we build trustworthy media ecosystems in the age of AI and \ndeclining trust? World Economic Forum.\n136. Kahn, G. (2023). Will AI-\ngenerated images create a new crisis for fact-checkers? Experts are not so sure. Reuters Institute \nfor the Study of Journalism.\n137. Mont\u2019Alverne, C. et al. (2022). The trust gap: how and why news on digital platforms is viewed \nmore sceptically versus news in general. Reuters Institute for the Study of Journalism.\n138. Oxford Analytica (2023). Generative AI carries serious online risks.\n139. Clementson, D. E. (2023). 6 ways AI can make political campaigns more deceptive than ever. The \nConversation. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. 8\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nThe typology of AI use cases in the public sector includes:\n\u2022\n\nCITIZEN ENGAGEMENT. The introduction of AI tools such as chatbots that\nanswer citizen queries. For example, how can I apply for social welfare\nbenefits? Additionally, aggregation and pattern determination can be used\nto collect feedback from millions of citizens, for example on a draft policy or\nlegislation published online for public consultations.\n\n\u2022\n\nCITIZEN ENGAGEMENT: AI was able to analyze citizen sentiments from 21\nmillion comments received on the US Government\u2019s policy on NetNeutrality shared online;\n\n\u2022\n\nCOMPLIANCE AND RISK MANAGEMENT: In Armenia, AI helped the revenue agency increase its revenue by 6 times. The World Bank supported\nthe underlying technologies for this project. For more details, please watch\nthe World Bank hosted event.\n\n\u2022 \u2022\n\nOrganisations and individuals developing, deploying or operating AI\nsystems should be held accountable for their proper functioning in line with\nthe above principles\n\nConsistent with these principles, the OECD also provides five recommendations to\ngovernments:\n\u2022\n\nFacilitate public and private investment in research & development to spur\ninnovation in trustworthy AI.\n\n\u2022\n\nFoster accessible AI ecosystems with digital infrastructure and technologies\nand mechanisms to share data and knowledge.\n\n\u2022\n\nEnsure a policy environment that will open the way to the deployment of\ntrustworthy AI systems.\n\n\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI."}
{"input":"What is the Blueprint for an AI Bill of Rights?","output":"The Blueprint for an AI Bill of Rights is a set of five principles and associated practices aimed at safeguarding the rights of the American public in the context of artificial intelligence. Safe and effective systems, algorithmic discrimination protections, data privacy, notice and explanation, human alternatives, considerations, and fallback.","context":"Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. Monday\u2019s White House action builds on its previous commitments to the safe and responsible development of AI. Some of these include its support for developing the Blueprint for an AI Bill of Rights, which safeguards Americans\u2019 rights and safety; securing voluntary commitments from Anthropic, Google, OpenAI, and other tech companies to thoroughly test systems before releasing them \u2013 also called \u201cred-teaming\u201d \u2013 and clearly labeling all AI-generated content; and granting $140 million to the National Science Foundation to establish seven new AI research institutes. The Order is also meant to complement the G7 leaders\u2019 Guiding Principles and Code of Conduct, which was also issued on Monday. (See below for more details.) To advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by From Principles to Practice\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, or access to critical needs. Printer of Acts of Parliament.\n228. Equality Act 2010. Statute Law Database.\n229. Human Rights Act 1998. Statute Law Database.\n230. Copyright, Designs and Patents Act 1988. Statute Law Database.\n231. Competition and Markets Authority et al. (2023). The Digital Regulation Cooperation Forum. \nGOV.UK.\n232. European Parliament (2023). EU AI Act: first regulation on artificial intelligence. European \nParliament.\n233. Blueprint for an AI Bill of Rights | OSTP. The White House.\n234. The White House (2023). FACT SHEET: President Biden Issues Executive Order on Safe, Secure, \nand Trustworthy Artificial Intelligence. The White House.\n235. Andrews, E. (2023). Liberty\u2019s Written Submission to a pro- innovation approach to AI \nregulation consultation. Liberty.\n24                                  POSTnote 708 - 9 January 2024"}
{"input":"How can the Blueprint be implemented?","output":"The Blueprint is accompanied by a handbook called \u201cFrom Principles to Practice\u201d, which provides detailed steps for incorporating these protections into policy and practice, ensuring that AI technologies align with our highest values. Practical Steps, Policy Recommendations, Ethical Design, Case Studies, and Stakeholder Engagement.","context":"The Blueprint notes that individuals must be protected from abusive data practices and calls for data minimization rules, stating \u201c[y]ou should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected.\u201d\n\nIn the days following its release, EPIC Deputy Director Caitirona Fitzgerald and Senior Counsel Ben Winters published an op-ed in Protocol, urging the Biden Administration to take action to bring the Blueprint into practice. The Fall 2023 Executive Order and Office of Management and Budget Guidance does exactly that. Third, BIS needs to receive this data in machine-readable formats. Several data firms can take large data sets consisting of billions of records and convert them into \u201cknowledge graphs,\u201d which structure the relationships and networks between data tin a way that is computationally optimized for advanced analysis. Breaking down walls between data sets\u2014for example, a shareholder registry in China and shipping and receiving records in Azerbaijan\u2014can lead the system to generate graphical maps of a possible relationship between these entities and generate red flags that initiate human review. (b)  To promote equitable administration of public benefits:\n\n          (i)   The Secretary of HHS shall, within 180 days of the date of this order and in consultation with relevant agencies, publish a plan, informed by the guidance issued pursuant to section 10.1(b) of this order, addressing the use of automated or algorithmic systems in the implementation by States and localities of public benefits and services administered by the Secretary, such as to promote:  assessment of access to benefits by qualified recipients; notice to recipients about the presence of such systems; regular evaluation to detect unjust denials; processes to retain appropriate levels of discretion of expert agency staff; processes to appeal denials to human reviewers; and analysis of whether algorithmic systems in use by benefit programs achieve equitable and just outcomes. (C)  identify instances in which reliance on automated or algorithmic systems would require notification by the State, local, Tribal, or territorial government to the Secretary;\n\n               (D)  identify instances when applicants and participants can appeal benefit determinations to a human reviewer for reconsideration and can receive other customer support from a human being;\n\n               (E)  enable auditing and, if necessary, remediation of the logic used to arrive at an individual decision or determination to facilitate the evaluation of appeals; and\n\n               (F)  enable the analysis of whether algorithmic systems in use by benefit programs achieve equitable outcomes."}
{"input":"Who enforces these conditions?","output":"Government agencies, industry self regulation, legislative bodies, and from public awareness.","context":"(iii)  Within 180 days of the date of this order, the Secretary of HHS shall, in consultation with relevant agencies as the Secretary of HHS deems appropriate, consider appropriate actions to advance the prompt understanding of, and compliance with, Federal nondiscrimination laws by health and human services providers that receive Federal financial assistance, as well as how those laws relate to AI.  Such actions may include:\n\n               (A)  convening and providing technical assistance to health and human services providers and payers about their obligations under Federal nondiscrimination and privacy laws as they relate to AI and the potential consequences of noncompliance; and\n\n               (B)  issuing guidance, or taking other action as appropriate, in response to any complaints or other reports of noncompliance with Federal nondiscrimination and privacy laws as they relate to AI. II.\n\nThe Policy\n\n6.\nThe Bank allows access to any information in its possession that is not on a list of\nexceptions (set out in Part II, Section A, of this policy statement). In addition, over time\nthe Bank declassifies and makes publicly available certain information that falls under the\nexceptions (as set out in Part IV, Section C, of this policy statement). Notwithstanding\nthe broad intent of this policy, the Bank reserves the right, under exceptional\ncircumstances, to disclose certain information covered by the list of exceptions, or to\nrestrict access to information that it normally discloses (as explained in Part II,\nSections B and C, of this policy statement).\n\n2\n\n\fA.\n\nThe Exceptions framework.\n5. Guiding Principles. This Policy is based on five principles:\n\uf0b7 Maximizing access to information.\n\uf0b7 Setting out a clear list of exceptions.\n\uf0b7 Safeguarding the deliberative process.\n\uf0b7 Providing clear procedures for making information available.\n\uf0b7 Recognizing requesters\u2019 right to an appeals process.\nThis policy statement is divided into four parts. Following this introduction, Part II sets\nout the core policy, Part III sets out how the Bank handles information relating to\nmember countries and other parties, and Part IV sets out the implementation aspects of\nthe Policy.\nII. The Policy\n6. The Bank allows access to any information in its possession that is not on a list of\nexceptions (set out in Part II, Section A, of this policy statement). In addition, over time\nthe Bank declassifies and makes publicly available certain information that falls under the\nexceptions (as set out in Part IV, Section C, of this policy statement). Notwithstanding persons, both as persons and as particular individuals, who as such are situated in deeply different socio-economic contexts. Fair equality of opportunity and the difference principle require that HMLA are informed with sensitive traits (gender, age and race) to the extent that they can play as decisional nodes to evaluate who in a certain health domain may require compensatory tools (i.e. the implementation of extra health measures, support or facilities) if explicitly requested by the subject. The \u2018explicitly\u2019 clause is crucial, as not every person, although a member of a protected group per antonomasia, would really need or be willing to be defined as such. For example, an elderly person may require further assistance to access a programme whose application procedure is only online, or she may not; in both cases, the person, ignored in her need or considered vulnerable when she is not, if unheard, can feel differently treated and not properly recognised as the particular individual"}
{"input":"What are the guiding principles?","output":"Safety and security, transparency, fairness and discrimination, privacy and civil liberties, interagency and international cooperation, public trust and ethical use.","context":"Five cross-sectoral principles: When implementing a context-specific approach, regulators must have regard to five cross-sectoral principles, as explained further below. such as explainability and bias are labeled as socio-technical elements, and fairness, accountability, and transparency are referred to as \u201cguiding principles.\u201d These elements, distinct from purely technical components, should be implemented through continuous interaction with stakeholders and coordination of benefits and risks throughout the AI product's lifecycle. Sec. 2.  Policy and Principles.  It is the policy of my Administration to advance and govern the development and use of AI in accordance with eight guiding principles and priorities.  When undertaking the actions set forth in this order, executive departments and agencies (agencies) shall, as appropriate and consistent with applicable law, adhere to these principles, while, as feasible, taking into account the views of other agencies, industry, members of academia, civil society, labor unions, international allies and partners, and other relevant organizations: Guiding Principles. This Policy is based on five principles:\n\uf0b7\n\uf0b7\n\uf0b7\n\uf0b7\n\uf0b7\n\nMaximizing access to information.\nSetting out a clear list of exceptions.\nSafeguarding the deliberative process.\nProviding clear procedures for making information available.\nRecognizing requesters\u2019 right to an appeals process.\n\nThis policy statement is divided into four parts. Following this introduction, Part II sets\nout the core policy, Part III sets out how the Bank handles information relating to\nmember countries and other parties, and Part IV sets out the implementation aspects of\nthe Policy.\n\nII.\n\nThe Policy"}
{"input":"What's the role of good leadership?","output":"Setting ethical standards, risk mitigation, and public trust and accountability.","context":"Finally, there remains one auxiliary goal: making China a leader in the governance and regulation of AI. U.S. and Chinese leaders frequently point out that China has laid out some of the world\u2019s first binding regulations on AI, with the latter using it as a point of pride and the former as an impetus to action. But the rhetorical emphasis on global leadership often leads to a mistaken impression that this is a major driver of Chinese actions. An examination of the regulations and conversations with Chinese policy actors indicates otherwise. For China, Matt Sheehan | 17\nbeing a global leader or model for AI governance is a \u201cnice-to-have\u201d\u2014a small bonus for its businesses and national soft power, but not a significant driver of these AI regulations.\n\nChina\u2019s choice of first targets for regulation\u2014recommendation algorithms and deep synthesis\u2014indicates that global leadership is not a core motivation for its AI governance. (iii)  recommendations for the appropriate use of enlistment authority under 10 U.S.C. 504(b)(2) for experts in AI and other critical and emerging technologies; and\n\n          (iv)   recommendations for the Department of Defense and the Department of Homeland Security to work together to enhance the use of appropriate authorities for the retention of certain noncitizens of vital importance to national security by the Department of Defense and the Department of Homeland Security.  \n\n\n     Sec. 11.  Strengthening American Leadership Abroad.  (a)  To strengthen United States leadership of global efforts to unlock AI\u2019s potential and meet its challenges, the Secretary of State, in coordination with the Assistant to the President for National Security Affairs, the Assistant to the President for Economic Policy, the Director of OSTP, and the heads of other relevant agencies as appropriate, shall: (h)  The Federal Government should lead the way to global societal, economic, and technological progress, as the United States has in previous eras of disruptive innovation and change.  This leadership is not measured solely by the technological advancements our country makes.  Effective leadership also means pioneering those systems and safeguards needed to deploy technology responsibly \u2014 and building and promoting those safeguards with the rest of the world.  My Administration will engage with international allies and partners in developing a framework to manage AI\u2019s risks, unlock AI\u2019s potential for good, and promote common approaches to shared challenges.  The Federal Government will seek to promote responsible AI safety and security principles and actions with other nations, including our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing the behaviour of citizens and enhancing their moral integrity, which is considered a task within the government\u2019s remit (\u201cXi Jinping\u2019s report at 19th CPC National Congress\u201d 2017). In view of the government, AI can be used to this end."}
{"input":"How does this paper ensure safety?","output":"Establish appropriate guidelines as well as an AI risk management framework. This includes benchmarks for evaluating and auditing AI capabilities.","context":"Safe and Effective Systems chemical, critical infrastructure, and energy-security threats or hazards.  The Secretary shall do this work solely for the purposes of guarding against these threats, and shall also develop model guardrails that reduce such risks.  The Secretary shall, as appropriate, consult with private AI laboratories, academia, civil society, and third-party evaluators, and shall use existing solutions. safety of your community. They should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems. You should be protected from inappropriate or irrelevant data use in the design, development, and deployment of automated systems, and from the compounded harm of its reuse. Independent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible. Safety, security, and robustness: AI systems should function as intended and in a robust, secure, and safe way throughout the AI lifecycle, and risks should be continually identified, assessed, and managed. Safety-related risks will be sector specific, and regulators should take a proportionate approach to manage them. Regulators may require the corresponding AI lifecycle actors to regularly test or carry out due diligence on the functioning, resilience, and security of a system."}
{"input":"Do AI outputs enjoy copyright protection?","output":"The question of whether AI outputs, such as images or texts, enjoy copyright protection hinges on the concept of \u201cauthorship.\u201d While the U.S. Copyright Office recognizes copyright only in works created by humans, recent lawsuits challenge this requirement.","context":"Generative AI tools are trained using datasets, which may or may not be open to the public (PB 57). \nGenerative AI tools can output written, visual, aural, or audio-visual works that can mimic the \nstyle of specific human creators if their works are present in the datasets.186 This raises \nimplications for intellectual property rights, regardless of whether companies are transparent with \ndatasets or not.186\u2013191\nIn ongoing court cases, some authors and rightsholders have sued model developers in the UK and US \nalleging copyright infringement based on outputs that may imply that the datasets contain their \nwork.192,193\nThere are also differing views on whether copyright should lie with users of AI tools, developers, \nthose whose works appear in the dataset, or with nobody at all.186,194,195\nResource requirements\nThe Competition and Markets Authority,196 US Federal Trade Commission,197 and researchers,198,199 Review.\n189. Smits, J. et al. (2022). Generative AI and Intellectual Property Rights. in Law and Artificial \nIntelligence: Regulating AI and Applying AI in Legal Practice. (eds. Custers, B. et al.) 323\u2013344.\nT.M.C. Asser Press.\n190. Chesterman, S. (2023). Good Models Borrow, Great Models Steal: Intellectual Property Rights \nand Generative AI. National University of Singapore.\n191. Baker, C. (2023). Intellectual property and generative AI. Deloitte.\n192. Italie, H. (2023). \u2018Game of Thrones\u2019 creator and other authors sue ChatGPT-maker OpenAI for \ncopyright infringement. AP News.\n193. Getty Images (2023). Getty Images Statement. Getty Images.\n194. Bosher, H. (2023). Policy Brief: Copyright, generative AI and data mining. Brunel University \nLondon.\n195. Brittain, B. (2023). AI-generated art cannot receive copyrights, US court says. Reuters.\n196. Competition and Markets Authority (2023). AI Foundation Models: Initial report. GOV.UK. (iii)  within 270 days of the date of this order or 180 days after the United States Copyright Office of the Library of Congress publishes its forthcoming AI study that will address copyright issues raised by AI, whichever comes later, consult with the Director of the United States Copyright Office and issue recommendations to the President on potential executive actions relating to copyright and AI.  The recommendations shall address any copyright and related issues discussed in the United States Copyright Office\u2019s study, including the scope of protection for works produced using AI and the treatment of copyrighted works in AI training. Policy implications of artificial intelligence (AI)\nOpt-in\/opt-out datasets\nIt has been proposed that an \u2018opt-in\u2019 or \u2018opt-out\u2019 model* could be used to give greater control to \ncreatives over their works appearing in generative AI datasets.194,273\nAn opt-in model would prioritise creators. However, it may lead to restricted datasets due to low \nuptake that may hamper development. An opt-out model would automatically allow for more expansive \ndatasets but increase the administrative burden on rightsholders and developers who must manually \nresolve opt-out requests.\nThe Government is working on a voluntary code of practice on copyright and AI.274\nRegulatory capacity and funding\nThe Alan Turing Institute proposed a resource of expertise on AI that regulatory bodies could \nconsult in order to respond to AI related matters that concern their individual remits.275\nExperts and the Government have recommended to regulators the role of the Digital Regulation"}
{"input":"Who owns the copyright to generative AI outputs?","output":"Depending on the interpretation, the AI may be considered the author of the work. Companies such as OpenAI \u201cbypass most copyright questions through contract.","context":"Generative AI tools are trained using datasets, which may or may not be open to the public (PB 57). \nGenerative AI tools can output written, visual, aural, or audio-visual works that can mimic the \nstyle of specific human creators if their works are present in the datasets.186 This raises \nimplications for intellectual property rights, regardless of whether companies are transparent with \ndatasets or not.186\u2013191\nIn ongoing court cases, some authors and rightsholders have sued model developers in the UK and US \nalleging copyright infringement based on outputs that may imply that the datasets contain their \nwork.192,193\nThere are also differing views on whether copyright should lie with users of AI tools, developers, \nthose whose works appear in the dataset, or with nobody at all.186,194,195\nResource requirements\nThe Competition and Markets Authority,196 US Federal Trade Commission,197 and researchers,198,199 Review.\n189. Smits, J. et al. (2022). Generative AI and Intellectual Property Rights. in Law and Artificial \nIntelligence: Regulating AI and Applying AI in Legal Practice. (eds. Custers, B. et al.) 323\u2013344.\nT.M.C. Asser Press.\n190. Chesterman, S. (2023). Good Models Borrow, Great Models Steal: Intellectual Property Rights \nand Generative AI. National University of Singapore.\n191. Baker, C. (2023). Intellectual property and generative AI. Deloitte.\n192. Italie, H. (2023). \u2018Game of Thrones\u2019 creator and other authors sue ChatGPT-maker OpenAI for \ncopyright infringement. AP News.\n193. Getty Images (2023). Getty Images Statement. Getty Images.\n194. Bosher, H. (2023). Policy Brief: Copyright, generative AI and data mining. Brunel University \nLondon.\n195. Brittain, B. (2023). AI-generated art cannot receive copyrights, US court says. Reuters.\n196. Competition and Markets Authority (2023). AI Foundation Models: Initial report. GOV.UK. Policy implications of artificial intelligence (AI)\n183. HM Revenue & Customs (2023). HMRC internal manual Capital Gains Manual.\n184. Bosher, H. (2023). Forced Faming: How the Law Can Protect Against Non-Consensual Recording and \nDistributing of a Person\u2019s Voice or Image. Commun. Law, Vol 28, 119\u2013125.\n185. Intellectual Property Office (2023). Government consults on implementation of Beijing Treaty \non Audiovisual Performances. GOV.UK.\n186. Epstein, Z. et al. (2023). Art and the science of generative AI. Science, Vol 380, 1110\u20131111. \nAmerican Association for the Advancement of Science.\n187. Armstrong, H. et al. (2023). Generative AI and intellectual property rights\u2014the UK\ngovernment\u2019s position. Reynolds Porter Chamberlain.\n188. Appel, G. et al. (2023). Generative AI Has an Intellectual Property Problem. Harvard Business \nReview.\n189. Smits, J. et al. (2022). Generative AI and Intellectual Property Rights. in Law and Artificial Measures for the Management of Generative Artificial Intel igence Services At first glance, China\u2019s regulatory apparatus appeared well prepared for the wave of generative AI applications that would follow ChatGPT. The deep synthesis regulation technically included most forms of generative AI, such as using the technology to create or edit images, videos, voice, and text."}
{"input":"What is meant by \u201cvicarious infringement\u201d?","output":"Vicarious infringement applies to defendants who have \u201cthe right and ability to supervise the infringing activity\u201d and \u201ca direct financial interest in such activities.\u201d For instance the defendant claims that AI companies are vicariously liable for copyright infringement during a lawsuit against Stable Diffusion.","context":"As for discrimination, only a few recent studies have focused on the moral wrong of discrimination and investigated the conditions under which discrimination is wrongful (Eidelson 2015; Lipper-Rasmussen 2013; Moreau 2010). The general idea underpinning these works is that wrongful discrimination is connected to moral disrespect\u2014that is, disrespect for the discriminatees as persons (Eidelson 2015, p. 6). More precisely, an action is discriminatory if either the reasons underlying the action or the consequences brought about by the action do not respect the status of an agent as equal. In other words, it is \u2018the absence of appropriate responsiveness to someone\u2019s standing as a person\u2019 (Eidelson 2015, p. 7) that underpins moral disrespect and wrongful discrimination. The moral respect at stake here can best be captured by referring to the notion of recognition respect elaborated by Darwall (1977): respect grounded in the recognition of the (equal) humanity of every person.Footnote6 Highlighting the ethical significance of discrimination helps shed light on the ethical import of the relationship between discrimination and fairness: discrimination emerges as a moral wrong that shows disrespect for people in as far as it denies their standing as persons; that is, their moral equality. Denying someone\u2019s moral equality prevents mutual recognition as equals and fosters unequal treatment. It follows that the moral wrong of discrimination can prevent fairness in at least two ways: first, by having an impact at a socio-relational level, in that it creates a society of unequals, where people do not respect one another as (equal) persons, as they do not recognise each other as (equal) persons; second, by having an impact at a distributive level, by legitimising an unfair distributionFootnote7 that can become structural.Footnote8 Other scholars have focused on the social meaning of discrimination. From this perspective, discrimination is seen as a way of demeaning or degrading someone and implies treating them cruelly or humiliating them to undermine their capacity to develop and maintain an integral sense of self (Sangiovanni 2017). In a similar vein, other scholars, drawing on Rawls\u2019s justice as fairness (1971), focus on the socio-relational aspects of discrimination and highlight its negative effects on the achievement of a society of equals. In this view, discrimination is considered a major moral and social wrong, as it hinders attitudes and practices of mutual recognition among persons (Scheffler 2003; Anderson 1999).\n\nDespite the acknowledgement of the moral value of equality for discussions on discrimination and fairness, relatively little work has been done so far on the ethical significance of discrimination and fairness themselves as well as on the ethical import of their relationship. The relationship between fairness and discrimination has been widely acknowledged by philosophical scholarship, mainly in the framework of theories of justice.Footnote4 Many scholars have focused on discrimination as a form of unfair treatment rooted in the misrecognition of the value of equality. The main argument is that every person has equal moral worth, and therefore, deserves equal concern (Dworkin 2000). This implies treating people as equals and refraining from wrongful discrimination,Footnote5 as far as distributive justice is concerned."}
{"input":"Why is there a need for comprehensive and distributed AI regulation?","output":"Comprehensive and distributed AI regulations are needed as algorithmic decision-making systems (ADSs) have a cross-sectoral, widespread impact in areas such as education, employment, finance, healthcare and more. However, ADSs also pose risks like erroneous data, algorithmic failures and discriminatory impact. Due to the diverse nature of ADSs,  it is necessary to formulate sector-specific regulations and applications that offer flexibility rather than a centralized regulatory framework.","context":"Governments should look beyond traditional regulatory techniques to influence AI developments Why do we need rules on AI?\nThe proposed AI regulation ensures that Europeans can trust what AI has to offer. While most AI systems pose limited to no risk and can contribute to solving many societal challenges, certain AI systems create risks that we must address to avoid undesirable outcomes.\n\nFor example, it is often not possible to find out why an AI system has made a decision or prediction and taken a particular action. So, it may become difficult to assess whether someone has been unfairly disadvantaged, such as in a hiring decision or in an application for a public benefit scheme.\n\nAlthough existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach."}
{"input":"What is the CASC approach?","output":"A central challenge of AI governance that necessitates an application specific regulatory approach is proliferation of ADSs in socioeconomic determinations. Two key interventions, addressing this challenge, are jointly known as the \u201cCASC Approach\u201d:\n - Allowing agencies to demand information to check and review ADSs that impact their responsibilities\n - A new regulatory instrument, the Critical Algorithmic System Classification (CASC), that allows agencies to issue and enforce regulations on ADSs","context":"(C)  technical implementation guides for effective screening; and\n\n               (D)  conformity-assessment best practices and mechanisms.\n\n          (iii)  Within 180 days of the establishment of the framework pursuant to subsection 4.4(b)(i) of this section, all agencies that fund life-sciences research shall, as appropriate and consistent with applicable law, establish that, as a requirement of funding, synthetic nucleic acid procurement is conducted through providers or manufacturers that adhere to the framework, such as through an attestation from the provider or manufacturer.  The Assistant to the President for National Security Affairs and the Director of OSTP shall coordinate the process of reviewing such funding requirements to facilitate consistency in implementation of the framework across funding agencies. Country Assistance Strategies (CAS)\nCAS Progress Reports\nInterim Strategy Notes\nCAS Completion Reports (included as an annex to the follow-on CAS)\nPublic Information Notice for a CAS\nJoint Staff Advisory Notes on Poverty Reduction Strategy Papers (PRSPs)\nCountry financing parameters (including the analyses that underpin their\npreparation)\nEconomic and Sector Work and Non-Lending Technical Assistance reports\nFactual Technical Documents that underpin project preparation\nProject Appraisal Documents (previously Staff Appraisal Reports)\nProject Papers\nProgram Documents for Development Policy Operation (DPOs)\nSupplemental Financing Documents (DPOs)\nTranche Release Documents (DPOs)\nIntegrated Safeguards Data Sheets\nCountry Assessment Reports on the Use of Country Systems (UCS)\nProject Assessments for UCS pilot countries\nImplementation Completion and Results Reports (ICR)\nNote on Cancelled Operations (NCO) (previously Project Completion Note)\nMonthly Loan and Credit Statements (a) 5 years after the date of a document classified as \u201cOfficial Use Only,\u201d and\n(b) 20 years after the date of a document classified as \u201cConfidential,\u201d or \u201cStrictly\nConfidential.\u201d\nThese documents are eligible for declassification if they do not contain or refer to\ninformation that fall under an exception that is not eligible for declassification (see\nparagraph 32 of the policy statement).\nA. Operational documents prepared by the Bank\n\uf0b7 Country Assistance Strategies (CAS)\n\uf0b7 CAS Progress Reports\n\uf0b7 Interim Strategy Notes\n\uf0b7 CAS Completion Reports (included as an annex to the follow-on CAS)\n\uf0b7 Public Information Notice for a CAS\n\uf0b7 Joint Staff Advisory Notes on Poverty Reduction Strategy Papers (PRSPs)\n\uf0b7 Country financing parameters (including the analyses that underpin their\npreparation)\n\uf0b7 Economic and Sector Work and Non-Lending Technical Assistance reports\n\uf0b7 Factual Technical Documents that underpin project preparation\n\uf0b7 Project Appraisal Documents (previously Staff Appraisal Reports) (ii)   Within 180 days of the date of this order, the Secretary of Commerce, acting through the Director of NIST, in coordination with the Director of OSTP, and in consultation with the Secretary of State, the Secretary of HHS, and the heads of other relevant agencies as the Secretary of Commerce may deem appropriate, shall initiate an effort to engage with industry and relevant stakeholders, informed by the framework developed under subsection 4.4(b)(i) of this section, to develop and refine for possible use by synthetic nucleic acid sequence providers:\n\n               (A)  specifications for effective nucleic acid synthesis procurement screening;\n\n               (B)  best practices, including security and access controls, for managing sequence-of-concern databases to support such screening;\n\n               (C)  technical implementation guides for effective screening; and\n\n               (D)  conformity-assessment best practices and mechanisms."}
{"input":"What are some limitations or disadvantages of the CASC approach?","output":"Some of the limitations\/disadvantages of the CASC approach are as follows: - Limited by the pace of the process, and its dependence of rule making makes the process an inherently  retroactive rather than proactive approach - A lengthy regulatory process undermines efficacy of the approach - Additional expertise and staff capacity required within covered agencies for effective regulation execution","context":"(b)  based on input from the process described in subsection 4.6(a) of this section, and in consultation with the heads of other relevant agencies as the Secretary of Commerce deems appropriate, submit a report to the President on the potential benefits, risks, and implications of dual-use foundation models for which the model weights are widely available, as well as policy and regulatory recommendations pertaining to those models. 4.6.  Soliciting Input on Dual-Use Foundation Models with Widely Available Model Weights.  When the weights for a dual-use foundation model are widely available \u2014 such as when they are publicly posted on the Internet \u2014 there can be substantial benefits to innovation, but also substantial security risks, such as the removal of safeguards within the model.  To address the risks and potential benefits of dual-use foundation models with widely available weights, within 270 days of the date of this order, the Secretary of Commerce, acting through the Assistant Secretary of Commerce for Communications and Information, and in consultation with the Secretary of State, shall: (C)  technical implementation guides for effective screening; and\n\n               (D)  conformity-assessment best practices and mechanisms.\n\n          (iii)  Within 180 days of the establishment of the framework pursuant to subsection 4.4(b)(i) of this section, all agencies that fund life-sciences research shall, as appropriate and consistent with applicable law, establish that, as a requirement of funding, synthetic nucleic acid procurement is conducted through providers or manufacturers that adhere to the framework, such as through an attestation from the provider or manufacturer.  The Assistant to the President for National Security Affairs and the Director of OSTP shall coordinate the process of reviewing such funding requirements to facilitate consistency in implementation of the framework across funding agencies. (a) 5 years after the date of a document classified as \u201cOfficial Use Only,\u201d and\n(b) 20 years after the date of a document classified as \u201cConfidential,\u201d or \u201cStrictly\nConfidential.\u201d\nThese documents are eligible for declassification if they do not contain or refer to\ninformation that fall under an exception that is not eligible for declassification (see\nparagraph 32 of the policy statement).\nA. Operational documents prepared by the Bank\n\uf0b7 Country Assistance Strategies (CAS)\n\uf0b7 CAS Progress Reports\n\uf0b7 Interim Strategy Notes\n\uf0b7 CAS Completion Reports (included as an annex to the follow-on CAS)\n\uf0b7 Public Information Notice for a CAS\n\uf0b7 Joint Staff Advisory Notes on Poverty Reduction Strategy Papers (PRSPs)\n\uf0b7 Country financing parameters (including the analyses that underpin their\npreparation)\n\uf0b7 Economic and Sector Work and Non-Lending Technical Assistance reports\n\uf0b7 Factual Technical Documents that underpin project preparation\n\uf0b7 Project Appraisal Documents (previously Staff Appraisal Reports)"}
{"input":"How the United States' approach to AI is influencing the future of AI geopolitics?","output":"Lack of government regulation for private corporations dominating the AI development landscape is a key contributor towards the overall advancement of AI technologies. However, if the US government was to take a more regulatory approach towards AI technology development, we could see significant monopolization of AI resources and talent.","context":"AI and Geopolitics\nHow Might AI Affect the Rise and Fall of Nations?\n\nPublished Nov 3, 2023\n\nby Barry Pavel, Ivana Ke, Michael Spirtas, James Ryseff, Lea Sabbag, Gregory Smith, Keller Scholl, Domenique Lumpkin\n\nNations across the globe could see their power rise or fall depending on how they harness and manage the development of artificial intelligence (AI). Regardless of whether AI poses an existential risk to humanity, governments will need to develop new regulatory frameworks to identify, evaluate, and respond to the variety of AI-enabled challenges to come. Although technology has often influenced geopolitics, the prospect of AI means that the technology itself could become a geopolitical actor. AI could have motives and objectives that differ considerably from those of governments and private companies. Humans' inability to comprehend how AI \"thinks\" and our limited understanding of the second- and third-order effects of our commands or requests of AI are also very troubling. Humans have enough trouble interacting with one another. It remains to be seen how we will manage our relationships with one or more AIs. As AI continues to advance, geopolitics may never be the same. Humans organized in nation-states will have to work with another set of actors\u2014AI-enabled machines\u2014of equivalent or greater intelligence and, potentially, highly disruptive capabilities. In the age of geotechnopolitics, human identity and human perceptions of our roles in the world will be distinctly different; monumental scientific discoveries will emerge in ways that humans may not be able to comprehend. Consequently, the AI development path that ultimately unfolds will matter enormously for the shape and contours of the future world. With the release of advanced forms of AI to the public early in 2023, public policy debates have rightly focused on such developments as the exacerbation of inequality, the loss of jobs, and the potential threat of human extinction if AI continues to evolve without effective guardrails. There has been less discussion about how AI might affect geopolitics and which actors might take the lead in the future development of AI or other advanced AI algorithms."}
{"input":"What is China\u2019s contribution to the advancement of AI?","output":"China has emerged as a leading force in AI, as key Chinese organizations (Huawei, Baidu and Beijing Academy of Artificial Intelligence) announced groundbreaking achievements. These announcements are representative of China\u2019s substantial investment in AI research and strategic partnerships. Through vast data resources and collaborative approaches, China\u2019s AI landscape may be a potential threat to US\u2019s global AI power dynamics.","context":"Against this backdrop, a report by PwC suggested that China is the country that has the most to gain from AI, with a boost in GDP of up to 26% by 2030 (\u201cSizing the Prize\u201d 2017). Estimates also suggest that AI could facilitate an increase in employment by 12% over the next two decades (\u201cNet Impact of AI on Jobs in China?\u201d 2018). Because of these potential benefits, President Xi has frequently spoken of the centrality of AI to the country\u2019s overall economic development (Hickert and Ding 2018; Kania 2018b). China has been pursuing the potential economic benefits of AI concretely and proactively for some time. For example, there has been a 500% increase in the annual installation of robotic upgrades since 2012. This rate is staggering, especially when compared to a rate of just over 100% in Europe (Shoham et al. 2018), equating to over double the number of robot installations in China than Europe. mentioned AI as one of the six critical areas for developing the country\u2019s emerging industries (CCP 2016), and as an important factor in stimulating economic growth. When read together, these documents indicate that there has been a conscious effort to develop and use AI in China for some time, even before \u2018the Sputnik moment\u2019. However, prior to 2016, AI was presented merely as one technology among many others, which could be useful in achieving a range of policy goals. This changed with the release of the AIDP. A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. The AIDP provides a longitudinal perspective on China\u2019s strategic situation regarding AI, including its comparative capabilities, the opportunities offered, and the potential risks. Following a technology-first approach, it may be tempting to concentrate one\u2019s attention on the stated capabilities of AI, to gain an insight into the types of technologies in which China is investing. However, this would likely offer only a short-term perspective and would soon be out of date as technological innovation advances rapidly. Furthermore, it would do little to explain why China is seeking to develop a strong AI sector in the decades to come. To this end, it is more useful to try to understand China\u2019s strategic focus from a policy-first approach, by analysing the areas where China considers that AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the"}
{"input":"What are some key challenges being faced by governments for AI regulation?","output":"Governments encounter difficulties in regulating AI due to its borderless nature, rapid pace of technological advancement, and lack of traditional chokepoints for regulation. To address these challenges, governments could adopt strategies focusing on resilience-building, expand regulatory toolboxes beyond traditional methods, partner with the private sector to improve risk assessments, and continue supporting innovation in AI.","context":"Governments should look beyond traditional regulatory techniques to influence AI developments So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground"}
{"input":"What are the key themes regarding emerging technologies covered in the 2023, G7 summit?","output":"The summit focused on responsible AI governance, aligning AI development with values like democracy and human rights, addressing potential risks associated with AI, promoting international collaboration and interoperability in AI governance frameworks.","context":"The Path to Trustworthy AI G7 Outcomes and Implications for Global AI Governance\n\nCommentary by Hiroki Habuka\n\nPublished June 6, 2023\n\nIntroduction\nThe G7 Summit, held from May 19 to 21, 2023, left a strong impression of unity among global leaders. Emerging technology such as artificial intelligence (AI), along with national security and energy, was highlighted as a key area requiring a strong alliance among G7 countries toward fundamental values such as democracy and human rights. The G7 Digital and Tech Ministers\u2019 Meeting, which took place a little earlier, also discussed responsible AI and global AI governance as one of the central topics. With the recent launch of large language AI models such as GPT-4, society anticipates a rapid integration of AI technologies, making it crucial to discuss the responsible use of AI and its governance. However, even the like-minded G7 countries approach AI governance differently, ranging from strict, comprehensive laws to sector-specific guidelines. Because of this, the consensus among G7 leaders on trustworthy AI will have a big impact on the way AI systems should be developed and operated around the world. This commentary takes a close look at two important outcome documents from the 2023 G7 summit that broach the subject of AI governance: the G7 Summit Communiqu\u00e9 and the Declaration of the G7 Digital and Tech Ministers\u2019 Meeting\u2014 collectively referred to as the G7 statements\u2014to predict potential cooperation on AI governance among the G7 countries.\n\nShared Understanding of Values, Principles, and Risks\nEstablishing Common Values G7 Leaders issue \u2018Guiding Principles\u2019 and voluntary \u2018Code of Conduct\u2019 on AI\nOn Monday, Group of Seven (G7) leaders issued a set of international Guiding Principles on artificial intelligence as well as a voluntary Code of Conduct for AI developers under the \u2018Hiroshima Artificial Intelligence Process.\u2019 In a joint statement, leaders called on organizations developing advanced AI systems to commit to the code of conduct as well as ministers to accelerate development of the Process\u2019 policy framework.\n\nThe Principles and Code are the culmination of work that began in May 2023 at the G7 summit, where the Hiroshima AI Process \u2013 born out of the EU-US Trade and Technology Council ministerial \u2013 was formed to find common policy priorities and promote guardrails for advanced AI systems globally. The G7 bloc includes Canada, France, Germany, Italy, Japan, Britain, the US, and the European Union. Conclusion\nThe G7 statements have highlighted the shared commitment to developing and implementing trustworthy AI that upholds values such as human rights and democracy. The statements also acknowledge risks including online abuse, threats to privacy, misuse of AI, and IP concerns. The G7 leaders have agreed to support the establishment of an interoperable governance framework to counteract potential gaps and fragmentation in global technology governance. They have also committed to assisting the efforts by organizations such as OECD and GPAI, and to collaborate in the development of international technical standards under SDOs."}
{"input":"How do international organizations and multi-stakeholder initiatives contribute to the development of trustworthy AI governance frameworks?","output":"Organizations such as OECD and UNESCO, along with initiatives like GPAI play an integral role in developing tools, regulations, technical standards and assurance techniques for trustworthy AI. Countries can then look at these initiatives as benchmarks in establishing regulatory frameworks and allocating civil liabilities.","context":"As the word \u201cstakeholder\u201d frequently appears in the G7 statements, the future of AI governance will likely not rely solely on top-down, government-led rule-setting. Instead, it suggests an approach where multi-stakeholder initiatives are discussed on a case-by-case basis and updated in an agile manner. Regulations and technical standards serve as critical tools to facilitate these efforts in building trustworthy AI. However, it is crucial to note that the key players in AI governance, who develop and use these tools, will be not only the government entities, but rather the private stakeholders including AI developers, users, and civil society organizations, and so on. The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d The Road Ahead: Achieving Interoperability in AI Governance Frameworks\nTo maximize the shared values while mitigating the aforementioned risks, the G7 statements highlighted the necessity of interoperability among AI governance frameworks to foster trustworthy AI. This list outlines what exactly such a framework entails:\n\nRisk-Based and Forward-Looking Approaches under Different Policy Frameworks These endeavors have a direct link to trustworthy AI. For instance, having trust in the dataset used to train an AI system is crucial to ensuring its transparency and accountability. Moreover, the question of how to build trust in data generated by AI systems is an inevitable challenge in realizing the DFFT. Discussions on how to make such trustworthy AI ecosystems will be promoted under the umbrella of the IAP in partnership with the public and private sectors.\nImplementing Agile, Distributed, and Multi-stakeholder Governance through Policy Incentives"}
{"input":"What role do principles such as fairness, accountability, transparency, and safety play in shaping international discussions on AI governance?","output":"Principles such as fairness, accountability, transparency, and safety are central to international discussions on AI governance, reflecting shared values among nations. These principles guide the development of regulatory frameworks, risk assessment processes, and multi-stakeholder initiatives aimed at promoting responsible AI development and deployment.","context":"Inclusive growth, sustainable development and well-being. AI should benefit people and the planet.\nHuman-centered values and fairness. AI systems should be designed in a way that respects the rule of law, human rights, democratic values and diversity, and they should include appropriate safeguards \u2013 for example, enabling human intervention when necessary \u2013 to ensure a fair and just society.\nTransparency and explainability. There should be transparency and a responsible disclosure around AI systems to ensure that people understand AI-based outcomes and can challenge them.\nRobustness, security and safety. AI systems must function in a robust, secure and safe way throughout their life cycles and potential risks should be continually assessed and managed.\nAccountability. Organizations and individuals developing, deploying or operating AI systems should be held accountable for their proper functioning in line with the above principles.\nINTERNATIONAL LAWS 6\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nMost advanced digital governments have issued governance frameworks,\nincluding ethical principles for the use of AI. Governance models incorporate three aspects: ethical principles, the role of a central agency, and\noperational framework.\nNational governments, including Australia, Canada, China, Japan, Singapore,\nUnited Arab Emirates, and the United States as well as international organizations including the European Commission, the Institute of Electrical and Electronics Engineers, International Organization for Standardization, United Nations, and World Economic Forum, are actively proposing governance models\nfor AI that emphasize common principles:\n\nEthical Principles\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\nPrivacy and data protection\nAccountability\nSafety and security\nTransparency and explainability\nFairness\nHuman control of technology\nProfessional responsibility\nPromotion of human values\n\n\u2022 Accountability and governance: Businesses should put in place governance measures that ensure effective oversight of the supply and use of AI systems, with clear lines of accountability established across the AI lifecycle. Regulators will likely implement this principle through regulatory guidance and assurance techniques.\nContestability and redress: Users, impacted third parties, and actors in the AI lifecycle should be able to contest an AI decision or outcome that is harmful or creates material risk of harm. Regulators will be expected to clarify existing routes to contestability and redress and implement proportionate measures to ensure that the outcomes of AI use are contestable where appropriate. The government\u2019s initial non-statutory approach will not create new rights or new routes to redress at this stage. Right to Transparency. All individuals have the right to know the basis of an AI decision that concerns them. This includes access to the factors, the logic, and techniques that produced the outcome.\nRight to Human Determination. All individuals have the right to a final determination made by a person.\nIdentification Obligation. The institution responsible for an AI system must be made known to the public.\nFairness Obligation. Institutions must ensure that AI systems do not reflect unfair bias or make impermissible discriminatory decisions.\nAssessment and Accountability Obligation. An AI system should be deployed only after an adequate evaluation of its purpose and objectives, its benefits, as well as its risks. Institutions must be responsible for decisions made by an AI system.\nAccuracy, Reliability, and Validity Obligations. Institutions must ensure the accuracy, reliability, and validity of decisions."}
{"input":"What is the U.S approach to AI risk management?","output":"The U.S. approach to AI risk management is characterized as risk-based, sectorally specific, and highly distributed across federal agencies. It relies on a patchwork of regulatory interventions and non-binding guidance documents. While these documents outline a risk-based approach and urge agencies to consider key facets of AI risk reduction, such as using scientific evidence, enforcing non-discrimination statutes, and promoting safe AI development and deployment, federal agencies have been slow to develop the required AI regulatory plans. The Biden administration revisited the topic of AI risks through the Blueprint for an AI Bill of Rights (AIBoR), endorsing a sectorally specific approach to AI governance. However, the AIBoR is nonbinding, and its principles have not led to a consistent federal approach to AI risks.","context":"Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. (c)  Within 365 days of the date of this order, to promote safe, responsible, and rights-affirming development and deployment of AI abroad:\n\n          (i)   The Secretary of State and the Administrator of the United States Agency for International Development, in coordination with the Secretary of Commerce, acting through the director of NIST, shall publish an AI in Global Development Playbook that incorporates the AI Risk Management Framework\u2019s principles, guidelines, and best practices into the social, technical, economic, governance, human rights, and security conditions of contexts beyond United States borders.  As part of this work, the Secretary of State and the Administrator of the United States Agency for International Development shall draw on lessons learned from programmatic uses of AI in global development. Risk-based approach. AI is a general-purpose technology that is likely to be ubiquitous and apply to a wide range of applications. This makes it harder to regulate well. Many propose a risk-based approach where higher-risk AI applications that can cause greater damage have proportionately greater regulation and lower-risk applications have less. This has merits, but there are many implementation obstacles including who and how risk is defined and measured, what are the minimum requirements to offer AI services, and what AI uses are deemed illegal. Further, Google argues that focusing regulation on the highest risks may deter innovation that also has the highest value to society. Risk Management, Standards, Audits and Assessments. The Commerce Department\u2019s NIST plays a key role in defining AI standards and risk management practices. They can have a wide impact, but they are mostly voluntary. Commerce\u2019s NTIA AI inquiry is focused on \u201cbuild(ing) an ecosystem of AI audits, assessments and other mechanisms,\u201d that are likely part of the US response. They are key to enforcement. These efforts can play an important role globally. \n\nU.S. states and cities likely pass targeted actions. There is a real but lower chance that a big state like California passes a major AI law, substantially altering the U.S. regulatory environment. \n\nDomain Specific Outcomes"}
{"input":"What is the EU's  approach to AI risk management?","output":"The EU's approach to AI risk management is comprehensive and multifaceted, incorporating various legislative measures and regulatory frameworks. The General Data Protection Regulation (GDPR) contains clauses related to algorithmic decision-making, including requirements for human supervision and the right to meaningful information about the logic of algorithmic systems. Additionally, the EU is enacting new legislation such as the Digital Services Act (DSA), Digital Markets Act (DMA), and the proposed AI Act. The AI Act, which is still under discussion, introduces a tiered system of regulatory obligations for different AI applications, including high-risk AI systems. It mandates standards for data quality, accuracy, robustness, non-discrimination, and human oversight, with significant fines for non-compliance. The EU's approach emphasizes transparency, accountability, and regulatory oversight across various digital environments.","context":"Under this fragmentation in AI governance approach, the G7 members agreed that policies and regulations should be risk-based and forward-looking. However, policymakers should be cautious about using the term \u201crisk-based.\u201d The EU AI Act defines risk-based as the classification of AI systems into four categories: unacceptable, high-risk, limited, or minimal\/low, depending on the purposes and actors using the system (e.g., social scoring by governments is classified as \u201cunacceptable,\u201d while critical infrastructures that could put the life and health of citizens at risk are classified as \u201chigh risk\u201d). In contrast, the U.S. NIST AI Risk Management Framework assumes that risk is assessed for each AI system based on the magnitude of risk and probability of occurrence. In short, the term risk-based varies from country to country, and its common meaning may not differ significantly from the general principle of regulation: necessary and proportional. Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7]"}
{"input":"What are emerging challenges in transatlantic AI Risk Management?","output":"Emerging challenges in transatlantic AI risk management include regulatory misalignment between the EU and the US, with the EU having comprehensive platform governance acts while the US lacks similar legislation. This discrepancy poses potential conflicts for multinational digital platforms operating in both regions. Additionally, the shifting nature of AI deployment, such as the emergence of large AI models and techniques like edge and federated machine learning, raises concerns about regulatory compliance across international borders. Close collaboration and alignment are crucial to navigating these challenges effectively and ensuring the responsible development and deployment of AI technologies.","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d (i)   lead efforts outside of military and intelligence areas to expand engagements with international allies and partners in relevant bilateral, multilateral, and multi-stakeholder fora to advance those allies\u2019 and partners\u2019 understanding of existing and planned AI-related guidance and policies of the United States, as well as to enhance international collaboration; and\n\n          (ii)  lead efforts to establish a strong international framework for managing the risks and harnessing the benefits of AI, including by encouraging international allies and partners to support voluntary commitments similar to those that United States companies have made in pursuit of these objectives and coordinating the activities directed by subsections (b), (c), (d), and (e) of this section, and to develop common regulatory and other accountability principles for foreign nations, including to manage the risk that AI systems pose. Governments should partner with the private sector to improve risk assessments\nIn light of the likely very widespread proliferation of advanced AI capabilities to private- and public-sector actors and well-resourced individuals, governments should work closely with leading private-sector entities to develop advanced forecasting tools, wargames, and strategic plans for dealing with what experts anticipate will be a wide variety of unexpected AI-enabled catastrophic events. (c)  Within 365 days of the date of this order, to promote safe, responsible, and rights-affirming development and deployment of AI abroad:\n\n          (i)   The Secretary of State and the Administrator of the United States Agency for International Development, in coordination with the Secretary of Commerce, acting through the director of NIST, shall publish an AI in Global Development Playbook that incorporates the AI Risk Management Framework\u2019s principles, guidelines, and best practices into the social, technical, economic, governance, human rights, and security conditions of contexts beyond United States borders.  As part of this work, the Secretary of State and the Administrator of the United States Agency for International Development shall draw on lessons learned from programmatic uses of AI in global development."}
{"input":"How has the White House Office of Science and Technology Policy (OSTP) attempted to address responsible AI policy, and what challenges remain in enforcing the Blueprint for an AI Bill of Rights?","output":"The OSTP published a Blueprint for an AI Bill of Rights outlining core principles for responsible AI use. However, challenges persist in determining how grievances will be reprimanded and if the non-binding document will prompt congressional action.","context":"Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. (iv)    required minimum risk-management practices for Government uses of AI that impact people\u2019s rights or safety, including, where appropriate, the following practices derived from OSTP\u2019s Blueprint for an AI Bill of Rights and the NIST AI Risk Management Framework:  conducting public consultation; assessing data quality; assessing and mitigating disparate impacts and algorithmic discrimination; providing notice of the use of AI; continuously monitoring and evaluating deployed AI; and granting human consideration and remedies for adverse decisions made using AI;\n\n          (v)     specific Federal Government uses of AI that are presumed by default to impact rights or safety;\n\n          (vi)    recommendations to agencies to reduce barriers to the responsible use of AI, including barriers related to information technology infrastructure, data, workforce, budgetary restrictions, and cybersecurity processes; Monday\u2019s White House action builds on its previous commitments to the safe and responsible development of AI. Some of these include its support for developing the Blueprint for an AI Bill of Rights, which safeguards Americans\u2019 rights and safety; securing voluntary commitments from Anthropic, Google, OpenAI, and other tech companies to thoroughly test systems before releasing them \u2013 also called \u201cred-teaming\u201d \u2013 and clearly labeling all AI-generated content; and granting $140 million to the National Science Foundation to establish seven new AI research institutes. The Order is also meant to complement the G7 leaders\u2019 Guiding Principles and Code of Conduct, which was also issued on Monday. (See below for more details.) OSTP published a \u201cBlueprint for an AI Bill of Rights,\u201d emphasizing a set of five principles and practices to help guide the design, use, and deployment of AI systems.\n2023\nThe NAIRR Task Force finalized an implementation plan\u2014\u201cStrengthening and Democratizing the U.S. Artificial Intelligence Innovation Ecosystem\u201d\u2014for how the federal government will achieve a democratic AI cyberinfrastructure. The plan includes a section on \u201cData and Datasets\u201d and calls for \u201canalysis-ready\u201d data sets to be defined using existing community-driven principles and standards.\nWhere is AI-ready data being applied?\nThere are at least three significant efforts within federal agencies to improve the AI-readiness of open data."}
{"input":"Which agencies have begun to adopt the blueprint\/guidelines?","output":"The Department of Defense (DOD): Implemented Ethical Principles for Artificial Intelligence. The U.S. Agency for International Development (USAID): Developed an Artificial Intelligence Action Plan. The Equal Employment Opportunity Commission (EEOC): Launched an AI and algorithmic fairness initiative in partnership with the Department of Labor. The Department of Energy (DOE): Established its own office to implement guidelines. The Department of Veterans Affairs (VA): Established its own office to implement guidelines. The Department of Health and Human Services (HHS): Established its own office to implement guidelines.","context":"to promote consistent AI-ready data standards. Further, just as agencies undertook a \u201cprivacy review step\u201d to ensure that data sets released to Data.gov did not unintentionally reveal personally identifiable information, a \u201cnutrition label\u201d could ensure data sets are transparent about inherent biases. Because standards-setting is so time intensive, the challenge is to make standards flexible enough to withstand evolving technology. Moreover, as data that agencies collect are often global and agencies use data from international partners, international collaboration is critical to ensure that any standard is interoperable across countries. (iv)   Within 240 days of the completion of the guidelines described in subsection 4.3(a)(iii) of this section, the Assistant to the President for National Security Affairs and the Director of OMB, in consultation with the Secretary of Homeland Security, shall coordinate work by the heads of agencies with authority over critical infrastructure to develop and take steps for the Federal Government to mandate such guidelines, or appropriate portions thereof, through regulatory or other appropriate action.  Independent regulatory agencies are encouraged, as they deem appropriate, to consider whether to mandate guidance through regulatory action in their areas of authority and responsibility. (ix)    requirements for public reporting on compliance with this guidance.\n\n     (c)  To track agencies\u2019 AI progress, within 60 days of the issuance of the guidance established in subsection 10.1(b) of this section and updated periodically thereafter, the Director of OMB shall develop a method for agencies to track and assess their ability to adopt AI into their programs and operations, manage its risks, and comply with Federal policy on AI.  This method should draw on existing related efforts as appropriate and should address, as appropriate and consistent with applicable law, the practices, processes, and capabilities necessary for responsible AI adoption, training, and governance across, at a minimum, the areas of information technology infrastructure, data, workforce, leadership, and risk management.  \n\n     (d)  To assist agencies in implementing the guidance to be established in subsection 10.1(b) of this section: (iv)   Within 365 days of the date of this order, the Secretary of HHS shall, in consultation with the Secretary of Defense and the Secretary of Veterans Affairs, establish an AI safety program that, in partnership with voluntary federally listed Patient Safety Organizations:\n\n               (A)  establishes a common framework for approaches to identifying and capturing clinical errors resulting from AI deployed in healthcare settings as well as specifications for a central tracking repository for associated incidents that cause harm, including through bias or discrimination, to patients, caregivers, or other parties; \n\n               (B)  analyzes captured data and generated evidence to develop, wherever appropriate, recommendations, best practices, or other informal guidelines aimed at avoiding these harms; and\n\n               (C)  disseminates those recommendations, best practices, or other informal guidance to appropriate stakeholders, including healthcare providers."}
{"input":"What specific sectors or domains regarding civil rights are emphasized in the blueprint?","output":"The civil rights of interest highlighted by the blueprint primarily revolve around lending, housing, and hiring.","context":"Sec. 7.  Advancing Equity and Civil Rights.\n\n     7.1.  Strengthening AI and Civil Rights in the Criminal Justice System.  (a)  To address unlawful discrimination and other harms that may be exacerbated by AI, the Attorney General shall:\n\n          (i)    consistent with Executive Order 12250 of November 2, 1980 (Leadership and Coordination of Nondiscrimination Laws), Executive Order 14091, and 28 C.F.R. 0.50-51, coordinate with and support agencies in their implementation and enforcement of existing Federal laws to address civil rights and civil liberties violations and discrimination related to AI; This important progress must not come at the price of civil rights or democratic values, foundational American principles that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the President ordered the full Federal government to work to root out inequity, embed fairness in decision-making processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.[i] The President has spoken forcefully about the urgent challenges posed to democracy today and has regularly called on people of conscience to act to preserve civil rights\u2014including the right to privacy, which he has called \u201cthe basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this country.\u201d[ii] 7.2.  Protecting Civil Rights Related to Government Benefits and Programs.  (a)  To advance equity and civil rights, consistent with the directives of Executive Order 14091, and in addition to complying with the guidance on Federal Government use of AI issued pursuant to section 10.1(b) of this order, agencies shall use their respective civil rights and civil liberties offices and authorities \u2014 as appropriate and consistent with applicable law \u2014 to prevent and address unlawful discrimination and other harms that result from uses of AI in Federal Government programs and benefits administration.  This directive does not apply to agencies\u2019 civil or criminal enforcement authorities.  Agencies shall consider opportunities to ensure that their respective civil rights and civil liberties offices are appropriately consulted on agency decisions regarding the design, development, acquisition, and use of AI in Federal Government programs and benefits administration.  To further these Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed."}
{"input":"Which two categories can Japan\u2019s AI regulations be classified under?","output":"Regulation for AI: Regulatory reform to promote the implementation of AI. Regulation on AI: Regulations to manage the risks associated with AI. ","context":"2 AI legislation in China So far, most of the activity in the crafting of AI regulations has occurred in the third and fourth layers.\n\nReal-World Roots\nThis layer is composed of the economic, political, social, and technological conditions that create the need for new policy and also limit the options for regulators. Like public policy anywhere in the world, Chinese AI regulations often get their initial impetus from an exogenous shift in the real world. This can be a major evolution in technological capabilities, a new business model emerging, or a shift in underlying social or political conditions in the country. Such changes provide a spark, a problem that needs to be addressed through a change in public policy. The other components of this layer\u2014economic, political and social conditions\u2014then help set the scope of what is possible with a regulation and what costs are acceptable. China\u2019s regulations are not exclusively focused on algorithms. The registry includes requirements to disclose the sources of training data, and the draft generative AI regulation has specific requirements on the data\u2019s diversity and \u201cobjectivity.\u201d Many other requirements, such as that AI-generated content \u201creflect Socialist Core Values,\u201d are defined based on outcomes rather than technical specifics. Where regulators focus their interventions will be an important component of Chinese AI governance going forward.\nBuilding Regulatory Tools and Bureaucratic Know-How\nChina\u2019s initial forays into governing AI have built up specific regulatory tools and broader bureaucratic know-how that can be deployed in future regulations. The algorithm registry is a standardized disclosure tool that ministries can easily include in future regulations, refin-ing its requirements as needed. The information currently disclosed\u2014such as data sources International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules."}
{"input":"What did the Ministry of Economy, Trade, and Industry (METI) state in regards to AI regulation in their July 2021 report?","output":"They stated \u201clegally-binding horizontal requirements for AI systems are deemed unnecessary at the moment.","context":"But the MOST\u2019s profile may rise again as regulatory attention turns toward the underlying technology, as in the draft generative AI regulation, which imposes requirements on model training.\n\nBeyond the CAC and the MOST, three of the more significant bureaucratic bodies are the MIIT, the Ministry of Public Security (MPS), and the State Administration of Market Reform (SAMR). The MIIT and the MPS have co-signed both the recommendation algorithm and deep synthesis regulations, while SAMR signed only the former. Each of these organizations will likely continue playing a significant role in regulations that touch on their respective areas. The MIIT, in particular, will likely take on a greater role as AI regulation moves from online content to industrial and commercial applications of the technology.\n\nAbove all these ministries sits China\u2019s State Council and the National People\u2019s Congress. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA). The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. Future-proof legislation:\nAs AI is a fast evolving technology, the proposal has a future-proof approach, allowing rules to adapt to technological change. AI applications should remain trustworthy even after they have been placed on the market. This requires ongoing quality and risk management by providers.\n\nNext steps:\nFollowing the Commission\u2019s proposal in April 2021, the regulation could enter into force late 2022\/early 2023 in a transitional period. In this period, standards would be mandated and developed, and the governance structures set up would be operational. The second half of 2024 is the earliest time the regulation could become applicable to operators with the standards ready and the first conformity assessments carried out."}
{"input":"What is a possible step for collaboration amongst the G7 countries in regards to AI policy?","output":"A relatively easy step would be to set up an AI incidents database that spans multiple countries.","context":"With the recent launch of large language AI models such as GPT-4, society anticipates a rapid integration of AI technologies, making it crucial to discuss the responsible use of AI and its governance. However, even the like-minded G7 countries approach AI governance differently, ranging from strict, comprehensive laws to sector-specific guidelines. Because of this, the consensus among G7 leaders on trustworthy AI will have a big impact on the way AI systems should be developed and operated around the world. This commentary takes a close look at two important outcome documents from the 2023 G7 summit that broach the subject of AI governance: the G7 Summit Communiqu\u00e9 and the Declaration of the G7 Digital and Tech Ministers\u2019 Meeting\u2014 collectively referred to as the G7 statements\u2014to predict potential cooperation on AI governance among the G7 countries.\n\nShared Understanding of Values, Principles, and Risks\nEstablishing Common Values The Path to Trustworthy AI G7 Outcomes and Implications for Global AI Governance\n\nCommentary by Hiroki Habuka\n\nPublished June 6, 2023\n\nIntroduction\nThe G7 Summit, held from May 19 to 21, 2023, left a strong impression of unity among global leaders. Emerging technology such as artificial intelligence (AI), along with national security and energy, was highlighted as a key area requiring a strong alliance among G7 countries toward fundamental values such as democracy and human rights. The G7 Digital and Tech Ministers\u2019 Meeting, which took place a little earlier, also discussed responsible AI and global AI governance as one of the central topics. AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. | 15\nincluding the Council of Europe, the EU, the Global Partnership on AI (GPAI), the\nGroup of Seven (G7), the Group of Twenty (G20), the Inter-American\nDevelopment Bank (IDB), the International Telecommunications Union (ITU),\nthe OECD, the United Nations, the United Nations Educational, Scientific and\nCultural Organization (UNESCO) and the World Bank.\nCross-border research on AI is also significant. For example, the French National\nResearch Agency, the German Research Foundation and the Japan Science and\nTechnology Agency have called for trilateral French-German-Japanese\ncollaborative research on AI over three years (2019-2021). In 2020, the United\nKingdom and the United States signed a declaration on co-operation for AI R&D\nthrough which they plan to drive technological breakthroughs, promote\nresearch collaboration and advance the development of trustworthy AI.\nMany European Union member states are also participating in European AI"}
{"input":"What is the name of the executive order released by the White House on October 30 regarding AI?","output":"The executive order is called the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence (EOAI)","context":"President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated. U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. Monday\u2019s White House action builds on its previous commitments to the safe and responsible development of AI. Some of these include its support for developing the Blueprint for an AI Bill of Rights, which safeguards Americans\u2019 rights and safety; securing voluntary commitments from Anthropic, Google, OpenAI, and other tech companies to thoroughly test systems before releasing them \u2013 also called \u201cred-teaming\u201d \u2013 and clearly labeling all AI-generated content; and granting $140 million to the National Science Foundation to establish seven new AI research institutes. The Order is also meant to complement the G7 leaders\u2019 Guiding Principles and Code of Conduct, which was also issued on Monday. (See below for more details.) On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI"}
{"input":"How does the EOAI contribute to U.S. leadership in AI governance?","output":"The EOAI mobilizes the federal government to develop guidelines, principles, and reports on AI use and development, enhancing U.S. leadership in AI governance.","context":"On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d The EOAI also calls for the accelerated development of AI standards with international partners in standards organizations. The EOAI along with the other domestic AI policies outlined will have significant international impacts through various channels.\n\nFirst and most immediately, these suites of domestic policy developments on AI governance give Vice President Kamala Harris as head of the U.S. delegation to the U.K. AI Safety Summit a real opportunity to lead the further development of international AI governance. The U.S. position on AI stands in stark contrast to the lack of strong U.S. leadership on privacy regulation, where the absence of federal privacy legislation created a vacuum that the EU\u2019s General Data Protection Regulation (GDPR) filled, allowing GDPR to become a leading model for privacy regulation worldwide. The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d"}
{"input":"What is the purpose of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)?","output":"The FCAI focuses on identifying opportunities for international cooperation on AI, especially in light of the pace and scope of domestic AI governance mechanisms being developed globally.","context":"The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d 1Note: The views expressed in this brief are those of the author and do not necessarily reflect those of the United Nations or its\nsenior management.\n\f   Science-Policy Brief for the Multistakeholder Forum on Science, Technology and Innovation for the SDGs, May 2022\n\n                                                             Table 1. Basic AI functions\n\n                                                                    Basic AI functions\n                         It can \u201csee\u201d      It can \u201chear\u201d             It can \u201cread\u201d         It can move by     It can \u201creason\u201d\n                         and identify                                                      itself, based on     and looks for\n        What it can do 24 |\nThe High-Level Expert Group on Artificial Intelligence (AI HLEG)\nResponsible entity: European Commission (EC)\nDescription: The High-Level Expert Group on Artificial Intelligence (AI HLEG)\nwas convened by the European Commission in 2016 to support the\nimplementation of the European Strategy on AI and completed its mandate in\nJuly 2020. The AI HLEG comprised representatives from academia, civil society,\nand industry that produced three outputs: the April 2019 Ethics Guidelines for\nTrustworthy Artificial Intelligence, the June 2019 Policy and Investment\nRecommendations for Trustworthy Artificial Intelligence, and the July 2020\nAssessment List for Trustworthy Artificial Intelligence (ALTAI). The European AI\nAlliance is a multi-stakeholder forum for engaging in a broad and open\ndiscussion of all aspects of AI development and its impact on the economy and\nsociety.\nIn February 2020, the European Commission issued a \u201cWhite Paper on Artificial AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance."}
{"input":"What were the recent rounds of semiconductor related export controls announced by the Biden administration?","output":"The recent round of semiconductor-related export controls announced by the Biden administration includes measures aimed at restricting China's access to advanced technologies from companies like Nvidia and AMD, blocking the sale of high-end AI chips, and limiting China's ability to design chips using U.S.-made software from companies such as Mentor Graphics, Cadence Design Systems, and Synopsys. Additionally, restrictions target semiconductor manufacturing equipment, affecting companies like SMIC and YMTC, and aim to prevent China from developing its own equipment by restricting access to U.S.-built components.","context":"More recently, on October 7, the Biden administration enacted a major set of export controls that restricted the sale of AI and semiconductor technology to China. The new export controls dramatically reduce China\u2019s prospects for becoming a superpower in AI technology and likewise reduce China\u2019s prospects for being self-sufficient in semiconductor technology.\n\nWhen targeted correctly and enforced effectively, export controls are a powerful tool of foreign policy, as seen in the 2018 restrictions on semiconductor sales to ZTE and Huawei. On October 7, 2022, the Department of Commerce announced major changes to export control policies toward China for the procurement of advanced AI and semiconductor technology used in both commercial and military AI systems. This policy document included important acknowledgments that previous export control approaches\u2014based on restricting the sale of dual-use technologies to military end users or for military end uses\u2014had diminished in effectiveness over the past decade. Specifically, the new policy stated that controls prior to October 7 \u201cgenerally only apply when the \u2018U.S. person\u2019 has knowledge that their activities are contributing to prohibited end uses or end users. China\u2019s military-civil fusion effort makes it more difficult to tell which items are made for restricted end uses, thereby diminishing the effect of these existing controls.\u201d While the CSET study was focused on the recent procurement of AI chips, the U.S. government has been expressing concern for years that China is too often succeeding in evading export controls on a much broader range of technologies. BIS has some flexibility to change its approach to regulation in order to make enforcement easier and more effective, as it did with the October 7 regulatory changes, but only Congress can appropriate additional resources for export license application review and export control enforcement.\n\nGiven the dramatic recent expansion of U.S. and allied export controls against Russia and China, these countries are certain to be massively increasing efforts to evade such controls.\n\nTwo of the largest expansions of U.S. and allied export controls in decades occurred in 2022: those targeting Russia in response to its invasion of Ukraine and those targeting China in response to its AI military modernization efforts and military-civil fusion policy. In October 2022, the Biden administration further expanded the scope of U.S. export controls when it released two new rules aimed at severely restricting Chinese artificial intelligence (AI) and semiconductor capabilities. A recent CSIS report, Choking Off China\u2019s Access to the Future of AI, assesses these controls in detail. What is novel about these controls is that they represent a sea change in the U.S. approach to China, particularly since the controls are both geographic in nature and unilateral. The administration has also indicated recently that it is shifting from a policy focused on delaying adversaries\u2019 technological advancements to one of directly degrading them. Though less severe than those imposed on Russia, these new export controls are intended to prevent China from achieving key strategic priorities. Both Russia and China are doubtless devoting massively increased resources to evading the new export controls."}
{"input":"What are the criticisms of the export controls?","output":"Critics of the export controls argue that while they aim to curb China's technological advancement, they may inadvertently harm U.S. companies and global innovation. Some contend that the restrictions could lead to reduced revenues for American chip manufacturers like Nvidia and AMD, while also potentially spurring China to accelerate its efforts to develop indigenous semiconductor technologies. Additionally, critics express concerns about the broader economic implications, including potential disruptions to global supply chains and increased tensions in U.S.-China relations. Moreover, there are fears that the controls could stifle collaboration and innovation in the semiconductor industry, ultimately hindering progress in AI and other emerging technologies.","context":"Private sector compliance resources are critical for the viability of U.S. export controls, but they are not a substitute for robust internal capabilities at BIS. As depicted in the simplified process in Figure 1, the export controls process begins with an entity (usually a company) submitting an export license application that provides information about the seller, buyer, and goods or services being sold. A company employee will draft the license application after having made an internal determination about the believed legality of the transaction. BIS holds numerous annual training and informational seminars in order to clarify the existing regulations and policies that companies are subject to. Thus, the most significant resources devoted to export controls are actually performed by private sector entities as part of their compliance and due diligence activities. Improved Export Controls Enforcement Technology Needed for U.S. National Security\nExecutive Summary\nAs technology has become increasingly central to strategic competition with Russia and China, export controls have moved to the forefront of U.S. foreign policy on technology issues. Most notably, restricting Russia\u2019s access to advanced technology through export controls is a key part of the U.S. response to Russia\u2019s invasion of Ukraine, as U.S. government officials have repeatedly stated.\n\nUnfortunately, nearly all the debate is focused on whether and when to apply export controls, not how to ensure that export controls are effectively administered and enforced once applied. However, major weaknesses in U.S. export control enforcement capacity are evident, as demonstrated by Russia\u2019s success in evading controls for weapons used in Ukraine.\n\nThere is a risk of drawing too much optimism from examples such as Huawei and ZTE. The concentrated nature of the advanced semiconductor manufacturing sector made these controls easier to enforce than many others. Additionally, export controls are, in some ways, like a sawblade that can become duller and less effective with repeated use. It remains critical that export controls are not only applied strategically and judiciously but that sufficient resources are also devoted to re-sharpening the blade."}
{"input":"Why is the inclusion of Chinese chip design companies on the Entity List significant?","output":"The inclusion of Chinese chip design companies on the Entity List is significant because it restricts their access to certain technologies and limits their ability to conduct business with entities in the United States, affecting their operations and growth potential.","context":"Since 2013, China has published several national-level policy documents, which reflect the intention to develop and deploy AI in a variety of sectors. For example, in 2015, the State Council released guidelines on China\u2019s \u2018Internet\u2009+\u2019 action. It sought to integrate the internet into all elements of the economy and society. The document clearly stated the importance of cultivating emerging AI industries and investing in research and development. In the same year, the 10-year plan \u2018Made in China 2025\u2019 was released, with the aim to transform China into the dominant player in global high-tech manufacturing, including AI (McBride and Chatzky 2019). Another notable example is the Central Committee of the Communist Party of China\u2019s (CCP) 13th 5-year plan,Footnote2 published in March 2016. The document mentioned AI as one of the six critical areas for developing the country\u2019s emerging industries (CCP 2016), and as an important factor in stimulating economic growth. When read together, these Export controls are critical to competing in this new era. Though the Biden administration\u2019s threats of export controls and sanctions did not successfully deter Russia\u2019s 2022 invasion, there is ample recent evidence of their power. The United States\u2019 leading position in key technology sectors, such as semiconductors, offers strong points of foreign policy leverage. In April 2018, the United States imposed sanctions upon Huawei and ZTE\u2014China\u2019s first- and second-largest telecommunications equipment companies, respectively, at the time\u2014as a penalty for violating the terms of an earlier settlement. In a matter of months, the ZTE sanctions, which, along with other restrictions, prevented the company from buying U.S.-designed semiconductors, rapidly transformed ZTE\u2019s financial situation from significant profitability and rapid growth to imminent bankruptcy. Huawei, for its part, lost access to U.S. semiconductor design software. As a result, Huawei\u2019s smartphone chip design subsidiary, With this said, the new AI \u2018national team\u2019 differs from previous state-sponsored national champions in that they are already internationally successful in their respective fields, independently of this preferential treatment. Furthermore, there is extensive domestic competition in the areas where national champions have been selected. This suggests that competition may not be stymied in the traditional manner. For instance, all the companies selected as AI national champions are developing technologies in Alibaba\u2019s designated area of smart cities (Ding 2019). In parallel with this, patronage does not prohibit smaller companies benefiting from the financial incentive structure. Technology start-ups within China often receive government support and subsidies for developing AI technologies. As an example, Zhongguancun Innovation Town is a purpose-built, government subsidised, incubator workspace that provides a suite of services to help Chinese technology start-ups succeed, often in the significant profitability and rapid growth to imminent bankruptcy. Huawei, for its part, lost access to U.S. semiconductor design software. As a result, Huawei\u2019s smartphone chip design subsidiary, HiSilicon, was forced to halt most operations, and Huawei is no longer one of the five largest global smartphone manufacturers."}
{"input":"What are some examples of opportunities for AI? Give a brief description\/explanation of each.","output":"People-assistive AI, sectoral opportunities, scientific opportunities, public sector opportunities, etc.","context":"things that minds can do. Some of these (e.g., reasoning,            directions of AI and its physical, economic and socio-\nreading, understanding speech) are normally described                political limitations.\nas \u2018intelligent\u2019. Others (e.g., vision, hearing, moving              In fact, AI\u2019s impacts depend greatly on the type of AI\naround natural obstacles) aren\u2019t. But all involve                    algorithm and also on its physical implementation with\npsychological skills \u2013 such as perception, association,              a myriad of interdependent information and\nprediction, planning, motor control \u2013 that enable humans             communication technologies (ICT) and infrastructures.\nand animals to attain their goals.\u201di                                 For example, deep neural networks (DNN) with\nIn that sense, AI is broader than what we typically                  supervised learning \u2013 which has become by far the most Artificial Intelligence (AI) seeks to make computers do                         full implications of AI without exploring the specificities\nthe sorts of things that minds can do, and various                              of the AI technology clusters concerned. It is hoped that\nfunctional or descriptive definitions exist. It is                              this primer provides an easily accessible guide to these\nimportant to note that AI has a long history, dating back                       specificities. Indeed, a black-box approach to \u201cAI\u201d is only\nto the 19th century, with many techniques dating to the                         of limited usefulness to most policy questions.\n1950s. In practice, various clusters of AI types are used\n\u2013 also in hybrid formats. Definitions \u2013 What is AI?\n\u2022 AI: Artificial Intelligence can be\ndefined as the ability of the software\nsystems to carry out tasks that\nusually require human intelligence:\nvision, speech, language,\nknowledge, and search.\n\u2022 ML: Machine Learning is the ability\nof the computer algorithm to learn\nfrom data and improve automatically.\n\u2022 NLP: Neuro-linguistic programming\nis the ability of an AI algorithm to\nread a text, convert speech into text,\nor vice versa.\n\u2022 Data Mining: The ability of the AI\nalgorithm to examine large amounts\nof raw data to determine patterns.\n\u2022 ANN: Artificial Neural Networks\nare AI algorithms that recognize\nrelationships between different data\nsets similar to how the human brain\nanalyzes such information. In that sense, AI is broader than what we typically                  supervised learning \u2013 which has become by far the most\nconsider human intelligence. It includes a wide range of             widely adopted AI technology in the 2010s \u2013 requires\ninformation-processing capacities. In fact, AI could                 enormous amounts of data and large amount of energy\ninclude aspects of intelligence that are far outside the             for data handling and computation. Hence, much talk\nreach of humans. And while AI requires physical                      about the \u201cdata economy\u201d these days. In terms of\nmachines, especially computers, it is really                         physical implementation, it is important to note that AI\ninformationally powerful virtual machines (e.g.,                     used on a mobile phone via a 5G network differs greatly\nsoftware) that make the machine intelligent. Hence, AI               from AI run on a supercomputer in a research lab. This"}
{"input":"What are some examples of risks of AI caused by humans?","output":"Deep fakes and hostile information campaigns","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d Risk-based approach. AI is a general-purpose technology that is likely to be ubiquitous and apply to a wide range of applications. This makes it harder to regulate well. Many propose a risk-based approach where higher-risk AI applications that can cause greater damage have proportionately greater regulation and lower-risk applications have less. This has merits, but there are many implementation obstacles including who and how risk is defined and measured, what are the minimum requirements to offer AI services, and what AI uses are deemed illegal. Further, Google argues that focusing regulation on the highest risks may deter innovation that also has the highest value to society. critical infrastructures (e.g. transport), that could put the life and health of citizens at risk;\neducational or vocational training, that may determine the access to education and professional course of someone\u2019s life (e.g. scoring of exams);\nsafety components of products (e.g. AI application in robot-assisted surgery);\nemployment, management of workers and access to self-employment (e.g. CV-sorting software for recruitment procedures);\nessential private and public services (e.g. credit scoring denying citizens opportunity to obtain a loan);\nlaw enforcement that may interfere with people\u2019s fundamental rights (e.g. evaluation of the reliability of evidence);\nmigration, asylum and border control management (e.g. verification of authenticity of travel documents);\nadministration of justice and democratic processes (e.g. applying the law to a concrete set of facts).\nHigh-risk AI systems will be subject to strict obligations before they can be put on the market: The Regulatory Framework defines 4 levels of risk in AI:\npyramid showing the four levels of risk: Unacceptable risk; High-risk; limited risk, minimal or no risk\n \nAll AI systems considered a clear threat to the safety, livelihoods and rights of people will be banned, from social scoring by governments to toys using voice assistance that encourages dangerous behaviour.\n\nHigh risk:\nAI systems identified as high-risk include AI technology used in:"}
{"input":"In which instances can the UN act as the arbiter of AI governance? How?","output":"Challenges to international security -  help ensure that there are no accountability gaps, for example by encouraging states to report analogous to reporting on the SDGs targets and the Universal Periodic Review that facilitates monitoring, assessing, and reporting on human rights practices","context":"The UN and the International Telecommunication\nUnion (ITU) have also taken steps towards\ninternational coordination by articulating an\napproach for the UN system to support AI adoption\nin developing countries and by hosting the annual\nAI for Good conference. UN agencies also apply\na baseline of standards in countries that may lack\nregulatory regimes, which can ultimately help guide\nand shape the formulation of national AI strategies. International forums such as the G20 and G7,\norganizations such as the UN, ITU, OECD\nand UNESCO and supranational and regional\norganizations such as the African Union can\ncoordinate policies and pool resources across\ncountries to devise and implement AI strategies.\nPolicies at international levels can also help to define\nand monitor regulation, practices, and standards in\nAI applications. Some international organizations\nhave already proposed guidelines and approaches\nfor building capabilities for governing AI, often\nleveraging soft law. The UN High-Level Panel on\nDigital Cooperation for example was convened by\nthe UN Secretary-General to advance global multistakeholder dialogue regarding the potential of\ndigital technologies to advance human wellbeing\nwhile mitigating any risks of these technologies.4\nThe UN and the International Telecommunication\nUnion (ITU) have also taken steps towards\ninternational coordination by articulating an\napproach for the UN system to support AI adoption AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. Effective implementation of national AI initiatives hinges on coordination\nCountries pursue different national governance models to co-ordinate the\nimplementation of their national AI policies across government, offering\nregulatory and ethical oversight (Figure 1). Models include:\n\u2022\n\nAssigning oversight of the development and implementation strategies\nto an existing ministry, department or body. Among existing ministries\nor agencies tasked with developing or implementing an AI strategy, the\nfollowing tend to drive the creation of AI strategies most often: 1)\ninformation technology and communications ministries; 2) economics or\nfinance ministries; or 3) education, science (and technology) and\ninnovation ministries.\n\n\u2022\n\nCreating a new governmental or independent AI co-ordination entity.\n\n\u2022"}
{"input":"What is the purpose of the development of this Recommendation?","output":"to set a standard that is implementable and flexible enough to stand the test of time in a rapidly evolving field.","context":"Three regulations require the deepest analysis: recommendation algorithms, \u201cdeep synthesis,\u201d and generative AI. These interconnected documents contain the most targeted and impactful regulations to date, creating concrete requirements for how algorithms and AI are built and deployed in China. Below is a brief overview of each regulation. The remainder of this paper and subsequent papers will expand on the intellectual roots and key bureaucratic actors behind these regulations.\nProvisions on the Management of Algorithmic Recommendations in Internet Information Services Provisions on the Management of Algorithmic Recommendations in Internet Information Services\nThe 2021 regulation on recommendation algorithms marked the start of China\u2019s more targeted restrictions on algorithms and AI. The original motivation for the regulation was CCP concern about the role of algorithms in disseminating information online. But as that imperative worked its way through the policy community and bureaucracy, many other adjacent applications of algorithms\u2014from setting schedules for workers to setting prices online\u2014were tacked on. The regulation also created a reusable bureaucratic tool that would be deployed repeatedly in future regulations. Providers were also told not to build algorithms that \u201cgo against ethics and morals\u201d by\n\u201cinducing users to become addicted or spend too much.\u201d Individual users were also granted new rights by the regulation, including the right to turn off algorithmic recommendation services, to delete tags used to personalize recommendations, and to receive an explanation when an algorithm has a major impact on their interests.\nFinally, the recommendation algorithm regulation created an important new tool for regulators: the algorithm registry. The registry is an online database of algorithms that have \u201cpublic opinion properties or . . . social mobilization capabilities.\u201d Developers of these algorithms are required to submit information Tracing the origin of the term \u201calgorithmic recommendation\u201d backward in Chinese state media shows that it first emerged during a 2017 CCP backlash against ByteDance\u2019s news and media apps, in which user feeds were dictated by algorithms. The party viewed this as threatening its ability to set the agenda of public discourse and began looking for ways to rein in algorithms used for information dissemination. Much of the final regulation is dedicated to these concerns, requiring that algorithmic recommendation service providers \u201cuphold mainstream value orientations\u201d and \u201cactively transmit positive energy.\u201d\nThe regulation included some more concrete measures for online content control, such as requiring that platforms manually intervene in lists of hot topics on social media to ensure they reflect government priorities."}
{"input":"What are the principles for responsible stewardship of trustworthy AI?","output":"Inclusive growth, human-centered values and fairness, transparency and explainability, robustness, security and safety, and accountability","context":"Right to Transparency. All individuals have the right to know the basis of an AI decision that concerns them. This includes access to the factors, the logic, and techniques that produced the outcome.\nRight to Human Determination. All individuals have the right to a final determination made by a person.\nIdentification Obligation. The institution responsible for an AI system must be made known to the public.\nFairness Obligation. Institutions must ensure that AI systems do not reflect unfair bias or make impermissible discriminatory decisions.\nAssessment and Accountability Obligation. An AI system should be deployed only after an adequate evaluation of its purpose and objectives, its benefits, as well as its risks. Institutions must be responsible for decisions made by an AI system.\nAccuracy, Reliability, and Validity Obligations. Institutions must ensure the accuracy, reliability, and validity of decisions. Inclusive growth, sustainable development and well-being. AI should benefit people and the planet.\nHuman-centered values and fairness. AI systems should be designed in a way that respects the rule of law, human rights, democratic values and diversity, and they should include appropriate safeguards \u2013 for example, enabling human intervention when necessary \u2013 to ensure a fair and just society.\nTransparency and explainability. There should be transparency and a responsible disclosure around AI systems to ensure that people understand AI-based outcomes and can challenge them.\nRobustness, security and safety. AI systems must function in a robust, secure and safe way throughout their life cycles and potential risks should be continually assessed and managed.\nAccountability. Organizations and individuals developing, deploying or operating AI systems should be held accountable for their proper functioning in line with the above principles.\nINTERNATIONAL LAWS \u2022\n\nOrganisations and individuals developing, deploying or operating AI\nsystems should be held accountable for their proper functioning in line with\nthe above principles\n\nConsistent with these principles, the OECD also provides five recommendations to\ngovernments:\n\u2022\n\nFacilitate public and private investment in research & development to spur\ninnovation in trustworthy AI.\n\n\u2022\n\nFoster accessible AI ecosystems with digital infrastructure and technologies\nand mechanisms to share data and knowledge.\n\n\u2022\n\nEnsure a policy environment that will open the way to the deployment of\ntrustworthy AI systems.\n\n\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI. These endeavors have a direct link to trustworthy AI. For instance, having trust in the dataset used to train an AI system is crucial to ensuring its transparency and accountability. Moreover, the question of how to build trust in data generated by AI systems is an inevitable challenge in realizing the DFFT. Discussions on how to make such trustworthy AI ecosystems will be promoted under the umbrella of the IAP in partnership with the public and private sectors.\nImplementing Agile, Distributed, and Multi-stakeholder Governance through Policy Incentives"}
{"input":"What are some recommendations for the government regarding international cooperation for trustworthy AI?","output":"Foster the sharing of AI knowledge internationally, encourage development of internationally comparable metrics","context":"\u2022\n\nOrganisations and individuals developing, deploying or operating AI\nsystems should be held accountable for their proper functioning in line with\nthe above principles\n\nConsistent with these principles, the OECD also provides five recommendations to\ngovernments:\n\u2022\n\nFacilitate public and private investment in research & development to spur\ninnovation in trustworthy AI.\n\n\u2022\n\nFoster accessible AI ecosystems with digital infrastructure and technologies\nand mechanisms to share data and knowledge.\n\n\u2022\n\nEnsure a policy environment that will open the way to the deployment of\ntrustworthy AI systems.\n\n\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI. is a relatively new phenomenon. To track these initiatives, the OECD AI Policy\nObservatory (OECD.AI) comprises over 620 national AI policies from over 60\ncountries and the European Union (EU). These resources provide a baseline to\nmap countries\u2019 AI policy initiatives according to the recommendations to\ngovernments contained in the OECD AI Principles (Box 1).\nBox 1. OECD AI Principles\nThe OECD Principles on Artificial Intelligence promote AI that is innovative,\ntrustworthy and respects human rights and democratic values. They were adopted\nin May 2019 by OECD member countries when they approved the OECD Council\nRecommendation on Artificial Intelligence. The OECD AI Principles identify five\ncomplementary values-based principles for the responsible stewardship of\ntrustworthy AI:\n\u2022 Artificial Intelligence Safety and Stability\nNations around the world are investing in artificial intelligence (AI) to improve their military, intelligence, and other national security capabilities. Yet AI technology, at present, has significant safety and security vulnerabilities. AI systems could fail, potentially in unexpected ways, due to a variety of causes. Moreover, the interactive nature of military competition means that one nation\u2019s actions affect others, including in ways that may be detrimental to mutual stability. There is an urgent need to explore actions that can mitigate these risks, such as improved processes for AI assurance, norms and best practices for responsible AI adoption, and confidence-building measures that improve stability among all nations. (i)   lead efforts outside of military and intelligence areas to expand engagements with international allies and partners in relevant bilateral, multilateral, and multi-stakeholder fora to advance those allies\u2019 and partners\u2019 understanding of existing and planned AI-related guidance and policies of the United States, as well as to enhance international collaboration; and\n\n          (ii)  lead efforts to establish a strong international framework for managing the risks and harnessing the benefits of AI, including by encouraging international allies and partners to support voluntary commitments similar to those that United States companies have made in pursuit of these objectives and coordinating the activities directed by subsections (b), (c), (d), and (e) of this section, and to develop common regulatory and other accountability principles for foreign nations, including to manage the risk that AI systems pose."}
{"input":"What is the role of the private sector? ","output":"Innovation, development and application of AI in the digital economy, and diffusion of AI technology on the supply side, and adoption and usage of AI on the demand side.","context":"The Role of the Private Sector\nThe intertwined relationship between AI\ndevelopment and the digital economy makes the\n\nworldbank.org\/digitaldevelopment\n7\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nwell-developed innovation ecosystems can drive\ntechnology diffusion in new areas, with a growing\nnumber of start-ups developing AI applications for\nnew and different contexts. One key way the private\nsector can help shape national AI strategies is\nthrough providing comments during consultation\nperiods, where feedback on strategy proposals is\nsolicited by government planning bodies, such as\nin Brazil. 3 4\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fHow can countries address these challenges?\nAI policy incorporating legislative foundations, institutional arrangements and a\ngovernance framework for the use of AI could provide the necessary enabling\nenvironment. This could be part of a broader Digital Transformation Policy or a\nstand-alone framework.\nA central innovation hub for AI in government could help pool scarce human\nand financial resources to support the initiatives of line ministries and start to\ndevelop a whole-of-government approach. In the use cases examined here,\nmost governments have already set-up the main hub for AI issues that serves\nas a central authority over AI projects being delivered by line agencies.\nThe AI hub\u2019s role could also be to connect industry expertise to the government\ndepartments and line agencies, to promote research, and build alliances with\nacademic institutions and the private sector. 17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\n6.\nAI adoption in the public sector: Policies\nto accelerate the adoption and use of AI in the\npublic sector to improve public service performance\noutcomes and efficiency. Approaches include PPPs,\npiloting programs, AI training courses for public\nadministrators and spreading awareness about AI\nopportunities.\n\nApproaches include fostering partnerships between\ntechnology companies and sectoral or industry\nactors, funding or support for national champions\nin target sectors, and spreading awareness about\nopportunities for productivity and efficiency gains.\n8.\nBuilding capabilities for AI governance:\nPolicies include consultations with, and forming\nadvisory bodies of, industry and academic experts,\npublic consultations and pilots for applications of\nAI to learn about opportunities and risks. private sector a crucial actor in AI development.\nThe private sector \u2013 including large enterprises,\nsmall and mid-size enterprises (SMEs) and startups \u2013 plays a dominant role in the innovation,\ndevelopment and application of AI in the digital\neconomy. AI research and product development\nis increasingly led by large technology companies\nwhich can afford to hire top talent, capture\nand obtain better data and employ large-scale\ncomputing facilities, experimentation labs and\ntestbeds for product design. While more limited in\nscalability and resources, SMEs have an important\nrole to play with diffusion of AI technology on\nthe supply side, and adoption and usage of AI on\nthe demand side. Start-ups can also help expand\nthe overall AI ecosystem as they seek out scalable\nbusiness models. Dynamic entrepreneurship and\n\nThe Role of the Private Sector\nThe intertwined relationship between AI\ndevelopment and the digital economy makes the\n\nworldbank.org\/digitaldevelopment\n7"}
{"input":"What are the eight policy domains within AI strategies?","output":"scientific research, AI talent development, entrepreneurial ecosystem, standards for ethical or trustworthy AI, data access, AI adoption in the public sector, strategic sectoral targeting of AI, building capabilities for AI governance","context":"16 https:\/\/www.oxfordinsights.com\/ai-readiness2019\n\nworldbank.org\/digitaldevelopment\n11\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nEight Policy Domains within AI Strategies\nNational strategies target a range of sub-goals\nor domain areas. Based on work by Dutton\net al (2018) and the findings of the European\nCommission\u2019s Coordinated Plan on Artificial\nIntelligence published in 201817, the variety of subgoals in national AI strategies can be categorized in\nthe following eight domain areas. These represent\ndistinct thematic areas that are the object of\npolicymaking to accelerate AI development and\nadoption at the country level. These domains are\ninfluenced by various tools and instruments as\ndescribed in the subsequent section, which in\nturn can simultaneously target multiple domains\nnoted below: available documents and interviews with experts\nand advisors between February 2019-July 2020.\nOnce the findings were collected, the approach\nwas to review the content of national AI policy\nand strategy documents using a matrix of variables\nbased on previous research on this subject (Dutton\net al. 2018; European Commission 2018). Overall,\na matrix of eight policy domain areas and six policy\ntools and instruments to implement national AI\nstrategies were used for understanding the role of\ngovernment, which is illustrated using heatmaps.\n\n16 https:\/\/www.oxfordinsights.com\/ai-readiness2019\n\nworldbank.org\/digitaldevelopment\n11\n\n\fANALYTICAL INSIGHTS - NOTE 4 AI policy design\nCountries are at different stages of the development and implementation of\nnational AI strategies and policies. Some countries, such as Canada and Finland,\ndeveloped their national AI strategies as early as 2017, closely followed by\nJapan, France, Germany and the United Kingdom in 2018. Other countries, such\nas Brazil, Egypt, Hungary, Poland and Spain, launched a national AI strategy\nmore recently. Several countries are currently in AI policy consultation and\ndevelopment processes.\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f|7\nNational AI strategies and policies are often initiated with a call to action in the\nform of a report, roadmap, or white paper that frames the high-level goals for\na strategy. The policy design and development stages often follow. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f4|\n\nAn overview of national AI strategies and\npolicies\n\nAs artificial intelligence (AI) advances across economies and societies, policy\nmakers and AI actors around the world seek to move from principles to practice.\nTo harness the benefits of AI while mitigating the risks, governments are\ninvesting in AI R&D; leveraging AI in specific industries such as transportation and\nhealthcare; building human capacity on AI; ensuring a fair labour market\ntransformation; reviewing and adapting relevant policy and regulatory\nframeworks and developing standards; and co-operating internationally. This\nGoing Digital Toolkit note provides an overview of the various AI policy\ninitiatives undertaken by governments and analyses these initiatives throughout\nthe AI policy cycle: 1) policy design; 2) policy implementation; 3) policy\nintelligence; and 4) approaches for international and multi-stakeholder cooperation on AI policy."}
{"input":"What do India and China both focus on enabling?","output":"Technological development","context":"implement AI in sectoral applications. India\u2019s\nstrategy outlines that, while the government is\nresponsible for providing an enabling environment\nand infrastructure, the private sector must lead\nthe implementation and adoption in use cases.\nThe Chinese government encourages cooperation\nbetween the public and private sectors, especially\nin terms of sharing data between the government\nand companies. Furthermore, since 2018, China\u2019s\ngovernment has been publicly encouraging several\n51\ndomestic \u201cAI Champions.\u201d\n\nThe strategies of both countries focus on enabling\ntechnological development. China\u2019s approach to AI\napplications has been characterized as \u201cexperiment\nfirst and regulate later,\u201d yielding rapid innovation\nand implementation, especially in healthcare (Nesta\n2020, p3). In terms of stakeholder engagement,\nChina has less involvement from stakeholders\noutside central government, whereas NITI While both India and China exhibit global\nleadership in developing comprehensive national\nAI strategies, China\u2019s AI ecosystem is more mature,\nand its government has committed a greater\nnumber of resources towards it. Both governments\ntake charge in defining priorities and designing\ntheir AI strategies. The Indian federal government\u2019s\nNITI Aayog think tank is tasked with designing\nits national strategy,48 and the resulting \u2018National\n\nBoth India and China\u2019s AI strategies are also\ndefined by the central government in a top-down\napproach, while relying on local governments to\nexecute strategies. Decision-making power rests\nin the hands of central government to design and\ndevelop the targets and policies of the AI strategy.\nCompared to India, China\u2019s policies more frequently\nreflect a directing role of government, such as direct\n\nworldbank.org\/digitaldevelopment\n\n48 Expert interview with member of NITI Aayog on May 9, 2019.\n\n24 The AIDP provides a longitudinal perspective on China\u2019s strategic situation regarding AI, including its comparative capabilities, the opportunities offered, and the potential risks. Following a technology-first approach, it may be tempting to concentrate one\u2019s attention on the stated capabilities of AI, to gain an insight into the types of technologies in which China is investing. However, this would likely offer only a short-term perspective and would soon be out of date as technological innovation advances rapidly. Furthermore, it would do little to explain why China is seeking to develop a strong AI sector in the decades to come. To this end, it is more useful to try to understand China\u2019s strategic focus from a policy-first approach, by analysing the areas where China considers that AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the AI presents opportunities. In this section, we focus on these areas of particular importance to China, on how and what China expects to gain from developing AI in each of them, and on some of the perceived risks present in each of these areas. The AIDP highlights three areas where AI can make a substantial difference within China: international competition, economic development, and social governance. They are strictly interrelated but, for the sake of clarity, we shall analyse them separately, and contextualise each of them by discussing the relevant literature surrounding the broader political backdrop and contemporary policy debates."}
{"input":"What is the affordable connectivity program and is it ending soon?","output":"Affordable Connectivity Program (ACP) was enacted as part of the 2021 Infrastructure Investment and Jobs Act to ensure broadband access. It offers a $30 monthly subsidy to about 23 million homes. Funds are likely to run out in late April or May 2024.","context":"with NVIDIA to establish a joint AI technology\ncenter. This collaboration provides the Finnish\nAI ecosystem with significant computing power,\ngraphical processing units (GPU) and AI software\nfor AI applications for research and industry use\n(FCAI 2020). Finland has also set up a 5G network\npilot (5GTNF) with private partners and research\nand academic partners to establish an environment\nfor research and business development purposes.26\nFinland\u2019s government has directly invested\ntowards various AI capacity building initiatives\nincluding research, learning reform, data\nmanagement and computational infrastructure.\nIn 2018 the government announced \u20ac160 million\nfor AI investment, which includes \u20ac34 million\nin funding for the AI Business program,27 a fund\nwhich focuses on developing new value from AI\nand the platform economy.28 This program has\nbeen authorized to allocate \u20ac100 million in funds\nover a four-year period to Finnish-registered\nstartups, SMEs and larger companies engaging (e)  The Federal Communications Commission is encouraged to consider actions related to how AI will affect communications networks and consumers, including by:\n\n          (i)    examining the potential for AI to improve spectrum management, increase the efficiency of non-Federal spectrum usage, and expand opportunities for the sharing of non-Federal spectrum;\n\n          (ii)   coordinating with the National Telecommunications and Information Administration to create opportunities for sharing spectrum between Federal and non-Federal spectrum operations;\n\n          (iii)  providing support for efforts to improve network security, resiliency, and interoperability using next-generation technologies that incorporate AI, including self-healing networks, 6G, and Open RAN; and worldbank.org\/digitaldevelopment\n26\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nput forward multiple initiatives publicly advocated\nby the president (Ahram Online 2020). In Ghana,\nthe government is also investing in innovation by\nlaunching a $10 million initiative called the National\n54\nEntrepreneurship and Innovation Plan (NEIP).\nFrom a bottom-up approach, the government\nencourages private associations to create hubs and\nconferences for AI skills at the local level. In South\nAfrica, private associations have been organizing\nconferences to teach deep learning and AI locally,\nsuch as the Deep Learning Indaba conference\n(Snow 2019).\nFinally, some of these countries have demonstrated\nconstraining policies to mitigate the risks of AI,\nwith a particular focus on data protection. In Kenya\nthe Open Data portal makes public government\n\n54 http:\/\/neip.gov.gh\/page\/2\/\n55 http:\/\/www.opendata.go.ke\/\n\n27 Despite the increased burden on BIS to ensure effective enforcement of export control rules, the increase in available funding for equipping BIS with new licensing and enforcement technology has not increased commensurate with the added enforcement responsibility. The Biden administration\u2019s budget submission for FY 2023 does include a request for a $66 million increase in the BIS budget. However, upon further inspection, this budget increase is less than it may seem. A full $36 million of the increase is for a new program called Information and Communication Technologies and Services (ICTS), which is an entirely new and previously unfunded BIS mission focused on policing U.S. imports of foreign technology, such as Huawei telecommunications equipment. An additional $23 million of the BIS budget request should be understood as addressing the loss of purchasing power due to inflation since FY 2020. In short, the demands upon BIS to perform its core function of export controls"}
{"input":"Why wouldn't congress extend the Affordable Connectivity Program (ACP)?","output":"Legislators may not vote for an extension unless the program's eligibility and costs are limited. The 'Hastert Rule' in the Republican House also requires majority Republican support for a vote, which is not assured.","context":"(e)  The Federal Communications Commission is encouraged to consider actions related to how AI will affect communications networks and consumers, including by:\n\n          (i)    examining the potential for AI to improve spectrum management, increase the efficiency of non-Federal spectrum usage, and expand opportunities for the sharing of non-Federal spectrum;\n\n          (ii)   coordinating with the National Telecommunications and Information Administration to create opportunities for sharing spectrum between Federal and non-Federal spectrum operations;\n\n          (iii)  providing support for efforts to improve network security, resiliency, and interoperability using next-generation technologies that incorporate AI, including self-healing networks, 6G, and Open RAN; and Procuring technology is only part of the solution. Congress needs to appropriate funds on an annual basis to ensure that BIS staff are able to leverage the technology. Furthermore, funding extra staff will help close ongoing gaps in enforcement, particularly as enforcement needs grow in number and complexity. In other words, simply acquiring new technologies is not a panacea on its own; successful use of new technologies depends on the agency\u2019s ability to use them effectively. 2.2 Implementing the AIDP Evidence from comparable organizations suggests that Congress should appropriate no less than $25 million annually for the next five years for BIS technology modernization.\n\nBased on discussions with members of the U.S. trade, law enforcement, and intelligence communities, as well as private sector technology providers, Congress should allocate $25 million of funding to BIS annually in addition to the initial system buildout, updates, training, and maintenance. It will require continued support for years to come, particularly if the export control enforcement environment becomes substantially more complex in the future."}
{"input":"Is access to the internet important for healthcare?","output":"Yes, particularly for telehealth. Studies show cost savings and reduced emergency department visits, exemplified in cancer patient and veteran care, highlighting internet access's importance in healthcare.","context":"to complete the action (Mann et al. 2016). In this case, from a Chinese perspective, one may argue that sharing of the healthcare data may pose little immediate threat to the individual, especially as Article 6 of the Regulations on the Management of Medical Records of Medical Institutions, Article 8 of the Management Regulations on Application of Electronic Medical Records, Article 6 of the Measures for the Management of Health Information, the Cybersecurity Law of the People\u2019s Republic of China, and the new Personal Information Security Specification all provide specific and detailed instructions to ensure data security and confidentiality (Wang 2019). However, it could potentially deliver significant benefit to the wider population. All five principles are relevant for understanding China\u2019s healthcare system as a whole but, from the perspective of analysing the ethics of China\u2019s use of AI in the medical domain, principles (a), (b), and (e) are the most important. They highlight that\u2014in contrast to the West, where electronic healthcare data are predominantly focused on individual health, and thus AI techniques are considered crucial to unlock \u2018personalised medicine\u2019 (Nittas et al. 2018)\u2014in China, healthcare is predominantly focused on the health of the population. In this context, the ultimate ambition of AI is to liberate data for public health purposesFootnote12 (Li et al. 2019a, b). This is evident from the AIDP, which outlines the ambition to use AI to \u2018strengthen epidemic intelligence monitoring, prevention and control,\u2019 and to \u2018achieve breakthroughs in big data analysis, Internet of Things, and other key technologies\u2019 for the purpose of strengthening community intelligent health management. The same aspect control,\u2019 and to \u2018achieve breakthroughs in big data analysis, Internet of Things, and other key technologies\u2019 for the purpose of strengthening community intelligent health management. The same aspect is even clearer in the State Council\u2019s 2016 official notice on the development and use of big data in the healthcare sector, which explicitly states that health and medical big data sets are a national resource and that their development should be seen as a national priority to improve the nation\u2019s health (Zhang et al. 2018).Footnote13 These outcomes are deeply harmful\u2014but they are not inevitable. Automated systems have brought about extraordinary benefits, from technology that helps farmers grow food more efficiently and computers that predict storm paths, to algorithms that can identify diseases in patients. These tools now drive important decisions across sectors, while data is helping to revolutionize global industries. Fueled by the power of American innovation, these tools hold the potential to redefine every part of our society and make life better for everyone."}
{"input":"Can you tell me about the new Executive Order on AI that Joe Biden is signing?","output":"The Executive Order on AI focuses on national security and includes responsibilities for various agencies. It aims to oversee AI development while Congress develops a comprehensive framework.","context":"At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups. President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated. The Executive Order was met with warm response from Congress and some civil society groups.\n\nIn a written statement to the press, Senate Majority Leader Charles Schumer (D-NY) congratulated the President on this \u201ccrucial step\u201d to ensure the US remains a leader of AI innovation, but warned that more is needed. \u201cAll executive orders are limited in what they can do, so it is now on Congress to augment, expand, and cement this massive start with legislation,\u201d Schumer said. He added that the Senate will \u201ccontinue to work in a bipartisan fashion and act with \u201curgency and humility.\u201d (b)  This order shall be implemented consistent with applicable law and subject to the availability of appropriations.\n\n     (c)  This order is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person.\n\n                             JOSEPH R. BIDEN JR.\n\nTHE WHITE HOUSE,\n  October 30, 2023."}
{"input":"What are the main parts of the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence?","output":"The Order includes creating AI safety and security standards, protecting consumer privacy and health, supporting workers, promoting innovation and competition, implementing global AI standards, and advancing equity and civil rights.","context":"Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated."}
{"input":"How are countries addressing frontier AI?","output":"The UK's Frontier AI Taskforce monitors AI developments, has issued reports, expanded research capacity, formed partnerships, and supported Isambard-AI, a supercomputer for AI safety research.","context":"categories of tools and instruments that were used\nto assess national AI strategies. The third section\npresents findings from eleven countries, beginning\nwith a more detailed analysis of the national AI\npolicies of Finland and the United Arab Emirates\n(UAE). This section also includes findings from an\nanalysis of a broader set of countries in Asia, Africa\nand South America, highlighting the approach\ntaken to AI in some developing countries. The final\nsection highlights future areas of research needed\nto expand upon this work. FINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re-\n\nview of the national AI strategies of Finland and the\nUAE to better understand the actions and motives\nof early AI-adopter countries. Many governments view AI as a strategic resource to enhance a countries\u2019\ncompetitiveness and to boost economic growth. AI can potentially contribute\n$13 trillion to the global economy by 2030, according to a 2018 report by\nMcKinsey & Company. At least 50 governments have developed or are in\nImportance of AI\nthe process of developing an AI strategy. However, the pace of AI adoption is\nuneven, and most countries that the Bank is actively supporting to modernize\n\u2022 $13 trillion potential by 2030\ngovernments, are either not ready for AI adoption or at very early stages. There\n\u2022 50 Governments are preparing\nis no country from the Africa or Latin America regions, for example, in the list of\nAI Strategies.\nthe top 20 countries on the AI Readiness Index developed by Oxford Insights.\nExcept for four economies, the Asia-Pacific region is also not well advanced in\nAI. Slower adoption of AI in client countries may have the potential to lead to Except for four economies, the Asia-Pacific region is also not well advanced in\nAI. Slower adoption of AI in client countries may have the potential to lead to\nfurther inequality between the rich and the poor nations. To ensure our clients are aware of the potential for AI to advance their\ndevelopment, this note highlights opportunities and risks that need to be managed, which may be of interest to those clients\nseeking to explore AI in their modernization strategies."}
{"input":"What are the primary national security risks associated with AI and advanced computing?","output":"National security risks include AI-enabled mass surveillance, cyberattacks, and novel biological weapons design by irresponsible actors.","context":"The risks are overblown. There are few legitimate concerns or negative externalities from AI. \n\nCompanies (and open source) should be allowed to build AI as fast and aggressively as they can. \n\nBig AI companies \u201cshould not\u202f(be) allowed to achieve regulatory capture,\u202fnot\u202fallowed to establish a government-protect cartel that is insulated from market competition\u2026\u201d  \n\n\u201cTo offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should\u2026use AI to maximize society\u2019s defensive capabilities.\u201d  \n\n\u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d critical infrastructures (e.g. transport), that could put the life and health of citizens at risk;\neducational or vocational training, that may determine the access to education and professional course of someone\u2019s life (e.g. scoring of exams);\nsafety components of products (e.g. AI application in robot-assisted surgery);\nemployment, management of workers and access to self-employment (e.g. CV-sorting software for recruitment procedures);\nessential private and public services (e.g. credit scoring denying citizens opportunity to obtain a loan);\nlaw enforcement that may interfere with people\u2019s fundamental rights (e.g. evaluation of the reliability of evidence);\nmigration, asylum and border control management (e.g. verification of authenticity of travel documents);\nadministration of justice and democratic processes (e.g. applying the law to a concrete set of facts).\nHigh-risk AI systems will be subject to strict obligations before they can be put on the market: Risk-based approach. AI is a general-purpose technology that is likely to be ubiquitous and apply to a wide range of applications. This makes it harder to regulate well. Many propose a risk-based approach where higher-risk AI applications that can cause greater damage have proportionately greater regulation and lower-risk applications have less. This has merits, but there are many implementation obstacles including who and how risk is defined and measured, what are the minimum requirements to offer AI services, and what AI uses are deemed illegal. Further, Google argues that focusing regulation on the highest risks may deter innovation that also has the highest value to society. to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response."}
{"input":"How might the implementation of on-chip governance mechanisms impact U.S. firms' global competitiveness?","output":"Implementing on-chip governance in commercial chips requires industry R&D. DoC export market access commitments can incentivize this without public funding, balancing revenue loss due to export restrictions.","context":"Domain Specific Outcomes \n\nTrade conflict emerges with the EU and others, creating a fragmented global regulatory environment. The United States likely globally advances its AI principles and voluntary business commitments to influence international rules (vs. China and EU approaches). IPR and copyright will be especially important for creative industries. The EU\u2019s AI Act will likely influence other major nations, frustrating U.S. leadership. Leading companies like Amazon, Apple, Google, Meta, Microsoft, and Nvidia likely face multiple AI regimes around the world.   \n\nAntitrust agencies lead the effort to forestall \u201cbig tech\u201d companies dominating AI, false and deceptive practices, and AI-driven fraud. The high cost and scale of AI foundation models likely leads to market concentration. We expect FTC actions designed as warnings shots to industry. The U.S. government continues to allow private corporations to develop AI without meaningfully regulating the technology or intervening in a way that changes those corporations' behavior. This approach fits with the long-standing belief in the United States that the free market (and its profit-driven incentives) is the most effective mechanism to rapidly advance technologies like AI.[1] In this world, U.S. government personnel continue to lag behind engineers in the U.S. technology sector, both in their understanding of AI and in their ability to harness its power. Private corporations direct the investment of almost all research and development funding to improve AI, and the vast majority of U.S. technical talent continues to flock to Silicon Valley. The U.S. government seeks to achieve its policy goals by relying on the country's innovators to develop new inventions that it could eventually purchase. In this world, the future relationship between the U.S. government and the Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC. Countries and corporations have long competed for power. Big technology companies challenge governments in many of the old ways while adding new approaches to the mix. Similar to traditional multinational corporations, such companies reach across national boundaries; however, big tech companies also influence local communities in much more comprehensive and invasive ways because they touch consumers' lives and gather data on their locations, activities, and habits. Big tech companies also affect national economies, domestic policy, and local politics in new ways because they influence the spread of information (and disinformation) and create new communities and subcultures. This has contributed to polarized populations in several countries and regime change in others. AI companies specifically enjoy access to vast investment funds and massive computing power, giving them additional advantages."}
{"input":"What are the proposed stages for the development and rollout of on-chip governance for data center AI chips?","output":"Initial stages include deploying firmware updates for export AI chips with early versions of hardware operating licenses. Future stages aim for 'tamper-evident' and eventually 'tamper-proof' chips, enhancing security and reducing the need for physical inspections.","context":"AI governance can utilize different parts of the AI supply chain as a point of entry. Measures can focus on regulating training data, algorithms, or computing power, or they can simply impose requirements on the final actions taken by an AI product, leaving the remedies up to the developer. China\u2019s approach to AI governance has been uniquely focused on algorithms. As the word \u201cstakeholder\u201d frequently appears in the G7 statements, the future of AI governance will likely not rely solely on top-down, government-led rule-setting. Instead, it suggests an approach where multi-stakeholder initiatives are discussed on a case-by-case basis and updated in an agile manner. Regulations and technical standards serve as critical tools to facilitate these efforts in building trustworthy AI. However, it is crucial to note that the key players in AI governance, who develop and use these tools, will be not only the government entities, but rather the private stakeholders including AI developers, users, and civil society organizations, and so on. Measures to Address Challenges\n\u2022 Issue and make fully transparent\nthe legal framework, AI policy,\nethical principles and operating\nframework.\n\u2022 Establish special AI\/Innovation Hub\nor government unit as a center of\nexcellence.\n\u2022 Develop whole-of-government,\ndata fabric AI architecture.\n\u2022 Ensure human oversight to\nmanage risks and safeguard\nagainst machine-invoked bias. Yet it is not entirely clear what kind of cooperation might extend beyond the existing national policies and partnerships with international initiatives. The additional sections of the G7 statements that are closely related to AI governance give some insights.\n\nIn the DFFT section, the creation of the new IAP has been agreed on. This arrangement would be rooted in multi-stakeholder problem-solving, guided by shared principles. Further, in the section on \u201cEmerging and Disruptive Technologies in Innovating Society and Economy,\u201d the necessity of an agile, dispersed, and multi-stakeholder governance approach for AI-driven cyber-physical systems is recognized, and a policy package has been suggested to actualize this."}
{"input":"What does the EU AI Act stand for?","output":"Proposal for a Regulation laying down harmonized rules for artificial intelligence.","context":"Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. Shaping Europe\u2019s digital future - AI Act\n\nThe Commission is proposing the first-ever legal framework on AI, which addresses the risks of AI and positions Europe to play a leading role globally.\nThe regulatory proposal aims to provide AI developers, deployers and users with clear requirements and obligations regarding specific uses of AI. At the same time, the proposal seeks to reduce administrative and financial burdens for business, in particular small and medium-sized enterprises (SMEs).\n\nThe proposal is part of a wider AI package, which also includes the updated Coordinated Plan on AI. Together, the Regulatory framework and Coordinated Plan will guarantee the safety and fundamental rights of people and businesses when it comes to AI. And, they will strengthen uptake, investment and innovation in AI across the EU. Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":"What is the primary purpose of the EU AI Act?","output":"The EU AI Act aims to regulate the sale and use of AI in the EU, ensuring the proper functioning of the single market by setting consistent standards across member states, and safeguarding health, safety, and fundamental rights.","context":"Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. International context. geopolitical competition, national security, and economic competitiveness. The AI debate is taking place globally. AI has direct implications for national security, military capabilities, and global economic competitiveness. This leads to pressures to stake out parochial positions, even as many of the issues are global in nature. The EU\u2019s AI Act will apply to non-EU companies providing AI services in Europe and will set a precedent (like its privacy rules) that other countries will likely follow. 31 countries have passed AI legislation and 13 more are debating AI laws. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":"What requirements must developers of high-risk AI systems meet under the EU AI Act?","output":"Developers of high-risk AI systems must adhere to a comprehensive set of requirements including risk management, data governance, transparency, and human oversight, and must register these systems in an EU-wide database.","context":"Although existing legislation provides some protection, it is insufficient to address the specific challenges AI systems may bring.\n\nThe proposed rules will:\naddress risks specifically created by AI applications;\npropose a list of high-risk applications;\nset clear requirements for AI systems for high risk applications;\ndefine specific obligations for AI users and providers of high risk applications;\npropose a conformity assessment before the AI system is put into service or placed on the market;\npropose enforcement after such an AI system is placed in the market;\npropose a governance structure at European and national level.\nA risk-based approach The EU\u2019s second category is \u201chigh risk\u201d AI, which is permitted but requires assessments before AI is released in the market and afterwards. This includes rigorous testing, documentation of data quality, and an accountability framework including human oversight. High risk services include autonomous vehicles, medical devices, critical infrastructure, education, and government services. Providers of high-risk AI must register their AI in an EU database managed by the Commission before introducing them in the market. Non-EU providers will require an authorized representative in the EU showing that they comply and have post-market monitoring. \n\nThird, \u201climited risk\u201d AI systems have minimal transparency requirements so that users can make informed decisions. This includes generating or manipulating images, audio or video. \n\nFourth, \u201clow\/minimal risk\u201d AI systems have no new obligations, but likely voluntary codes of conduct. Co-rapporteur Dragos Tudorache (Renew, Romania) said: \u201cThe EU is the first in the world to set in place robust regulation on AI, guiding its development and evolution in a human-centric direction. The AI Act sets rules for large, powerful AI models, ensuring they do not present systemic risks to the Union and offers strong safeguards for our citizens and our democracies against any abuses of technology by public authorities. It protects our SMEs, strengthens our capacity to innovate and lead in the field of AI, and protects vulnerable sectors of our economy. The European Union has made impressive contributions to the world; the AI Act is another one that will significantly impact our digital future\u201d. The EU\u2019s AI Act intends to be the \u201cworld\u2019s first comprehensive AI law\u201d. Central to the EU\u2019s approach, AI systems are classified into four tiers of risk, and different tiers are subject to different regulations. Implementation will be a challenge, for example even defining AI systems and AI risks is problematic. EU businesses have released an open letter stating it \u201cwould jeopardize Europe's competitiveness and technological sovereignty without effectively tackling the challenges we are and will be facing\u201d. A new EU AI office would be created to monitor enforcement. Penalties include fines of up to 6% of total worldwide revenue. Citizens also have the right to file complaints against the AI provider. \n\nAI that falls in the EU\u2019s highest risk category of \u201cunacceptable risk\u201d is prohibited, with certain limited exceptions. Examples include social scoring that classifies people based on behavior and socio-economic status, or real-time biometric identification like face recognition."}
{"input":"What were the main goals of China's New Generation Artificial Intelligence Development Plan (AIDP) launched in July 2017?","output":"China's AIDP aimed to establish China as the world leader in AI by 2030, transform AI into a major economic force, and become a primary influencer in setting global ethical norms and standards for AI technology.","context":"2.1 The New generation artificial intelligence development plan (AIDP)\nReleased in July 2017 by the State Council (which is the chief administrative body within China), the \u2018New Generation Artificial Intelligence Development Plan\u2019 (AIDP) acts as a unified document that outlines China\u2019s AI policy objectives. Chinese media have referred to it as \u2018year one of China\u2019s AI development strategy\u2019 (\u201cChina AI Development Report\u201d 2018, p. 63). The overarching aim of the policy, as articulated by the AIDP, is to make China the world centre of AI innovation by 2030, and make AI \u2018the main driving force for China\u2019s industrial upgrading and economic transformation\u2019 (AIDP 2017). The AIDP also indicates the importance of using AI in a broader range of sectors, including defence and social welfare, and focuses on the need to develop standards and ethical norms for the use of AI. Altogether, the Plan provides a comprehensive AI strategy and challenges other leading powers in many key areas. The Plan will be guided by a new AI Strategy Advisory Committee, established in November 2017, and will be coordinated by the Ministry of Science and Technology (MIST), alongside the AI Plan Promotion Office, and other relevant bodies (\u201cAI Policy-China\u201d n.d.).Footnote3 Although these bodies will provide central guidance, the Plan is not meant to act as a centrally enacted initiative. The AIDP instead functions as a stamp of approval for de-risking and actively incentivising local projects that make use of AI. Recognising this point is important: the AIDP is an ambitious strategy set by the central government, but the actual innovation and transformation is expected to be driven by the private sector and local governments. In other words, it is more appropriate to view the AIDP as a highly incentivised \u2018wish list\u2019, to nudge, and coordinate other relevant stakeholders, rather than a central directive (Sheehan 2018). This is why the 3-year plan promoting the AIDP (2018\u20132020) emphasises mentioned AI as one of the six critical areas for developing the country\u2019s emerging industries (CCP 2016), and as an important factor in stimulating economic growth. When read together, these documents indicate that there has been a conscious effort to develop and use AI in China for some time, even before \u2018the Sputnik moment\u2019. However, prior to 2016, AI was presented merely as one technology among many others, which could be useful in achieving a range of policy goals. This changed with the release of the AIDP. The AIDP should not be read in isolation when considering China\u2019s AI strategy (Ding 2018), but it does provide the most transparent and influential indication of the driving forces behind China\u2019s AI strategy. Because of the AIDP\u2019s significance (in terms of policy) and importance (in terms of strategy), in the rest of this article, we shall use it as the organisational skeleton for explaining the drivers and ethical boundaries shaping China\u2019s approach to AI."}
{"input":"How is China implementing its AI development strategy according to the AIDP?","output":"China's AI development strategy involves creating a conducive environment for AI by 2020, achieving significant breakthroughs in AI technology by 2025, and becoming the global innovation center for AI by 2030.","context":"The AIDP should not be read in isolation when considering China\u2019s AI strategy (Ding 2018), but it does provide the most transparent and influential indication of the driving forces behind China\u2019s AI strategy. Because of the AIDP\u2019s significance (in terms of policy) and importance (in terms of strategy), in the rest of this article, we shall use it as the organisational skeleton for explaining the drivers and ethical boundaries shaping China\u2019s approach to AI. A limited number of reports have attempted to assess the plausibility of China\u2019s AI strategy given China\u2019s current technical capabilities (Ding 2018; \u201cChina AI Development Report\u201d 2018). Others have sought to understand specific areas of development, for instance, security or economic growth (Barton et al. 2017; \u201cNet Impact of AI on jobs in China\u201d 2018; Allen 2019). However, to grasp the ramified implications and direction of the AIDP, it is insufficient to analyse specific elements in isolation or to consider only technical capabilities. Instead, a more comprehensive and critical analysis of the driving forces behind China\u2019s AI strategy, its political economy, cultural specificities, and the current relevant policy debates, is required to understand China\u2019s AI strategy. This is the task we undertake in this article. 2.1 The New generation artificial intelligence development plan (AIDP)\nReleased in July 2017 by the State Council (which is the chief administrative body within China), the \u2018New Generation Artificial Intelligence Development Plan\u2019 (AIDP) acts as a unified document that outlines China\u2019s AI policy objectives. Chinese media have referred to it as \u2018year one of China\u2019s AI development strategy\u2019 (\u201cChina AI Development Report\u201d 2018, p. 63). The overarching aim of the policy, as articulated by the AIDP, is to make China the world centre of AI innovation by 2030, and make AI \u2018the main driving force for China\u2019s industrial upgrading and economic transformation\u2019 (AIDP 2017). The AIDP also indicates the importance of using AI in a broader range of sectors, including defence and social welfare, and focuses on the need to develop standards and ethical norms for the use of AI. Altogether, the Plan provides a comprehensive AI strategy and challenges other leading powers in many key areas. The Plan will be guided by a new AI Strategy Advisory Committee, established in November 2017, and will be coordinated by the Ministry of Science and Technology (MIST), alongside the AI Plan Promotion Office, and other relevant bodies (\u201cAI Policy-China\u201d n.d.).Footnote3 Although these bodies will provide central guidance, the Plan is not meant to act as a centrally enacted initiative. The AIDP instead functions as a stamp of approval for de-risking and actively incentivising local projects that make use of AI. Recognising this point is important: the AIDP is an ambitious strategy set by the central government, but the actual innovation and transformation is expected to be driven by the private sector and local governments. In other words, it is more appropriate to view the AIDP as a highly incentivised \u2018wish list\u2019, to nudge, and coordinate other relevant stakeholders, rather than a central directive (Sheehan 2018). This is why the 3-year plan promoting the AIDP (2018\u20132020) emphasises"}
{"input":" What ethical considerations are highlighted in China's approach to AI development?","output":" Ethical considerations in China's AI development include establishing normative boundaries for acceptable AI uses, emphasizing the importance of ethical norms, standards, and the balance between government interests and ethical concerns in AI applications.","context":"China is a central actor in the international debate on the development and governance of AI. It is important to understand China\u2019s internal needs, ambitions in the international arena, and ethical concerns, all of which are shaping the development of China\u2019s AI policies. It is also important to understand all this not just externally, from a Western perspective, but also internally, from a Chinese perspective. However, some ethical safeguards, constraints and desiderata are universal and are universally accepted and cherished, such as the nature and scope of human rights.Footnote15 They enable one to evaluate, after having understood, China\u2019s approach to the development of AI. This is why in this article we have sought to contribute to a more comprehensive and nuanced analysis of the structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory of, China\u2019s AI strategy. It emphasises that the Chinese government is aware of the potential benefits, practical risks, and the ethical challenges that AI presents, and that the direction of China\u2019s AI strategy will largely be determined by the interplay of these factors and by the extent to which government\u2019s interests may outweigh ethical concerns. Section 5 concludes the paper by summarising the key findings of our analysis. structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible To provide this contextualised understanding, Sect. 2 maps relevant AI legislation in China. We argue that, although previous policy initiatives have stated an intent to develop AI, these efforts have been fractious and viewed AI as one of many tools in achieving a different set goal. In contrast, the AIDP is the first national-level legislative effort that focuses explicitly on the development of AI as a unified strategy. Following this, Sect. 3 analyses the interventions and impact of the AIDP on three strategic areas identified in the document, namely: international competition, economic growth, and social governance. Section 4 focuses on China\u2019s aim to develop ethical norms and standards for AI. There we argue that, although the debate is in its early stages, the desire to define normative boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory"}
{"input":" What is the UK Government's vision for AI, as mentioned in the document?","output":" The UK Government aspires to make the UK an 'AI superpower,' leveraging AI development to benefit society and the economy, and plans to host a global summit on AI.","context":"POSTnote 708\nBy Ansh Bhatnagar, Devyani Gajjar\n9 January 2024\nPolicy implications of artificial\nintelligence (AI)\n Overview\n\u2022   Artificial intelligence (AI) is developing at a rapid pace and can be found throughout society \nin a growing range of everyday applications and decision- making. There are implications for \nsecurity, privacy, transparency, liability, labour rights, intellectual property and \ndisinformation. It presents some risks and benefits to democracy more widely.\n\u2022   There is no dedicated AI legislation in the UK. Existing legislation restricts how AI can be \nused in practice, such as in relation to data protection, equality and human rights, and \nintellectual property.\n\u2022   In March 2023, the UK Government announced a \u2018pro-innovation\u2019 approach to AI regulation, which \nlargely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, signed by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety.\nThe rapid advancement of AI capabilities means that policy implications are continuously shifting. \nSome stakeholders have called for further Government action.\n* The Kings Speech announced an Automated Vehicles Bill that seeks to introduce new legal \nframeworks to support safe commercial developments of self-driving vehicles.16\n\u2020 An exascale supercomputer is a computer that can perform 1018 (a quintillion) operations per \nsecond.\n2                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nTable: Definitions of AI and related concepts\nDefinitions are not universally agreed, are continuously evolving, and are linked.\nTerm                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise."}
{"input":" What approach does the UK propose for AI regulation compared to the EU?","output":" Unlike the EU's rules-based approach, the UK proposes a contextual, sector-based regulatory framework, utilizing existing regulators and laws, supplemented by new 'central functions' to support AI regulation.","context":"largely regulates AI via existing laws enforced by existing regulators. It outlined cross-sectoral \nprinciples, such as safety, security, robustness, transparency, fairness, accountability, \ncontestability, and redress, for existing regulators to consider. The approach applies to the whole \nof the UK, although some policy areas are devolved.\n\u2022  The Government has brought forward legislation and regulatory action on automated vehicles and \ndata protection and digital information.\n\u2022   Some stakeholders have indicated that additional legislation and action may be required, \nincluding mandatory impact assessments, bans on certain AI applications, and a right for human \nintervention to challenge AI decision- making. There are concerns that regulators are not currently \nequipped with the staffing, expertise or funding to regulate AI.\nPolicy implications of artificial intelligence (AI)\nBackground AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] UK Government Publishes AI Regulatory Framework\nApril 04, 2023\nThe UK government published a white paper on March 29 setting out a \u201cpro-innovation\u201d UK regulatory framework for artificial intelligence (AI). The framework centers upon five cross-sectoral principles, of which implementation will be context-specific to the use of AI, rather than the technology itself. The government does not propose introducing a new regulator or any new legal requirements on businesses, instead leveraging existing powers of UK regulators and their domain-specific expertise. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":" How does the document address the challenge of regulating biometric data and AI?","output":" It emphasizes the need for new rights and protections to govern biometric technologies effectively, highlighting a lack of widespread public support for the use of biometrics without clear limitations and safeguards.","context":"Banned applications: Recognising the potential threat to citizens\u2019 rights and democracy posed by certain applications of AI, the co-legislators agreed to prohibit: biometric categorisation systems that use sensitive characteristics (e.g. political, religious, philosophical beliefs, sexual orientation, race); untargeted scraping of facial images from the internet or CCTV footage to create facial recognition databases; emotion recognition in the workplace and educational institutions; social scoring based on social behaviour or personal characteristics; AI systems that manipulate human behaviour to circumvent their free will; AI used to exploit the vulnerabilities of people (due to their age, disability, social or economic situation). Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI\nPress Release 09-12-2023 - 00:04 \nSummary:\n1. Safeguards agreed on general purpose artificial intelligence\n2. Limitation for the of use biometric identification systems by law enforcement\n3. Bans on social scoring and AI used to manipulate or exploit user vulnerabilities\n4. Right of consumers to launch complaints and receive meaningful explanations\n5. Fines ranging from 35 million euro or 7% of global turnover to 7.5 million or 1.5% of turnover with existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA).\nThe Act defines different risk levels with corresponding levels of regulation. It bans certain \nhigh-risk applications, such as live facial recognition.\nIn the US, a \u2018Blueprint for an AI Bill of Rights\u2019 has been outlined.233 Currently these are \nnon-binding guidelines that aim to address discrimination, data privacy, and transparency. In \nOctober 2023, US President Joe Biden signed an Executive Order on AI mandating standards and \ndisclosures for the largest AI companies, and measures to protect workers and disadvantaged \ngroups.234\nPotential future regulations\nHuman intervention in automated decision- making\nSome civil society groups235 and academics236,237 have suggested a law to enshrine a right to human \nintervention in automated decision-making. There are two ways to implement this: GOV.UK.\n245. Trades Union Congress (2023). Briefing: Data Protection and Digital Information Bill (V2). \nTUC.\n246. Yuill, B. (2023). How the new Data Bill waters down protections. Public Law Project.\n247. Hill, K. (2012). How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did. \nForbes.\n248. Waldman, A. E. (2019). Power, Process, and Automated Decision- Making. Fordham Law Rev., Vol \n88, 613\u2013632.\n249. Equality and Human Rights Commission (2020). Civil and political rights in Great Britain: \nsubmission to the UN. EHRC.\n250. Rahman-Jones, I. et al. (2023). AI facial recognition: Campaigners and MPs call for ban. BBC \nNews.\n251. Science, Innovation and Technology Committee (2023). Science, Innovation and Technology \nCommittee Oral evidence: Governance of artificial intelligence (AI), HC 945. House of Commons.\n252. Science, Innovation and Technology Committee The governance of artificial intelligence: \ninterim report. House of Commons."}
{"input":" What is the primary aim of the AI Ecological Education Policy Framework proposed in the study?","output":" The AI Ecological Education Policy Framework aims to address the multifaceted implications of AI integration in university teaching and learning, organized into Pedagogical, Governance, and Operational dimensions.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021"}
{"input":" How does the study suggest students should be involved in AI policy planning and implementation in universities?","output":" The study suggests that students should play an active role in drafting and implementing AI policy in universities, ensuring their perspectives and needs are considered.","context":"workshops to discuss challenges and support local\npractitioners and start-ups.7 Additionally, citizen\nand civil society engagement and involvement\nin AI policymaking is often used to build trust\nin AI adoption among society. This can transpire\nthrough public consultations and workshops to\ninform the development of AI policies. Universities\nand academic institutions also play an active role in\nsupporting local AI research and applications. Policy implementation\nInvesting in AI R&D\nAI Research Competence Centres\nResponsible entity: German Federal Ministry of Education and Research\nDescription: Some strategies call for the establishment of AI hubs, research\ncentres that will help with R&D efforts as they relate to AI. As part of Germany\u2019s\nAI strategy, \u201cAI research competence centres\u201d in Munich, T\u00fcbingen, Berlin,\nDortmund, and Dresden aim to become more deeply interconnected, and\nfunding from the government will double by 2022.\nRead more: https:\/\/www.oecd.ai\/dashboards\/countries\/Germany.\n\nAN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021 and non-profit communities when developing educational programmes, tools\nand technologies.\nBox 2. Key considerations when implementing AI strategies and policies\nWhen designing and implementing AI policies and strategies, policy makers should\nconsider the following to ensure they foster innovation while promoting the\nimplementation of trustworthy, human-centred AI systems:\n\u2022 1.\nScientific research: Policies include the\ncreation of new research centers, hubs, partnerships\nor programs in AI research.\n\n5.\nData access: Policies to increase access\nto quality data for machine learning models by\nimproving data sharing infrastructures and rules\nfor data portability. Initiatives include opening\npublic datasets, creating new open data platforms,\ndata marketplaces, data trusts or local annotated\ndatasets.\n\n2.\nAI talent development: Policies include\nmeasures to train domestic talent or to attract\nand retain international talent, including funding\nfor AI-specific Master\u2019s degree, PhD and other\nacademic programs. This category also includes\n\n17 https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence\n\nworldbank.org\/digitaldevelopment\n12\n\n\fANALYTICAL INSIGHTS - NOTE 4"}
{"input":" What strategies does the document recommend for integrating AI in higher education while addressing ethical and practical concerns?","output":" The document recommends strategies such as interdisciplinary planning, policies on equitable and ethical use of AI, developing a master plan for AI in education, pilot testing and evaluation, and fostering local AI innovations for education.","context":"(d)  To help ensure the responsible development and deployment of AI in the education sector, the Secretary of Education shall, within 365 days of the date of this order, develop resources, policies, and guidance regarding AI.  These resources shall address safe, responsible, and nondiscriminatory uses of AI in education, including the impact AI systems have on vulnerable and underserved communities, and shall be developed in consultation with stakeholders as appropriate.  They shall also include the development of an \u201cAI toolkit\u201d for education leaders implementing recommendations from the Department of Education\u2019s AI and the Future of Teaching and Learning report, including appropriate human review of AI decisions, designing AI systems to enhance trust and safety and align with privacy-related laws and regulations in the educational context, and developing education-specific guardrails. are then adopted to leverage private sector skills with agility to allow\ninfrastructure, training in digital\niterative, problem-driven approaches to requests-for-proposals.\nskills, and research\n\u00bb Promote transparency and accountability through inclusion and multi\u2022 Identify and manage risks at all\nstakeholder engagement at every step of the AI policy design and\nstages of implementation\nimplementation. Implementation teams should also manage the risks\nassociated with AI, such as bias, security, and unintended consequences.\n\u00bb Manage adverse ethical implications of AI through broader economic policies including those that aim to develop human\ncapital, ensure fair competition, and incentivize human-enhancing AI solutions, among others.\n\u00bb Promote training in digital skills, and broader education in science, technology, engineering, and mathematics (STEM) to\nsupport people as they adjust to the shifting nature of work in the coming decades. Unskilled people and disadvantaged In April 2023, EPIC Senior Counsel Ben Winters and Equal Justice Works Fellow Grant Fergusson published a series of blog posts explaining how each of these instruct how entities using AI can do it more responsibly.\n\nUniversal Guidelines for Artificial Intelligence\nIn October 2018, over 250 experts and 60 organizations, representing more than 40 countries, endorsed the Universal Guidelines for Artificial Intelligence (\u201cUGAI\u201d). The guidelines were organized by the Public Voice. The guidelines in full are: emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to establish ethical principles but did form an AI ethics committee in mid-2018 with this purpose in mind (\u201cAI association to draft ethics guidelines\u201d 2019)."}
{"input":" What executive action did the Biden-Harris Administration take in Fall 2023 regarding AI, and what are its key requirements?","output":" In Fall 2023, the Biden-Harris Administration issued an Executive Order entitled Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. The order emphasizes the need for regulation of high-risk AI and recognizes the link between privacy and AI. It requires developers of the most powerful AI systems to share their safety test results with the government, promises federal support for the development and use of privacy-preserving techniques, mandates an evaluation of how agencies collect and use commercially available data, and requires increased training on investigating and prosecuting civil rights violations related to AI.","context":"U.S. FEDERAL STRATEGY\nThere have been no major legislative movement in Congress, but plenty of relevant bills introduced in the last several years. Anna Lenhart has published a report about the different ways legislation introduced in Congress would impact AI.\n\nFall 2023 Executive Action: Executive Order and an Office of Management & Budget Memo\nOn October 30, 2023, the Biden-Harris Administration issued an Executive Order entitled \u201cSafe, Secure, and Trustworthy Development and Use of Artificial Intelligence\u201dthat emphasizes the need for regulation of high-risk AI and critically recognizes the link between privacy and AI. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies with all Federal laws and to promote robust technical evaluations, careful oversight, engagement with affected communities, and rigorous regulation.  It is necessary to hold those developing and deploying AI accountable to standards that protect against unlawful discrimination and abuse, including in the justice system and the Federal Government.  Only then can Americans trust AI to advance civil rights, civil liberties, equity, and justice for all."}
{"input":" What are the five major principles outlined in the Blueprint for an AI Bill of Rights released by the Office of Science and Technology Policy in Fall 2022?","output":" The five major principles are: Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. These principles are aimed at ensuring that AI systems are developed and used in a manner that protects individuals from abuses and promotes fairness, privacy, and accountability.","context":"Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. To advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by From Principles to Practice\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, or access to critical needs. Monday\u2019s White House action builds on its previous commitments to the safe and responsible development of AI. Some of these include its support for developing the Blueprint for an AI Bill of Rights, which safeguards Americans\u2019 rights and safety; securing voluntary commitments from Anthropic, Google, OpenAI, and other tech companies to thoroughly test systems before releasing them \u2013 also called \u201cred-teaming\u201d \u2013 and clearly labeling all AI-generated content; and granting $140 million to the National Science Foundation to establish seven new AI research institutes. The Order is also meant to complement the G7 leaders\u2019 Guiding Principles and Code of Conduct, which was also issued on Monday. (See below for more details.) with existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA).\nThe Act defines different risk levels with corresponding levels of regulation. It bans certain \nhigh-risk applications, such as live facial recognition.\nIn the US, a \u2018Blueprint for an AI Bill of Rights\u2019 has been outlined.233 Currently these are \nnon-binding guidelines that aim to address discrimination, data privacy, and transparency. In \nOctober 2023, US President Joe Biden signed an Executive Order on AI mandating standards and \ndisclosures for the largest AI companies, and measures to protect workers and disadvantaged \ngroups.234\nPotential future regulations\nHuman intervention in automated decision- making\nSome civil society groups235 and academics236,237 have suggested a law to enshrine a right to human \nintervention in automated decision-making. There are two ways to implement this:"}
{"input":" What are the OECD AI Principles, and how do they propose to ensure the responsible use of AI?","output":" Adopted in 2019 and endorsed by 42 countries, the OECD AI Principles propose to ensure AI's responsible use through inclusive growth, sustainable development, human-centered values and fairness, transparency and explainability, robustness, security and safety, and accountability. These principles are designed to promote AI systems that benefit people and the planet while respecting human rights, and democratic values, and ensuring safety and security throughout their lifecycle.","context":"\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI.\n\nTo develop practical guidance to implement the AI Principles as mandated by the\nOECD Council, the OECD convened a multi-stakeholder and multi-disciplinary OECD\nNetwork of Experts on AI in early 2020 that is developing a report on the State of\nimplementation of the OECD AI Principles: Insights from national AI policies (OECD,\n2021[1]). The report provides good practices and lessons learned on the\nimplementation of the five recommendations to policy makers contained in the\nOECD AI Principles.\nSource: oecd.ai\/ai-principles and oecd.ai\/network-of-experts. is a relatively new phenomenon. To track these initiatives, the OECD AI Policy\nObservatory (OECD.AI) comprises over 620 national AI policies from over 60\ncountries and the European Union (EU). These resources provide a baseline to\nmap countries\u2019 AI policy initiatives according to the recommendations to\ngovernments contained in the OECD AI Principles (Box 1).\nBox 1. OECD AI Principles\nThe OECD Principles on Artificial Intelligence promote AI that is innovative,\ntrustworthy and respects human rights and democratic values. They were adopted\nin May 2019 by OECD member countries when they approved the OECD Council\nRecommendation on Artificial Intelligence. The OECD AI Principles identify five\ncomplementary values-based principles for the responsible stewardship of\ntrustworthy AI:\n\u2022 \u2022\n\nOrganisations and individuals developing, deploying or operating AI\nsystems should be held accountable for their proper functioning in line with\nthe above principles\n\nConsistent with these principles, the OECD also provides five recommendations to\ngovernments:\n\u2022\n\nFacilitate public and private investment in research & development to spur\ninnovation in trustworthy AI.\n\n\u2022\n\nFoster accessible AI ecosystems with digital infrastructure and technologies\nand mechanisms to share data and knowledge.\n\n\u2022\n\nEnsure a policy environment that will open the way to the deployment of\ntrustworthy AI systems.\n\n\u2022\n\nEmpower people with the skills for AI and support workers for a fair\ntransition.\n\n\u2022\n\nCo-operate across borders and sectors to progress on responsible\nstewardship of trustworthy AI. | 25\nInstitute for Research in Digital Science and Technology (INRIA) in Paris) support\nthe operation of four expert working groups on: Responsible AI (Montr\u00e9al);\nData Governance (Montr\u00e9al); the Future of Work (Paris); and Innovation &\nCommercialisation (Paris).\nRead more: https:\/\/gpai.ai\/.\nOECD Network of Experts on AI (ONE AI)\nResponsible entity: Organisation of Economic Co-operation and Development\n(OECD)\nDescription: OECD member countries adopted a set of AI principles in May\n2019, the first set of intergovernmental principles and recommendations to\ngovernments for trustworthy AI. In early 2020, the OECD launched OECD.AI, a\nplatform to share and shape AI policies that provide data and multidisciplinary\nanalysis on artificial intelligence. Also in early 2020, the OECD\u2019s Committee on\nDigital Economy Policy tasked the OECD Network of Experts on AI (ONE AI)\nwith proposing practical guidance for implementing the OECD AI principles for"}
{"input":" What is the Department of State's stance on AI and its global impact?","output":" The Department of State recognizes AI as central to the global technological revolution, acknowledging both the opportunities and challenges it presents. It aims to further scientific and technological capabilities, promote democracy and human rights, and work with partners and allies to establish shared norms for responsible AI use.","context":"to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Europe is likely to emerge, and private companies will advance their own \u201cresponsible AI\u201d initiatives and face a fragmented global AI regulatory landscape. Looming China competition will escalate a \u201cdon\u2019t fall behind\u201d debate. The Federal Trade Commission (especially) and Department of Justice likely fire warning shots to forestall AI concentration in big tech. There is a real but less likely chance that a key U.S. state (e.g., California) passes major AI legislation, or that a big AI-related disaster leads to a strong national response. The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. My Administration places the highest urgency on governing the development and use of AI safely and responsibly, and is therefore advancing a coordinated, Federal Government-wide approach to doing so.  The rapid speed at which AI capabilities are advancing compels the United States to lead in this moment for the sake of our security, economy, and society.\n\n     In the end, AI reflects the principles of the people who build it, the people who use it, and the data upon which it is built.  I firmly believe that the power of our ideals; the foundations of our society; and the creativity, diversity, and decency of our people are the reasons that America thrived in past eras of rapid change.  They are the reasons we will succeed again in this moment.  We are more than capable of harnessing AI for justice, security, and opportunity for all."}
{"input":" How does the Department of State contribute to international AI policy through the OECD?","output":" The Department provides policy guidance and leadership in the OECD AI Policy Observatory, facilitating dialogue and evidence-based policy analysis. It supports the OECD Network of Experts on AI (ONE AI) and contributes to 47 AI initiatives, including COVID-19 response and safety guidance for automated transportation.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 13 AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f16 | AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f| 21 is a relatively new phenomenon. To track these initiatives, the OECD AI Policy\nObservatory (OECD.AI) comprises over 620 national AI policies from over 60\ncountries and the European Union (EU). These resources provide a baseline to\nmap countries\u2019 AI policy initiatives according to the recommendations to\ngovernments contained in the OECD AI Principles (Box 1).\nBox 1. OECD AI Principles\nThe OECD Principles on Artificial Intelligence promote AI that is innovative,\ntrustworthy and respects human rights and democratic values. They were adopted\nin May 2019 by OECD member countries when they approved the OECD Council\nRecommendation on Artificial Intelligence. The OECD AI Principles identify five\ncomplementary values-based principles for the responsible stewardship of\ntrustworthy AI:\n\u2022"}
{"input":" How does the United States approach the use of AI in military operations and international humanitarian law?","output":" The United States believes international humanitarian law provides a robust framework for regulating weapons, including AI-powered autonomous functions. It supports international discussions to understand AI's risks and benefits in military operations, particularly its potential to enhance compliance with humanitarian law and reduce civilian harm.","context":"(i)   lead efforts outside of military and intelligence areas to expand engagements with international allies and partners in relevant bilateral, multilateral, and multi-stakeholder fora to advance those allies\u2019 and partners\u2019 understanding of existing and planned AI-related guidance and policies of the United States, as well as to enhance international collaboration; and\n\n          (ii)  lead efforts to establish a strong international framework for managing the risks and harnessing the benefits of AI, including by encouraging international allies and partners to support voluntary commitments similar to those that United States companies have made in pursuit of these objectives and coordinating the activities directed by subsections (b), (c), (d), and (e) of this section, and to develop common regulatory and other accountability principles for foreign nations, including to manage the risk that AI systems pose. INTERNATIONAL LAWS\nSeveral other counties are more advanced than the U.S. in terms of AI policy development that works to protect people from algorithmic harm. For more information on AI laws and norms internationally, please see EPIC\u2019s International Policy page. (a)  provide guidance to the Department of Defense, other relevant agencies, and the Intelligence Community on the continued adoption of AI capabilities to advance the United States national security mission, including through directing specific AI assurance and risk-management practices for national security uses of AI that may affect the rights or safety of United States persons and, in appropriate contexts, non-United States persons; and\n\n     (b)  direct continued actions, as appropriate and consistent with applicable law, to address the potential use of AI systems by adversaries and other foreign actors in ways that threaten the capabilities or objectives of the Department of Defense or the Intelligence Community, or that otherwise pose risks to the security of the United States or its allies and partners.  \n\n     Sec. 5. Promoting Innovation and Competition. The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with"}
{"input":" What is the purpose of the new policies announced by the Department of Homeland Security (DHS) regarding AI?","output":" The new policies aim to ensure the responsible use of artificial intelligence by DHS, focusing on harnessing AI's benefits while managing its risks. These policies, developed by the DHS Artificial Intelligence Task Force, cover the use of technologies like face recognition to advance DHS missions in a manner that respects privacy, civil rights, and civil liberties.","context":"\u201cArtificial intelligence is a powerful tool we must harness effectively and responsibly,\u201d said Secretary of Homeland Security Alejandro N. Mayorkas. \u201cOur Department must continue to keep pace with this rapidly evolving technology, and do so in a way that is transparent and respectful of the privacy, civil rights, and civil liberties of everyone we serve. I am grateful that Eric Hysen, who already co-chairs our Artificial Intelligence Task Force, has agreed to serve as our Department\u2019s first Chief AI Officer, providing the leadership and experience necessary to harness AI\u2019s enormous potential and ensure its responsible use across DHS.\u201d\n\n\u202f\u201cArtificial intelligence provides the Department with new ways to carry out our mission to secure the homeland,\u201d said CIO and Chief AI Officer Eric Hysen. \u201cThe policies we are announcing today will ensure that the Department\u2019s use of AI is free from discrimination and in full compliance with the law, ensuring that we retain the public's trust.\u201d Policy Statement 139-06 \u201cAcquisition and Use of Artificial Intelligence and Machine Learning by DHS Components:\u201d The policy statement establishes the foundation for DHS\u2019s use of AI with a clear set of principles. These principles include that DHS systems, programs, and activities using AI will conform to the requirements of Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in\u202fthe Federal Government. In addition, DHS will only acquire and use AI in a manner that is consistent with the Constitution and all other applicable laws and policies. Also, DHS will not collect, use, or disseminate data used in AI activities, or establish AI-enabled\u202fsystems that make or support decisions, based on the inappropriate consideration of race, ethnicity, gender, national origin, religion, gender, sexual orientation, gender identity, age, nationality, medical condition, or disability. DHS Names First Chief Artificial Intelligence Officer\u202f\n\n\u202fWASHINGTON \u2013 Today, the Department of Homeland Security (DHS) announced new policies to ensure responsible use of artificial intelligence (AI) by the Department. The policies, developed by the DHS Artificial Intelligence Task Force (AITF), build on a commitment from the Biden-Harris Administration to manage the risk and harness the benefits of AI. The Department uses AI technologies to advance its missions, including combatting fentanyl trafficking, strengthening supply chain security, countering child sexual exploitation, and protecting critical infrastructure. These new policies establish key principles for the responsible use of AI and specify how DHS will ensure that its use of face recognition and face capture technologies is subject to extensive testing and oversight. In March 2023, Secretary Mayorkas tasked the Homeland Security Advisory Council to\u202fexamine and provide recommendations on the development of the Department\u2019s AI Strategy. The Council has formed two subcommittees, one to focus on how DHS can leverage AI to advance critical missions, and the second on how DHS should be building defenses to nefarious uses of AI by adversaries. The Council provides recommendations to the Secretary on ways the Department can better meet the challenges of the evolving threat landscape and seize opportunities to better serve the American people."}
{"input":" What are the differences between Policy Statement 139-06 and Directive 026-11?","output":" The key differences between Policy Statement 139-06 and Directive 026-11 are in their focus and specific mandates. Policy Statement 139-06 provides a foundational framework for DHS's AI use, emphasizing conformity with Executive Order 13960 and legal and constitutional adherence, with a strong stance against discrimination. In contrast, Directive 026-11 specifically addresses the use of face recognition and capture technologies, detailing requirements for testing against bias, periodic evaluations, and offering U.S. citizens the right to opt out of non-law enforcement uses, alongside a comprehensive review process to ensure these technologies meet established standards and respect civil liberties.","context":"(d)  To assist agencies in implementing the guidance to be established in subsection 10.1(b) of this section:\n\n          (i)   within 90 days of the issuance of the guidance, the Secretary of Commerce, acting through the Director of NIST, and in coordination with the Director of OMB and the Director of OSTP, shall develop guidelines, tools, and practices to support implementation of the minimum risk-management practices described in subsection 10.1(b)(iv) of this section; and\n\n          (ii)  within 180 days of the issuance of the guidance, the Director of OMB shall develop an initial means to ensure that agency contracts for the acquisition of AI systems and services align with the guidance described in subsection 10.1(b) of this section and advance the other aims identified in section 7224(d)(1) of the Advancing American AI Act (Public Law 117-263, div. G, title LXXII, subtitle B). information is classified accordingly and restricted from disclosure pursuant to paragraph 14 of this\npolicy statement. framework.\n5. Guiding Principles. This Policy is based on five principles:\n\uf0b7 Maximizing access to information.\n\uf0b7 Setting out a clear list of exceptions.\n\uf0b7 Safeguarding the deliberative process.\n\uf0b7 Providing clear procedures for making information available.\n\uf0b7 Recognizing requesters\u2019 right to an appeals process.\nThis policy statement is divided into four parts. Following this introduction, Part II sets\nout the core policy, Part III sets out how the Bank handles information relating to\nmember countries and other parties, and Part IV sets out the implementation aspects of\nthe Policy.\nII. The Policy\n6. The Bank allows access to any information in its possession that is not on a list of\nexceptions (set out in Part II, Section A, of this policy statement). In addition, over time\nthe Bank declassifies and makes publicly available certain information that falls under the\nexceptions (as set out in Part IV, Section C, of this policy statement). Notwithstanding (iv)   Within 240 days of the completion of the guidelines described in subsection 4.3(a)(iii) of this section, the Assistant to the President for National Security Affairs and the Director of OMB, in consultation with the Secretary of Homeland Security, shall coordinate work by the heads of agencies with authority over critical infrastructure to develop and take steps for the Federal Government to mandate such guidelines, or appropriate portions thereof, through regulatory or other appropriate action.  Independent regulatory agencies are encouraged, as they deem appropriate, to consider whether to mandate guidance through regulatory action in their areas of authority and responsibility."}
{"input":" How does DHS intend to incorporate AI into its mission while addressing the challenges posed by its use?","output":" DHS plans to leverage AI to advance its critical missions while ensuring accuracy, fairness, and equity. This includes forming specialized groups for responsible AI use guidance, risk assessment, and mitigation strategies, as well as fostering a whole-of-government effort to develop safe, secure, and trustworthy AI technologies.","context":"To learn more about how DHS uses AI technologies to protect the country, visit DHS.gov. DHS Names First Chief Artificial Intelligence Officer\u202f\n\n\u202fWASHINGTON \u2013 Today, the Department of Homeland Security (DHS) announced new policies to ensure responsible use of artificial intelligence (AI) by the Department. The policies, developed by the DHS Artificial Intelligence Task Force (AITF), build on a commitment from the Biden-Harris Administration to manage the risk and harness the benefits of AI. The Department uses AI technologies to advance its missions, including combatting fentanyl trafficking, strengthening supply chain security, countering child sexual exploitation, and protecting critical infrastructure. These new policies establish key principles for the responsible use of AI and specify how DHS will ensure that its use of face recognition and face capture technologies is subject to extensive testing and oversight. Policy Statement 139-06 \u201cAcquisition and Use of Artificial Intelligence and Machine Learning by DHS Components:\u201d The policy statement establishes the foundation for DHS\u2019s use of AI with a clear set of principles. These principles include that DHS systems, programs, and activities using AI will conform to the requirements of Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in\u202fthe Federal Government. In addition, DHS will only acquire and use AI in a manner that is consistent with the Constitution and all other applicable laws and policies. Also, DHS will not collect, use, or disseminate data used in AI activities, or establish AI-enabled\u202fsystems that make or support decisions, based on the inappropriate consideration of race, ethnicity, gender, national origin, religion, gender, sexual orientation, gender identity, age, nationality, medical condition, or disability. In March 2023, Secretary Mayorkas tasked the Homeland Security Advisory Council to\u202fexamine and provide recommendations on the development of the Department\u2019s AI Strategy. The Council has formed two subcommittees, one to focus on how DHS can leverage AI to advance critical missions, and the second on how DHS should be building defenses to nefarious uses of AI by adversaries. The Council provides recommendations to the Secretary on ways the Department can better meet the challenges of the evolving threat landscape and seize opportunities to better serve the American people."}
{"input":" What are the main tools in the AI regulatory toolbox for evaluating algorithmic systems?","output":" The AI regulatory toolbox includes expanding transparency requirements, performing algorithmic audits, developing AI sandboxes, leveraging the AI assurance industry, and learning from whistleblowers. Each tool has its strengths and weaknesses and requires different levels of expertise and statutory authority from regulators.","context":"Spain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023\nWhile AI legislation advances, some regulators are experimenting with gathering information aboutalgorithmic systems and their potential societal effects.\nThis experimentation has developed a toolbox of AI regulatory strategies, each with differentstrengths and weaknesses.\nThese potential interventions include transparency requirements, algorithmic audits, AI sandboxes,leveraging the AI assurance industry, and welcoming whistleblowers.\nOf particular interest is how oversight agencies can learn about algorithmic systems,as well as their societal impact, harms, and legal compliance. As agencies experimentin gathering this information, it is possible to broadly characterize an emerging AIregulatory toolbox for evaluating algorithmic systems, particularly those with greaterrisk of harm. Governments around the world are implementing foundational policies to regulateartificial intelligence (AI) and algorithmic systems more generally. While legislation isadvancing, regulators should not wait idly for legislators to act. Instead, regulatorsshould be actively learning about the algorithmic systems in their regulatory domainand evaluating those systems for compliance under existing statutory authority.\nMany regulatory agencies have started this work, including the U.S. Federal TradeCommission\u2019s (FTC)\nand Consumer Financial Protection Bureau(CFPB), new algorithmic regulators in\nand\n, and onlineplatform regulators such as the UK\u2019s\n(OFCOM) and the\n. These agencies and others havestarted to implement novel approaches and policies for AI regulation.\nOffice of Technology\nthe Netherlands\nSpain\nOffice of Communications\nEuropean Centre for Algorithmic Transparency\nCOMMENTARY\nThe AI regulatory toolbox: How governments candiscover algorithmic harms\nAlex Engler\nOctober 9, 2023 Agencies should use the tools they have to understandand regulate AI\nRegulators should actively consider what steps are necessary and valuable in theirdomains to ensure their regulatory mission is preserved. This includes cataloging andobserving emerging uses of algorithmic systems in their field, exploring what theirexisting statutory authority allows for, and hiring staff with expertise in algorithmicsystems. Regulators may benefit from a gap analysis\u2014identifying where current\ndocumented\nthousands ofpages\nled to congressional hearings\noffer cash rewards for whistleblowers\nauthorities and capacities are lacking so that they can inform legislators, who are farless likely to understand the nuances of every regulatory subfield.\nWhile regulators may often lack the most appropriate and best suited tools forinformation gathering about algorithmic systems, many will have some authority toperform information gathering. Beyond the interventions explored here, regulators canalso learn from scientific research\nacademics\njournalists\nperformed an audit\nengaged in algorithmic audits\nhow involved they are\nAn AI regulatory sandbox is meant to systematically improve communication betweenregulators and regulated entities, most frequently AI developers. Participation in AIsandboxes, which is often voluntary, is meant to ease regulatory compliance and offerlegal certainty to companies while improving regulators\u2019 understanding of the design,development, and deployment of a type of AI system. This may also help regulatorsidentify potential legal problems with a particular AI system during its development. Inaddition to preventing harms, this can enable an AI developer to make earlier\u2014therebypotentially less costly\u2014course corrections on its algorithms."}
{"input":" How do algorithmic transparency requirements benefit AI regulation and what challenges might regulators face in implementing them?","output":" Algorithmic transparency requirements help in making the functioning of AI systems more visible to affected individuals, the general public, and other organizations, potentially leading to better-informed choices and improvements in AI systems. However, regulators need to carefully craft these requirements to avoid too much flexibility that allows companies to selectively disclose self-serving information.","context":"an individual right to \u201cmeaningful information aboutthe logic\u201d of algorithmic systems. This has led companies, such as\n, to offer responses\u2014albeit limited ones\u2014to requests for information aboutalgorithmic decisions. Although not yet passed into law, the forthcoming EU AI Act willalso create\nsubstantial new transparency requirements\n(https:\/\/www.brookings.edu\/articles\/the-eu-and-us-diverge-on-ai-regulation-atransatlantic-\ncomparison-and-steps-to-alignment\/#anchor5)\n, likely including directdisclosure of chatbots and public reporting about high-risk AI systems.\nTransparency requirements require little expertise and capacity from governmentagencies, making them an appealing early step in AI regulation. However, regulators doneed to be careful in specifying transparency requirements\u2014vaguely or poorly wordedrequirements can permit too much flexibility in algorithmic transparency, allowing forcompanies to cherry pick self-serving statistics. When transparency requirements are sufficiently narrowly tailored to a type ofalgorithmic system, they can lead to a wide range of benefits. Public information aboutan algorithmic function can help individuals and other businesses make better choicesabout which AI developers to patronize or work with. AI developers may themselvesrealize from public disclosures that their systems are not performing at the state of theart, leading them to prioritize product improvements. Yet, even if more transparencydoes not lead to introspection, public information can help journalists and civil societyorganizations identify subpar and potentially harmful systems. Journalism can lead topublic scrutiny that leads to change in the practices of AI developers, while civilsociety organizations may make use of lawsuits to punish lax algorithmic practices. Allthese benefits can arise even without the regulators themselves using publicinformation, although transparency also helps inform better policymaking The toolbox includes expanding transparency, performing algorithmic audits,developing AI sandboxes, leveraging the AI assurance industry, and learning fromwhistleblowers. These interventions have different strengths and weaknesses forgoverning different types of AI systems, and further, they require different internalexpertise and statutory authorities. To better inform AI policymaking, regulators shouldbe aware of these tools and their trade-offs.\nMandating corporate disclosures is a key function of many government agencies, andthis role is also valuable in markets of algorithmic systems. Algorithmic transparency isamong the most thoroughly studied subfields of AI, which has resulted in a widevariety of approaches, including transparency measures for affected individuals, thegeneral public, and to other organizations, such as other businesses or regulatorsthemselves. Appropriate transparency and explainability: Transparency refers to the communication of appropriate information about an AI system and explainability refers to the extent to which it is possible for relevant parties to access, interpret, and understand the decision-making processes. Parties directly affected by the use of an AI system should also be able to access sufficient information about AI systems to be able to enforce their rights. Regulators will likely implement this principle through regulatory guidance.\nFairness: AI systems should not undermine the legal rights of individuals or organizations, discriminate unfairly against individuals, or create unfair market outcomes (e.g., in equality and human rights, data protection, consumer law, or financial regulations). Regulators may implement this principle through a combination of guidance (sector specific and jointly), technical standards and assurance techniques, as well as enforcing existing statutory obligations."}
{"input":" How can engagement with the AI assurance industry benefit AI regulation?","output":" The AI assurance industry, comprising companies that specialize in monitoring, evaluation, and legal compliance of AI systems, can help advance democratic goals by offering tools for internal monitoring and legal compliance. Regulators can encourage the use of these tools as a signal of regulatory compliance and engage with the industry to inform and learn about specific technical functions and societal impacts of algorithmic systems.","context":"Although this industry is distinct from governance, regulators should actively engagewith the AI assurance industry to advance democratic goals, perhaps best\n. Regulators can issue guidance that encourages regulated companies toconsider using AI assurance tools, even possibly noting this could be interpreted as apotential signal of regulatory compliance. Further, regulators can inform and learn fromthe AI assurance industry. By communicating about specific technical functions andthe societal impacts of algorithmic systems in a regulated field, regulators can help AIassurance companies strive towards not just nominal compliance, but meaningfullybetter outcomes. For instance, regulators concerned with discrimination couldencourage relevant AI assurance companies to Leverage the AI Assurance Industry 4.\nthereproducibility that is necessary\nA common strand across the entire AI assurance industry is a mixed business modelthat advertises both profit-motivated improvements to algorithmic systems and betterpreparedness for regulatory and legal compliance. For instance, several AI assurancecompanies stress the value of internal monitoring, so corporate leaders canunderstand and scrutinize the function of their own algorithms, in addition tohighlighting future legal requirements. This likely a stronger sales pitch to potentialclients, especially given that most AI laws are still being drafted, rather than beingimplemented.\nAlthough this industry is distinct from governance, regulators should actively engagewith the AI assurance industry to advance democratic goals, perhaps best Accountability and governance: Businesses should put in place governance measures that ensure effective oversight of the supply and use of AI systems, with clear lines of accountability established across the AI lifecycle. Regulators will likely implement this principle through regulatory guidance and assurance techniques.\nContestability and redress: Users, impacted third parties, and actors in the AI lifecycle should be able to contest an AI decision or outcome that is harmful or creates material risk of harm. Regulators will be expected to clarify existing routes to contestability and redress and implement proportionate measures to ensure that the outcomes of AI use are contestable where appropriate. The government\u2019s initial non-statutory approach will not create new rights or new routes to redress at this stage. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":" Who says that AI is too important not to regulate?","output":" Google states that AI is too important not to regulate\u2014and too important not to regulate well.","context":"Governments should look beyond traditional regulatory techniques to influence AI developments I. The AI Regulation Debate: A Wide Spectrum of Views. The AI regulation debate has become heated and politicized, reflecting the immense hopes and fears that we have invested in AI.   \n\nHysterical fear and paranoia. On one end, leading technologist and venture capitalist Marc Andreessen  says \u201cwe have a full-blown moral panic about AI right now.\u201d He views this as \u201chysterical fear\u201d that is \u201cirrational\u201d and compares it to earlier introduction of new technologies that were similarly feared but were overblown and subsequently managed. He believes that AI is simply a computer program that is owned, controlled and used by people, and that AI does not have its own goals or its own wants. Moreover, he emphasizes, AI can be a force for tremendous good. Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes"}
{"input":" How does the United States' approach to AI regulation compare to the European Union's?","output":" The United States is not likely to pass a broad national AI law over the next few years.","context":"The most likely outcome for the United States is a bottom-up patchwork quilt of executive branch actions. Unlike Europe, the United States is not likely to pass a broad national AI law over the next few years. Successful legislation is likely focused on less controversial and targeted measures like funding AI research and AI child safety. This likely disappoints proponents of strong national AI regulation. This outcome will be messier and have gaps, but no broad national law does not mean no regulation. We\u2019re likely to see domain-specific agency actions especially in health care, financial services, housing, workforce, and child safety, plus multiple executive orders. This patchwork quilt of rules, if implemented well, could be grounded in the expertise of specific agencies and more tailored to innovation. The U.S. government will likely boost spending on AI and AI research, especially in defense and intelligence, and use its buying power to shape the market. AI trade friction with Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA)."}
{"input":" Who is likely to face multiple AI regulatory regimes around the world?","output":" Leading companies like Amazon, Apple, Google, Meta, Microsoft, and Nvidia are likely to face multiple AI regulatory regimes around the world.","context":"Governments should look beyond traditional regulatory techniques to influence AI developments Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes These positions are rooted in an aspect of reality, but they also create a blind spot: the regulations themselves. The specific requirements and restrictions they impose on China\u2019s AI products matter. They will reshape how the technology is built and deployed in the country, and their effects will not stop at its borders. \nThey will ripple out internationally as the default settings for Chinese technology exports. They will influence everything from the content controls on language models in Indonesia to the safety features of autonomous vehicles in Europe. China is the largest producer of AI research in the world, and its regulations will drive new research as companies seek out techniques to meet regulatory demands. \nAs U.S.-and Chinese-engineered AI systems increasingly play off one another in financial markets and international airspace, understanding the regulatory constraints and fail-safe mechanisms that shape their behavior will be critical to global stability."}
{"input":" Why is international cooperation on AI governance considered necessary, according to the document?","output":" International cooperation on AI governance is deemed necessary to enhance the effectiveness of domestic AI governance efforts. This includes facilitating the exchange of AI governance experiences, addressing externalities and extraterritorial impacts of domestic AI governance that might stifle innovation, and broadening global access to computing power and data essential for building and training AI models.","context":"Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 The U.S., thankfully, is not alone in pushing ahead on domestic forms of AI governance. For instance, the EU AI Act is close to being finalized and countries such as Brazil, the U.K., Canada, and Japan are all developing their own approaches to AI governance. While domestic AI governance is where addressing risks from AI and developing AI\u2019s social and economic potential must begin, these domestic efforts are building blocks to international AI governance. The work of the Brookings\/CEPS Forum on Cooperation in AI (FCAI)\u2014a track 1.5 multistakeholder dialogue\u2014has over the last three-plus years been focused on identifying opportunities for international cooperation on AI, and this work is now even more pressing in light of the pace and scope of domestic AI governance mechanisms being developed globally. The EOAI helps here by tasking the Departments of State and Commerce to \u201cestablish robust international frameworks for harnessing AI benefits and managing its risks and ensuring safety.\u201d AI governance has been a focus of discussions in the G7, the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI). Going forward, even more intensive engagement is needed through these international forums, in FCAI, as well as progress in developing international AI standards. Expanding commitments in trade agreements and digital economy agreements is another opportunity to build the needed international cooperation on AI governance. On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI"}
{"input":" How do the White House Voluntary AI Commitments relate to international AI governance efforts?","output":" The White House Voluntary AI Commitments have become the foundation for the International Code of Conduct for Organizations Developing Advanced AI Systems.","context":"On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 The United States is likely to take a decentralized bottom-up approach that is messy and may have gaps and inconsistencies. This will likely disappoint proponents of strong AI regulation. Yet, over time this may produce a patchwork quilt of AI rules that if implemented well is grounded in the expertise of specific agencies and more tailored to innovation. The tremendous risks and opportunities of AI have made it a presidential-level issue. The White House is coordinating executive agencies, as each moves ahead with actions in its own domain. The lack of meaningful regulation of social media is widely seen as a failure in Washington, and bolsters efforts to be more proactive with AI. Yet the decentralized structure of the U.S. government, political differences, and the complexities of AI forestall big, quick actions. There is also a real but less-likely probability that a big AI-related failure sparks a strong national government action. Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC."}
{"input":" Who is leading the U.S. delegation to the U.K. AI Safety Summit and what opportunity does this position provide?","output":" Vice President Kamala Harris.","context":"For full guest lists and more details on the forums, check out the Tech Policy Press US Senate \u2018AI Insight Forum\u2019 Tracker.\n\nUK AI Safety Summit 2023\n\nThe two-day, first-of-its-kind AI Safety Summit will officially kick off on Wednesday, Nov. 1. Hosted by the UK government, Prime Minister Rishi Sunak will bring global leaders, government officials, tech executives, and experts to Bletchley Park to discuss \u201chighly capable\u201d frontier AI systems and the future risks they pose. The Financial Times published a full list of attendees, which includes tech leaders from Anthropic, OpenAI, and Google, government officials from Ireland and Kenya, and researchers from the Oxford Internet Institute and the Stanford Cyber Policy Institute, among a long list of others. Some of the most notable guests include: The EOAI along with the other domestic AI policies outlined will have significant international impacts through various channels.\n\nFirst and most immediately, these suites of domestic policy developments on AI governance give Vice President Kamala Harris as head of the U.S. delegation to the U.K. AI Safety Summit a real opportunity to lead the further development of international AI governance. The U.S. position on AI stands in stark contrast to the lack of strong U.S. leadership on privacy regulation, where the absence of federal privacy legislation created a vacuum that the EU\u2019s General Data Protection Regulation (GDPR) filled, allowing GDPR to become a leading model for privacy regulation worldwide. On day one of the Summit, Vice President Harris announced new US actions meant to build on Monday\u2019s Executive Order, with the stated goal of strengthening international rules and norms with different allies and partners. This includes launching the US AI Safety Institute (AISI) inside the National Institute of Standards and Technology (NIST), and via the Department of Commerce, that will operationalize its AI Risk Management Framework. The Office of Management and Budget (OMB) has developed first-ever draft policy guidance for public review, according to the Vice President, that will build on the Blueprint for an AI Bill of Rights and NIST\u2019s Framework. Additionally, the US is joining 31 other nations, including countries like France, Germany, and the UK, in endorsing the Political Declaration on Responsible Military Use of Artificial Intelligence and Autonomy that responsibly and lawfully harnesses AI capabilities, like autonomous functioning systems. And finally, the White House The UK Department for Science, Innovation and Technology\u2019s \u2018Frontier AI Taskforce\u2019 issues second report\nOnly eighteen months into its formation, and just ahead of the UK-organized AI Safety Summit, the \u2018Frontier AI Taskforce\u2019 issued its second progress report. Since its first report in September, the group has tripled its research capacity, cemented new partnerships with leading AI organizations, and supported the development of Isambard-AI, an AI supercomputer where more intensive safety research will be conducted, among other developments."}
{"input":" How does Japan's approach to AI regulation differ from the EU's?","output":" Japan has developed and revised AI-related regulations with the goal of maximizing AI\u2019s positive impact on society, rather than suppressing it out of overestimated risks. The emphasis is on a risk-based, agile, and multistakeholder process, rather than a one-size-fits-all obligation or prohibition. The European Commission published the draft Artificial Intelligence Act, which classifies AI according to four levels and prescribes corresponding obligations, including enhanced security, transparency, and accountability measures.","context":"International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules. Chinese policy actors have even described the first-mover nature of their regulations as an added difficulty. When China began work on these regulations, the debates on the EU\u2019s AI Act were well underway in Europe, and Chinese policy analysts hoped that they could follow those debates and learn from the act. But slow progress on the AI Act meant that they had to forge ahead without the benefit of international guideposts or comparisons. For the United States, one benefit of its comparatively slow progress on AI governance is the op-portunity to learn from regulatory experiments abroad\u2014if policymakers are willing to take foreign regulations seriously. \u201c\u2026Prevent the risk of China achieving global AI dominance\u2026\u201d and ensure \u201cWe win, they lose.\u201d   \n\nIn between the EU\u2019s more interventionist approach and venture capitalists\u2019 more hands-off approach, many businesses and civil society groups are advocating for varying degrees of regulation. For example, Microsoft has proposed a comprehensive approach in its, \u201cGoverning AI: A Blueprint for the Future\u201d. It would create a new government AI agency, a new AI-legal framework, require safety brakes for AI systems that control critical infrastructure, and would license AI datacenters that run critical AI.  \n\nAI rules create winners and losers, as some are constrained, and others have more freedom and resources. Governments\u2014leery of being criticized for heavy handed regulation\u2014are quick to speak of innovation but are also responding to public concerns about AI.  \n\nII. Top Ten Parameters Driving Regulatory Design and Outcomes Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach."}
{"input":" What are the three basic philosophies set forth by Japan's Social Principles of Human-Centric AI?","output":" human dignity, diversity and inclusion, and sustainability.","context":"The aforementioned principles bear some similarity to those supported in the Global North (Floridi and Cowls 2019), yet institutional and cultural differences mean that the outcome is likely to be significantly different. China\u2019s AI ethics needs to be understood in terms of the country\u2019s culture, ideology, and public opinion (Triolo and Webster 2017). Although a full comparative analysis is beyond the scope of this article, it might be anticipated, for example, that the principles which emerge from China place a greater emphasis on social responsibility and group and community relations, with relatively less focus on individualistic rights, thus echoing earlier discussions about Confucian ethics on social media (Wong 2013). 6\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nMost advanced digital governments have issued governance frameworks,\nincluding ethical principles for the use of AI. Governance models incorporate three aspects: ethical principles, the role of a central agency, and\noperational framework.\nNational governments, including Australia, Canada, China, Japan, Singapore,\nUnited Arab Emirates, and the United States as well as international organizations including the European Commission, the Institute of Electrical and Electronics Engineers, International Organization for Standardization, United Nations, and World Economic Forum, are actively proposing governance models\nfor AI that emphasize common principles:\n\nEthical Principles\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\nPrivacy and data protection\nAccountability\nSafety and security\nTransparency and explainability\nFairness\nHuman control of technology\nProfessional responsibility\nPromotion of human values\n\n\u2022 Alongside establishing material goals, the AIDP outlines a specific desire for China to become a world leader in defining ethical norms and standards for AI. Following the release of the AIDP, the government, public bodies, and industry within China were relatively slow to develop AI ethics frameworks (Lee 2018; Hickert and Ding 2018). However, there has been a recent surge in attempts to define ethical principles. In March 2019, China\u2019s Ministry of Science and Technology established the National New Generation Artificial Intelligence Governance Expert Committee. In June 2019, this body released eight principles for the governance of AI. The principles emphasised that, above all else, AI development should begin from enhancing the common well-being of humanity. Respect for human rights, privacy and fairness were also underscored within the principles. Finally, they highlighted the importance of transparency, responsibility, collaboration, and agility to deal with new and emerging China is a central actor in the international debate on the development and governance of AI. It is important to understand China\u2019s internal needs, ambitions in the international arena, and ethical concerns, all of which are shaping the development of China\u2019s AI policies. It is also important to understand all this not just externally, from a Western perspective, but also internally, from a Chinese perspective. However, some ethical safeguards, constraints and desiderata are universal and are universally accepted and cherished, such as the nature and scope of human rights.Footnote15 They enable one to evaluate, after having understood, China\u2019s approach to the development of AI. This is why in this article we have sought to contribute to a more comprehensive and nuanced analysis of the structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are"}
{"input":" Who is responsible for publishing the AI Governance in Japan Ver. 1.1 report, and what does it conclude about the necessity of legally-binding horizontal requirements for AI systems in Japan?","output":" The AI Governance in Japan Ver. 1.1 report was published by Japan's Ministry of Economy, Trade, and Industry (METI), and it concludes that legally-binding horizontal requirements for AI systems are deemed unnecessary at the moment in Japan.","context":"The Ministry of Science and Technology is another key player, one that may see its profile rise due to recent government restructuring and increased focus on regulating underlying AI research. \nFeeding into this bureaucratic rulemaking are several think tanks and scholars, notably the China Academy for Information Communications Technology and Tsinghua University\u2019s Institute for AI International Governance.\nIn the years ahead, China will continue rolling out targeted AI regulations and laying the groundwork for a capstone national AI law. Any country, company, or institution that hopes to compete against, cooperate with, or simply understand China\u2019s AI ecosystem must examine these moves closely. \nThe subsequent papers in this series will dig into the details of these regulations and how they came about, deepening understanding of Chinese AI governance to date and giving a preview of what is likely coming around the bend. 2 AI legislation in China In this series of three papers, I will attempt to reverse engineer Chinese AI governance. \nI break down the regulations into their component parts\u2014the terminology, key concepts, and specific requirements\u2014and then trace those components to their roots, revealing how 3.The \u201cPolicy Funnel\u201d of China\u2019s AI Governance Major governance initiatives tend to proceed from left to right through this funnel, though often not in a linear fashion. This subset excludes several laws and regulations that impact AI development, such as the 2021 Personal Information Protection Law. It also excludes local government regulations, such as those covering autonomous vehicles, and national policy documents that focus on stimulating the AI industry rather than regulating it. The study includes some regulations that focus on algorithms rather than AI itself. It also briefly covers government documents that lay out high-level guidance for the ethics and governance of AI. Within that scope, table 1 outlines ten particularly significant AI governance documents."}
{"input":" What are the five core principles identified in the White House's Blueprint for an AI Bill of Rights?","output":" The five core principles identified in the White House's Blueprint for an AI Bill of Rights are intended to guide and govern the effective development and implementation of AI systems, with a focus on preventing civil and human rights abuses.","context":"Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed. Monday\u2019s White House action builds on its previous commitments to the safe and responsible development of AI. Some of these include its support for developing the Blueprint for an AI Bill of Rights, which safeguards Americans\u2019 rights and safety; securing voluntary commitments from Anthropic, Google, OpenAI, and other tech companies to thoroughly test systems before releasing them \u2013 also called \u201cred-teaming\u201d \u2013 and clearly labeling all AI-generated content; and granting $140 million to the National Science Foundation to establish seven new AI research institutes. The Order is also meant to complement the G7 leaders\u2019 Guiding Principles and Code of Conduct, which was also issued on Monday. (See below for more details.) To advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by From Principles to Practice\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public\u2019s rights, opportunities, or access to critical needs. The Executive Order can be broken down into eight parts, as outlined by CNBC:\nCreating new safety and security standards for AI\nProtecting consumer privacy\nProtecting consumers overall by evaluating potentially harmful AI-related health-care practices\nSupporting workers\nPromoting innovation and competition\nWorking with international partners to implement AI standards globally\nDeveloping guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination"}
{"input":" How does the inclusion of law enforcement in AI governance proposals vary between the White House's Blueprint and other recommendations?","output":" The White House's Blueprint excludes law enforcement from its provisions whereas other recommendations advocate for the explicit inclusion of law enforcement in AI governance to address ethical and legal concerns.","context":"(A)  identify areas where AI can enhance law enforcement efficiency and accuracy, consistent with protections for privacy, civil rights, and civil liberties; and\n\n               (B)  recommend best practices for law enforcement agencies, including safeguards and appropriate use limits for AI, to address the concerns set forth in section 13(e)(i) of Executive Order 14074 as well as the best practices and the guidelines set forth in section 13(e)(iii) of Executive Order 14074; and  \n\n          (iii)  supplement the report set forth in subsection 7.1(b)(i) of this section as appropriate with recommendations to the President, including with respect to requests for necessary legislation.  \n\n     (c)  To advance the presence of relevant technical experts and expertise (such as machine-learning engineers, software and infrastructure engineering, data privacy experts, data scientists, and user experience researchers) among law enforcement professionals: Developing guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination\nSome top takeaways from the Executive Order, according to the MIT Technology Review's Tate Ryan-Mosley and Melissa Heikkil\u00e4, include requiring the Department of Commerce to develop guidance for labeling AI-generated content, such as watermarking tools; compelling developers to share safety test results for new AI models that may pose a national security risk; and forcing AI companies to be more transparent about how their new models work, among other \u201csafety and security\u201d priorities. (A)  sentencing;\n\n               (B)  parole, supervised release, and probation;\n\n               (C)  bail, pretrial release, and pretrial detention;\n\n               (D)  risk assessments, including pretrial, earned time, and early release or transfer to home-confinement determinations;\n\n               (E)  police surveillance;\n\n               (F)  crime forecasting and predictive policing, including the ingestion of historical crime data into AI systems to predict high-density \u201chot spots\u201d;\n\n               (G)  prison-management tools; and\n\n               (H)  forensic analysis;  \n\n          (ii)   within the report set forth in subsection 7.1(b)(i) of this section:\n\n               (A)  identify areas where AI can enhance law enforcement efficiency and accuracy, consistent with protections for privacy, civil rights, and civil liberties; and Principles and International Agreements\nU.S. STATE AND LOCAL LAWS (ENACTED)\nNo meaningful federal legislation has passed, however states have been very active.\n\nEPIC has published \u201cState of State AI Policy\u201d to track the fast-moving world of AI legislation in states and localities around the country.\n\n2022 \u2013 2023 Legislative Session\n\n2021 \u2013 2022 Legislative Session\n\nFRAMEWORKS\nWhite House Blueprint for an AI Bill of Rights\nThe Office of Science and Technology Policy released a wide-ranging \u201cBlueprint for an AI Bill of Rights\u201d in Fall 2022, a document setting out how people should be able to expectThe five major principles are Safe and Effective Systems; Freedom from Algorithmic Discrimination; Data Privacy; Notice and Explanation; Human Alternatives, Consideration, and Fallback. The document lays out why these principles are critical, examples of where they are violated, and examples of how they\u2019ve been addressed."}
{"input":" Which two organizations are collaborating to release guidance on the use of AI in employment decisions to protect against discrimination towards people with disabilities?","output":" The Equal Employment Opportunity Commission (EEOC) and the Department of Justice are collaborating to release guidance on the use of AI in employment decisions to protect against discrimination towards people with disabilities.","context":"(F)  enable the analysis of whether algorithmic systems in use by benefit programs achieve equitable outcomes.\n\n     7.3.  Strengthening AI and Civil Rights in the Broader Economy.  (a)  Within 365 days of the date of this order, to prevent unlawful discrimination from AI used for hiring, the Secretary of Labor shall publish guidance for Federal contractors regarding nondiscrimination in hiring involving AI and other technology-based hiring systems.\n\n     (b)  To address discrimination and biases against protected groups in housing markets and consumer financial markets, the Director of the Federal Housing Finance Agency and the Director of the Consumer Financial Protection Bureau are encouraged to consider using their authorities, as they deem appropriate, to require their respective regulated entities, where possible, to use appropriate methodologies including AI tools to ensure compliance with Federal law and: that might be less discriminatory instead of simply detecting biased results. Further,regulators can encourage and highlight AI assurance companies that establishprocesses which enable some degree of independent scrutiny, such as with consistentevaluation standards, although this is challenging to do when AI assurance companiesdepend on AI developers for revenue.\nRegulators should also welcome information from affected individuals andwhistleblowers from AI developers\u2014both of whom may have unique information aboutalgorithmic systems.\nIndividuals who are subjected to algorithmic systems may have specific insight intothe function of those systems. Several U.S. agencies, such as the Equal EmploymentOpportunity Commission, explicitly Developing guidance for federal agencies\u2019 use and procurement of AI\nAdvancing equity and civil rights by creating guidance and research that avoids further algorithmic discrimination\nSome top takeaways from the Executive Order, according to the MIT Technology Review's Tate Ryan-Mosley and Melissa Heikkil\u00e4, include requiring the Department of Commerce to develop guidance for labeling AI-generated content, such as watermarking tools; compelling developers to share safety test results for new AI models that may pose a national security risk; and forcing AI companies to be more transparent about how their new models work, among other \u201csafety and security\u201d priorities. (d)  Artificial Intelligence policies must be consistent with my Administration\u2019s dedication to advancing equity and civil rights.  My Administration cannot \u2014 and will not \u2014 tolerate the use of AI to disadvantage those who are already too often denied equal opportunity and justice.  From hiring to housing to healthcare, we have seen what happens when AI use deepens discrimination and bias, rather than improving quality of life.  Artificial Intelligence systems deployed irresponsibly have reproduced and intensified existing inequities, caused new types of harmful discrimination, and exacerbated online and physical harms.  My Administration will build on the important steps that have already been taken \u2014 such as issuing the Blueprint for an AI Bill of Rights, the AI Risk Management Framework, and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) \u2014 in seeking to ensure that AI complies"}
{"input":"What financial information is not provided by The Bank?","output":" The Bank does not provide access to the following financial information: (a)Estimates of future borrowings by IBRD, contributions by individual donors to IDA, financial forecasts and credit assessments, and data on investment, hedging, borrowing, and cash management transactions15 generated by or for the Bank\u2019s treasury operations for the World Bank Group entities and other parties. (b)Documents, analysis, correspondence, or other information used or produced to execute financial and budgetary transactions, or to support the preparation of internal and external financial reports. (c)Details of individual transactions under loans and trust funds, information regarding amounts overdue from borrowers, or actions taken before any loans are placed in nonaccrual status. (d)Banking or billing information of World Bank Group entities, member countries, clients, donors, recipients, or vendors, including consultants.","context":"17.\nFinancial Information. The Bank does not provide access to the following\nfinancial information:14\n\n12\n\n13\n\n14\n\n15\n\n16\n\n(a)\n\nEstimates of future borrowings by IBRD, contributions by individual\ndonors to IDA, financial forecasts and credit assessments, and data on\ninvestment, hedging, borrowing, and cash management transactions15\ngenerated by or for the Bank\u2019s treasury operations for the World Bank\nGroup entities and other parties.\n\n(b)\n\nDocuments, analysis, correspondence, or other information used or\nproduced to execute financial and budgetary transactions, or to support the\npreparation of internal and external financial reports.\n\n(c)\n\nDetails of individual transactions under loans and trust funds, information\nregarding amounts overdue from borrowers, or actions taken before any\nloans are placed in nonaccrual status.16 However, if no subsequent Board meeting is expected, the Bank makes publicly available Reports to\nthe Board from its Committees, with deliberative information removed (summary portion of the Green\nSheet).\nThis is a heterogeneous group of documents that includes information notes, technical briefing papers,\nPowerPoint presentations that supplement Board papers, presentations to Board Committees, and\nadministrative papers (such as meeting notices) that are not used as a basis for consultation or decision\nbut are used solely for information or administrative purposes.\nCertain financial information is eligible for declassification and disclosure over time, as indicated in\nPart IV, Section C, of this policy statement.\nThis includes holdings, positions, and performance information for World Bank Group entities and\nother parties.\nFor example, the Bank does not disclose the monetary value of payments to, and names of, consultants. other parties.\nFor example, the Bank does not disclose the monetary value of payments to, and names of, consultants.\nHowever, the Bank routinely discloses monthly statements of loans and credits, and individual loan,\ncredit, and trust fund documents such as financing, loan, development credit, development grant, and\nproject agreements; guarantee agreements; administration agreements; and grant or trust fund 2\n\n\fA.\n\nThe Exceptions\n\n7.\nThe Bank does not provide access to information whose disclosure could cause\nharm to specific parties or interests. Accordingly, the Bank does not provide access to\ndocuments that contain or refer to the information listed in paragraphs 8-17.2\n8.\nPersonal Information. The Bank\u2019s Principles of Staff Employment require the\nBank to establish and maintain appropriate safeguards to respect the personal privacy of\nstaff members and protect the confidentiality of personal information about them.\nAccordingly, the Bank does not provide access to the following information, except to\nthe extent expressly permitted by the Staff Rules.3\n(a)\n\nPersonal information, including personal staff records, medical\ninformation, and personal communications (including e-mail) of the\nfollowing individuals and their families: Executive Directors, their\nAlternates, and their Senior Advisers; the President of the Bank; other\nBank officials; and Bank staff.\n\n(b)"}
{"input":"What are the Guiding Principles of the World Bank Policy on Access to Information?","output":" 1. Maximizing access to information. 2. Setting out a clear list of exceptions. 3. Safeguarding the deliberative process. 4. Providing clear procedures for making information available. 5. Recognizing requesters\u2019 right to an appeals process.","context":"Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction Guiding Principles. This Policy is based on five principles:\n\uf0b7\n\uf0b7\n\uf0b7\n\uf0b7\n\uf0b7\n\nMaximizing access to information.\nSetting out a clear list of exceptions.\nSafeguarding the deliberative process.\nProviding clear procedures for making information available.\nRecognizing requesters\u2019 right to an appeals process.\n\nThis policy statement is divided into four parts. Following this introduction, Part II sets\nout the core policy, Part III sets out how the Bank handles information relating to\nmember countries and other parties, and Part IV sets out the implementation aspects of\nthe Policy.\n\nII.\n\nThe Policy World Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\nI. Introduction\n1. The World Bank1 recognizes that transparency and accountability are of\nfundamental importance to the development process and to achieving its mission to end\nextreme poverty and promote shared prosperity. Transparency is essential to building\nand maintaining public dialogue and increasing public awareness about the Bank\u2019s\ndevelopment role and mission. It is also critical for enhancing good governance,\naccountability, and development effectiveness. Openness promotes engagement with\nstakeholders, which, in turn, improves the design and implementation of projects and framework.\n5. Guiding Principles. This Policy is based on five principles:\n\uf0b7 Maximizing access to information.\n\uf0b7 Setting out a clear list of exceptions.\n\uf0b7 Safeguarding the deliberative process.\n\uf0b7 Providing clear procedures for making information available.\n\uf0b7 Recognizing requesters\u2019 right to an appeals process.\nThis policy statement is divided into four parts. Following this introduction, Part II sets\nout the core policy, Part III sets out how the Bank handles information relating to\nmember countries and other parties, and Part IV sets out the implementation aspects of\nthe Policy.\nII. The Policy\n6. The Bank allows access to any information in its possession that is not on a list of\nexceptions (set out in Part II, Section A, of this policy statement). In addition, over time\nthe Bank declassifies and makes publicly available certain information that falls under the\nexceptions (as set out in Part IV, Section C, of this policy statement). Notwithstanding"}
{"input":" What can a requester who is denied access to information by the World Bank do to protect his or her right?","output":" He or she can file an appeal, but must meet one of two requirements: First, the requester is able to establish a prima facie case that the Bank has violated this Policy by improperly or unreasonably restricting access to information that it would normally disclose under the Policy. Second, the requester is able to make a public interest case to override the Policy exceptions that restrict the information requested (limited to those exceptions set out in some paragraphs).","context":"34\n35\n\nThe Appeals Board established under this Policy comprises three outside experts on access to\ninformation matters. Panel members are nominated by the President of the World Bank and endorsed\nby the Bank\u2019s Board of Executive Directors.\nSee paragraph 36 (a).\nSee paragraph 36 (b).\n\n16\n\n\finitial decision to deny access to the information requested, the notice specifies the\nreasons. If the AI Committee or Appeals Board reverses the decision to deny access, the\nrequester is notified of the decision and of the process for making the information\navailable to the requester.\n\n17 Public Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\nPublic Disclosure Authorized\n\n79034\n\nThe World Bank Policy on Access to Information\n\nJuly 1, 2013\n\n\fWorld Bank Policy on Access to Information\nThis statement sets out the policy of the World Bank on public access to information in its\npossession. This policy statement, which reflects revisions approved by the World Bank\u2019s\nBoard on April 3, 2013, supersedes the World Bank Policy on Access to Information,\ndated July 1, 2010. The Policy, as revised, takes effect on July 1, 2013.\n\nI.\n\nIntroduction 31.\nThe Bank declassifies and discloses\u2014routinely on the Bank\u2019s external website,\nand in response to requests\u2014certain types of restricted information (including\ninformation prepared under earlier disclosure policies of the Bank) as their sensitivity\ndiminishes over time. Some restricted information is not eligible for declassification.\n32.\nInformation Not Eligible for Declassification. The information that is restricted\nfrom disclosure under the exceptions set out in paragraphs 8-15 and 17 (d) of this policy\nstatement (i.e., Personal Information; Communications of Executive Directors\u2019 Offices;\nEthics Committee; Attorney-Client Privilege; Security and Safety; Information Restricted\nUnder Separate Disclosure Regimes and Other Investigative Information; Information\nProvided by Member Countries or Third Parties in Confidence; Corporate Administrative\nMatters; and banking or billing information of World Bank Group entities, member Bank to locate the information within a reasonable period of time. If a particular\ndocument is required, it should be identified precisely, preferably by date and title.\nElectronic submissions should be directed through the World Bank website:\nwww.worldbank.org\/wbaccess. Requests may also be made in person at the InfoShop in\nWashington, DC, or the Public Information Centers located in member countries.\n25. Timelines for Responding to Requests. The Bank acknowledges receipt of\nwritten requests for information within 5 working days, and endeavors to provide a more\ncomprehensive response within 20 working days. Additional time may be needed in\nspecial circumstances, including, for example, those involving complex or voluminous\nrequests, or requests requiring review by or consultations with internal Bank units,\nexternal parties, the Access to Information Committee, or the Board.\n26. Unreasonable or Unsupported Requests. The Bank reserves the right to refuse"}
{"input":" What is the main difference between the Bank's Access to Information Policy (AI Policy) and the previous Disclosure policy?","output":" There are four main differences. First, under the AI Policy, significantly more information on Bank operations and Board proceedings is available. Second, the new Policy permits the public release of some documents prior to discussion by the World Bank's Board of Executive Directors. Third, certain restricted information is eligible for declassification after 5, 10, or 20 years. Last but not least, it also establishes an appeal mechanism that provides public recourse when the Bank denies access to information.","context":"AI POLICY HIGHLIGHTS\n\nWhat is the main difference between the Bank's Access to Information Policy (AI Policy) and the previous\nDisclosure policy?\n\nThe AI Policy, launched in 2010, moved the Bank from an approach that spelled out what information is disclosed to\none that allows the Bank to disclose any information in its possession that is not on a list of exceptions set out in the AI\nPolicy.\n\n    \uf0b7    Under the AI Policy, significantly more information on Bank operations and Board proceeding is available.\n    \uf0b7    The new Policy permits public release of some documents prior to discussion by the World Bank's Board of\n         Executive Directors.\n    \uf0b7    Certain restricted information is eligible for declassification after 5, 10, or 20 years.\n    \uf0b7    It also establishes an appeal mechanism that provides public recourse when the Bank denies access to\n         information. AI FAQs\n\nAI POLICY HIGHLIGHTS\n\n    \uf0b7    What is the main difference between the World Bank's Access to Information Policy (AI Policy) and the\n         previous Disclosure policy?\n    \uf0b7    Is the AI Policy retroactive?\n    \uf0b7    Was the AI Policy discussed with member countries?\n    \uf0b7    What can be disclosed under the AI Policy?\n    \uf0b7    What are the exceptions to disclosing information?\n    \uf0b7    Why does the AI Policy restrict some types of information from disclosure?\n\nPROACTIVE DISCLOSURE\n\n    \uf0b7    Where do I find World Bank information?\n    \uf0b7    When do project documents become available?\n    \uf0b7    When do Board papers and Board records become available?\n\nREQUESTING INFORMATION TO THE WORLD BANK\n\n    \uf0b7    Can I contact a Bank staff directly for Bank\u2019s information?\n    \uf0b7    What does it cost to get information from the Bank?\n    \uf0b7    What if the Bank restricts something it is supposed to release when requested?\n    \uf0b7    What is the process to file an appeal?\n\n\n\nAI POLICY HIGHLIGHTS Go Top\n\nWhy does the AI Policy restrict some types of information from disclosure?\n\nThe AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the\npublic and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff and other\nparties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure\ncould cause harm to well defined interests.\n\n                                                                                                                    Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information? as \u201cOfficial Use Only,\u201d \u201cConfidential,\u201d or \u201cStrictly Confidential\u201d are not\npublicly available unless the information content of the e-mail becomes\neligible for declassification and disclosure over time.\n(c) The Bank does not provide access to e-mail that resides outside its records\nmanagement system (including e-mail that does not pertain to official matters\nand e-mail containing personal information or communications of Bank staff\nand other officials; see also paragraphs 8 (a) and 16 (a) and (b) of this policy\nstatement).\n15\nE. Access to Information Committee\n35. To facilitate the implementation of this Policy, the Bank has created the Access to\nInformation Committee (AI Committee), an administrative body that reports to Bank\nManagement. The AI Committee advises Management on the application of this Policy\nto complex issues, reviews proposals to disclose information that is on the list of\nexceptions,28 receives and rules on appeals under this policy statement,29 establishes"}
{"input":" Why does the AI Policy restrict some types of information from disclosure?","output":" The AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the public and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff, and other parties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure could cause harm to well-defined interests.","context":"Go Top\n\nWhy does the AI Policy restrict some types of information from disclosure?\n\nThe AI Policy represents a balance between the Bank's interest in providing the maximum amount of information to the\npublic and its obligations to protect the confidentiality of information pertaining to shareholders, clients, staff and other\nparties, and to protect the deliberative process. The information on the list of exceptions is restricted because disclosure\ncould cause harm to well defined interests.\n\n                                                                                                                    Go Top\n\nPROACTIVE DISCLOSURE\n\n\n\nWhere do I find World Bank information? AI POLICY HIGHLIGHTS\n\nWhat is the main difference between the Bank's Access to Information Policy (AI Policy) and the previous\nDisclosure policy?\n\nThe AI Policy, launched in 2010, moved the Bank from an approach that spelled out what information is disclosed to\none that allows the Bank to disclose any information in its possession that is not on a list of exceptions set out in the AI\nPolicy.\n\n    \uf0b7    Under the AI Policy, significantly more information on Bank operations and Board proceeding is available.\n    \uf0b7    The new Policy permits public release of some documents prior to discussion by the World Bank's Board of\n         Executive Directors.\n    \uf0b7    Certain restricted information is eligible for declassification after 5, 10, or 20 years.\n    \uf0b7    It also establishes an appeal mechanism that provides public recourse when the Bank denies access to\n         information. Go Top\n\fWhat can be disclosed under the AI Policy?\n\nThe Bank can disclose any information in its possession, provided that the information does not fall under one or more\nof the exceptions set out in the AI Policy or the Bank has decided, on an exceptional basis, to restrict access to the\ninformation.\n\n                                                                                                                    Go Top\n\nWhat are the exceptions to disclosing information?\n\nThe Policy explicitly sets out 10 categories of information that are considered to be exceptions and not eligible for\ndeclassification: an individual right to \u201cmeaningful information aboutthe logic\u201d of algorithmic systems. This has led companies, such as\n, to offer responses\u2014albeit limited ones\u2014to requests for information aboutalgorithmic decisions. Although not yet passed into law, the forthcoming EU AI Act willalso create\nsubstantial new transparency requirements\n(https:\/\/www.brookings.edu\/articles\/the-eu-and-us-diverge-on-ai-regulation-atransatlantic-\ncomparison-and-steps-to-alignment\/#anchor5)\n, likely including directdisclosure of chatbots and public reporting about high-risk AI systems.\nTransparency requirements require little expertise and capacity from governmentagencies, making them an appealing early step in AI regulation. However, regulators doneed to be careful in specifying transparency requirements\u2014vaguely or poorly wordedrequirements can permit too much flexibility in algorithmic transparency, allowing forcompanies to cherry pick self-serving statistics."}
{"input":" What should an appeal contain?","output":" The appeal must contain the following: 1. The original case number provided in the Bank\u2019s response to the request for information. 2. A description of the information originally requested. 3. A statement explaining the facts and the grounds that support the claim.","context":"interest reasons, an override of the Policy\u2019s exceptions is justified.\n40.\nNotification of Decision. Requesters are notified if the appeal is dismissed:\n(a) for failure to file within the required time; (b) for failure to provide sufficient\ninformation that would reasonably support the appeal; or (c) for appealing a matter that\nthe AI Committee or Appeals Board does not have authority to consider. After the\nAI Committee or the Appeals Board reaches its decision concerning the appeal, the\nrequester is notified of the decision. If the AI Committee or Appeals Board upholds the\n33 interest reasons, an override of the Policy\u2019s exceptions is justified.\n40. Notification of Decision. Requesters are notified if the appeal is dismissed:\n(a) for failure to file within the required time; (b) for failure to provide sufficient\ninformation that would reasonably support the appeal; or (c) for appealing a matter that\nthe AI Committee or Appeals Board does not have authority to consider. After the\nAI Committee or the Appeals Board reaches its decision concerning the appeal, the\nrequester is notified of the decision. If the AI Committee or Appeals Board upholds the\n33 The Appeals Board established under this Policy comprises three outside experts on access to\ninformation matters. Panel members are nominated by the President of the World Bank and endorsed\nby the Bank\u2019s Board of Executive Directors.\n34 See paragraph 36 (a).\n35 See paragraph 36 (b).\n17\ninitial decision to deny access to the information requested, the notice specifies the appeals within 45 working days of receiving an appeal (delays are communicated in\nwriting to the requester).\n38.\nAppealing the AI Committee\u2019s Denial\u2014Appeals Board. The Bank has\nestablished an independent Appeals Board33 to consider appeals alleging that the Bank\nviolated this Policy by restricting access to information that it would normally disclose\nunder the Policy,34 if the AI Committee upholds the initial decision to deny access; the\nAppeals Board does not consider appeals concerning requests to override the Policy\u2019s\nexceptions.35 The Appeals Board has the authority to uphold or reverse the relevant\ndecisions of the AI Committee, and the Appeals Board\u2019s decisions in such instances are\nfinal. Appeals to the Appeals Board must be filed, in writing, within 60 calendar days\nafter the AI Committee\u2019s decision to uphold the Bank\u2019s initial decision to deny access.\nThe Appeals Board makes its best efforts to consider all appeals that are received within deny access.\n16\nappeals within 45 working days of receiving an appeal (delays are communicated in\nwriting to the requester).\n38. Appealing the AI Committee\u2019s Denial\u2014Appeals Board. The Bank has\nestablished an independent Appeals Board33 to consider appeals alleging that the Bank\nviolated this Policy by restricting access to information that it would normally disclose\nunder the Policy,34 if the AI Committee upholds the initial decision to deny access; the\nAppeals Board does not consider appeals concerning requests to override the Policy\u2019s\nexceptions.35 The Appeals Board has the authority to uphold or reverse the relevant\ndecisions of the AI Committee, and the Appeals Board\u2019s decisions in such instances are\nfinal. Appeals to the Appeals Board must be filed, in writing, within 60 calendar days\nafter the AI Committee\u2019s decision to uphold the Bank\u2019s initial decision to deny access.\nThe Appeals Board makes its best efforts to consider all appeals that are received within"}
{"input":" The important issues in the field of global data access, sharing, and governance include which global topics?","output":" Access to and sharing of data are critical to enable AI\u2019s benefits across sectors. Policies must encourage data access and sharing while addressing associated risks, for countries to harness the full potential. In many countries, policymakers and regulators face difficulties finding common definitions and common ground in discussions, cooperation, and coordination on data governance, at national and international levels. They focus on aspects relevant to their policy domains and jurisdiction. Therefore, data should be governed to maximize its benefits while addressing risks and challenges, including protecting the rights of individuals and organizations. This requires comprehensive policy to address cross-cutting challenges while accounting for the specificities of data governance in domains like trade or competition (OECD, forthcoming[19]). These include: First, Balancing the trade-offs between data openness and control. The more openly data is accessed, shared, and re-used (for example, with open data), the higher its potential social and economic benefits, but also the greater the associated risks. Second, Addressing potentially conflicting interests and regulations. Data collected and used to inform AI systems are often (co-)created by the interaction of many stakeholders in the global data ecosystem, in some cases without them being aware. Facilitating data access and sharing for AI requires disentangling and reconciling these interests and data-governance frameworks. Third, Aligning incentives for investment in data and its reuse. While the marginal costs of transmitting, copying, and processing data can be close to zero, substantial investment is often required to generate and collect data and enable data sharing and reuse for AI. Fair distribution of the benefits from data can help address incentive challenges.","context":"of open public data. In Norway, the Br\u00f8nn\u00f8ysund Register Centre and the\nNorwegian Digitalisation Agency have established a national directory of data\nsupplied by different public agencies. The directory provides an overview of\nthe data each agency has and how it is shared across government, as well as\ndatasets that are made publicly available. Portugal also plans to create a\ncentralised repository for administrative data.\nOrganisations focused on data have also been created or are being considered.\nThe Spanish AI strategy, for example, recommends the creation of a National\nData Institute. In parallel, countries and regional institutions seek to incentivise\ndata sharing in the private sector. The United Kingdom, in collaboration with\nthe Open Data Institute and Innovate UK, has launched three pilot projects to\nexplore data trust frameworks for safe, secure and equitable data transfers.\nEuropean countries are co-operating to create a European data space (GAIA-X), on sectoral applications in countries at the lower\nend of the income scale. In terms of policy and\nregulation, future research is needed to address a)\nthe mix of policy instruments such as legislation,\ntechnical interventions and standards, and industry\nguidelines; b) the additional range of potential risks\nto be addressed including data security, privacy,\ntransparency, safety and data bias; c) scenarios\nto predict, preempt or mitigate unforeseen\nconsequences and negative externalities, including\nrising unemployment, social and economic\ninequality; and finally d) legal and judicial systems\nto address new legal questions such as liability for\nautonomous systems and privacy concerns related\nto the processing or storing of personal data.\n\nworldbank.org\/digitaldevelopment\n32\n\n\fANALYTICAL INSIGHTS - NOTE 4 The Role of International and\nRegional Organizations\n\nInternational organizations also work on\nharmonizing data protection regulation across\nregions and promote access to data and markets.\n\n3 http:\/\/participa.br\/profile\/estrategia-brasileira-de-inteligencia-artificial\n4 United Nations, Secretary-General\u2019s High-level Panel on Digital\n\nCooperation, https:\/\/www.un.org\/en\/digital-cooperation-panel\/. Its final\nreport (UN Secretary-General\u2019s High-level Panel 2019) makes five sets\nof recommendations: i) Build an inclusive digital economy and society,\nii) Develop human and institutional capacity, iii) Protect human rights\nand human agency, iv) Promote digital trust, security and stability, and v)\nFoster global digital cooperation. The final report was submitted in June\n2019 for a broad consultation process on the topics covered in the report.\n\nworldbank.org\/digitaldevelopment\n8\n\n\fANALYTICAL INSIGHTS - NOTE 4 Institutional Arrangement for Partnership\n\nThe performance of AI systems is greatly determined by data. To allow people worldwide to reap the benefits of AI systems, the free flow of high-quality data is essential. However, worldwide data policies significantly diverge, covering domains such as privacy, government access, security, harmful content, and IP rights. Initially brought forward by former prime minister Shinzo Abe at Davos in 2019, the DFFT initiative seeks to enhance the cross-border flow of data that is beneficial for business and social problem-solving. This goal is pursued while simultaneously ensuring trust in privacy, security, IP rights, and other associated areas. Commitments to the DFFT have been reiterated at the Japan 2019 G20, the UK 2021 G7, and the Germany 2022 G7. However, no specific collaborative framework has been established so far to achieve this goal."}
{"input":" What are the investment projects in AI research and development in various countries?","output":" Countries are funding national AI-related research institutes and projects through grants; consolidating AI research networks and collaborative platforms; prioritizing AI investments in specific sectors; pursuing AI-related mission-oriented innovation policies; and procuring AI systems for the public sector. Budgets for AI R&D vary across countries. Since 2020, the United States has dedicated USD 1 billion or more annually to non-defense AI R&D and created national AI research institutes. The EU Horizon 2020 program committed EUR 1.5 billion to AI research over two years and expected an additional EUR 20 billion in 2020 from the private sector and member states, with the Horizon Europe program continuing these efforts.","context":"AI policy implementation\nInvesting in AI R&D\nEnhancing national AI research and development (R&D) capabilities is a key\ncomponent of many national AI strategies and policies. AI is a general-purpose\ntechnology with implications across industries. It is also called an \u201cinvention of\na method of invention\u201d (Cockburn, 2018[2]) and is already widely used by\nscientists and inventors to facilitate innovation. Entirely new industries could\nbe created based on the scientific breakthroughs enabled by AI. This\nunderscores the key role of governments in providing sustained investment in\nAI basic research with long term horizons, particularly in areas under-served by\nmarket-driven investments. In addition, research institutions in all areas require\ncapable AI systems to remain competitive, particularly in biomedical science\nand life science fields.\nThe allocation of public budgets to AI R&D varies in scale across countries. 6.\nStrategic investments and funding:\nAvenues for funding can include public\nprocurement, research grants, awards or \u201cgrand\n\nworldbank.org\/digitaldevelopment\n14\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nchallenges.\u201d The public sector can support the\ncreation of new research centers or programs in\nbasic or applied AI research or provide direct\ninvestment for start-ups to compensate for\nshortages in venture capital funding. Funding can\ntarget specific research needs based on priorities,\n\nincluding projects using AI applications for\nsocietal good such as biomedicine, education\nor climate research. This category also includes\nscholarship funding for Master\u2019s degree or PhD\nstudents in machine learning.\n\nFINDINGS FROM A REVIEW OF NATIONAL AI STRATEGIES\nThis section presents emerging practices in AI\npolicymaking across a broad geographic range of\neleven countries. It begins with a more detailed re- |9\nmentioned. The OECD has begun to investigate how to assess government\nspending on AI-related R&D through proxy approaches, however, no\ncomprehensive method exists yet to track and compare AI R&D funding across\ncountries and agencies (OECD, 2021[3]).\nGovernments allocate funding for AI to: 1) support the establishment of\nnational AI research institutes; 2) consolidate AI research networks and\ncollaborative platforms; 3) prioritise AI investments in targeted sectors; 4)\npursue AI mission-oriented innovation policies; and 5) procure AI systems for\nthe public sector.\nNational AI strategies and policies often outline how countries plan to invest in\nAI to build or leverage their comparative advantages. They also encourage\nbusinesses to develop solutions that will boost growth and well-being.\nCountries tend to prioritise a handful of economic sectors, including mobility\n(such as logistics and transportation), energy, health, and agriculture (Figure 2). The primary objective of this working paper is to\ndevelop a better understanding of governments\u2019\napproaches to AI by presenting findings from a\n\n1 https:\/\/hai.stanford.edu\/sites\/default\/files\/ai_index_2019_report.pdf\n2 https:\/\/www.oxfordinsights.com\/government-ai-readiness-index-2020\n\nworldbank.org\/digitaldevelopment\n\n2\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nreview of national AI strategies from around the\nworld. It also includes a more detailed analysis\nof the strategic approaches taken by two early\nadopters. This article is organized in four sections.\nThe first section discusses the increasing global\nAI divide in terms of availability of investments,\ntalent and research before further exploring\nthe balance of opportunities and risks of AI for\ndeveloping countries. This section also notes the\nroles of different stakeholders in supporting AI\ndevelopment within countries. The second section\npresents details of eight policy domains and six"}
{"input":" What aspects should be focused on in the hardware field for the development of AI?","output":" First, It includes using Machine learning models and techniques to learn in an automated manner through patterns and inferences rather than explicit instructions from a human. Second, Connectivity allows the transfer of large volumes of data in real or quasi-real time, while computing infrastructure (hardware and software) executes the mathematical operations needed to calibrate or train an AI system and infer its results. The combination of high-quality connectivity, data, computing infrastructure, and AI technologies continues to enable innovative and disruptive new services.","context":"software) that make the machine intelligent. Hence, AI               from AI run on a supercomputer in a research lab. This\ncomprises a very wide range of approaches, techniques                is also a reason why it is difficult to differentiate AI\nand technologies.                                                    impacts from those of the underlying, necessary\n                                                                     infrastructure and related technologies. In particular,\nTo understand AI\u2019s current and future impacts on                     DNN requires \u201cbig data\u201d applications, data centers and\nhumanity\u2019s global aspirations - such as the SDGs - the               cloud computing. Similarly, there is a continuum of\nfunctional    definition     above      which      specifies         process or system automation tasks \u2013 in factories run by\npsychological skills is useful. For example, this approach           robots to online digital automation of tasks - some of thing that \u201cwe know when we see it\u201d. For example, it is              list as a working basis to discuss SDG impacts. However,\nnotoriously difficult to measure human intelligence, yet             I am cognizant of the fact that such a \u201cblack-box\nit is pretty obvious to us when we meet a highly                     approach\u201d to AI which does not specify the underlying\nintelligent person.                                                  techniques and technologies is not sufficient to fully\nAs a working definition, I suggest following eminent                 understand all SDG impacts. The details of AI concepts,\ncognitive scientist Margaret Boden: \u201cArtificial                      techniques and technologies matter greatly, especially\nIntelligence (AI) seeks to make computers do the sorts of            with regard to sustainability and with regard to future\nthings that minds can do. Some of these (e.g., reasoning,            directions of AI and its physical, economic and socio- In that sense, AI is broader than what we typically                  supervised learning \u2013 which has become by far the most\nconsider human intelligence. It includes a wide range of             widely adopted AI technology in the 2010s \u2013 requires\ninformation-processing capacities. In fact, AI could                 enormous amounts of data and large amount of energy\ninclude aspects of intelligence that are far outside the             for data handling and computation. Hence, much talk\nreach of humans. And while AI requires physical                      about the \u201cdata economy\u201d these days. In terms of\nmachines, especially computers, it is really                         physical implementation, it is important to note that AI\ninformationally powerful virtual machines (e.g.,                     used on a mobile phone via a 5G network differs greatly\nsoftware) that make the machine intelligent. Hence, AI               from AI run on a supercomputer in a research lab. This We outline several scenarios that illustrate how AI development could unfold in the near term, depending on who is in control. We held discussions with leading technologists, policymakers, and scholars spanning many sectors to generate our findings and recommendations. We presented these experts with the scenarios as a baseline to probe, reflect on, and critique. We sought to characterize the current trajectory of AI development and identify the most important factors for governing the evolution of this unprecedented technology.\n\nWho could control the development of AI?"}
{"input":" Can you give me some examples of the ethical issues raised by artificial intelligence?","output":" Sure, here are some examples. How can we ensure that algorithms do not infringe fundamental human rights\u2014from privacy and data confidentiality to freedom of choice and freedom of conscience? Can freedom of action be guaranteed when our desires are anticipated and guided? How can we ensure that social and cultural stereotypes are not replicated in AI programming, notably when it comes to gender discrimination? Can these circuits be duplicated? Can values be programmed, and by whom? How can we ensure accountability when decisions and actions are fully automated? How do we make sure that no one\u2014wherever they are in the world\u2014is deprived of the benefits of these technologies? How do we ensure that AI is developed in a transparent way, so that global citizens, whose lives it affects, have a say in its development?","context":"104. World Health Organization (2021). Ethics and governance of artificial intelligence for health. \nWorld Health Organization.\n105. Zhang, J. et al. (2023). Ethics and governance of trustworthy medical artificial intelligence. \nBMC Med. Inform. Decis. Mak., Vol 23, 7.\n106. Farhud, D. D. et al. (2021). Ethical Issues of Artificial Intelligence in Medicine and \nHealthcare. Iran. J. Public Health, Vol 50, i\u2013v.\n107. Gerke, S. et al. (2020). Ethical and legal challenges of artificial intelligence-driven \nhealthcare. Artif. Intell. Healthc., 295\u2013336.\n108. Murphy, K. et al. (2021). Artificial intelligence for good health: a\nscoping review of the ethics literature. BMC Med. Ethics, Vol 22, 14.\n19                                  POSTnote 708 - 9 January 2024 4 The debate on digital ethics and AI in China emphasises the importance of AI being available, reliable, comprehensible, and controllable (Si 2019). Finally, the Chinese Association for Artificial Intelligence (CAII)Footnote10 has yet to establish ethical principles but did form an AI ethics committee in mid-2018 with this purpose in mind (\u201cAI association to draft ethics guidelines\u201d 2019). In the following sections, we shall focus on the debate about AI ethics as it is emerging in connection with privacy and medical ethics because these are two of the most mature areas where one may grasp a more general sense of the current \u2018Chinese approach\u2019 to digital ethics. The analysis of the two areas is not meant to provide an exhaustive map of all the debates about ethical concerns over AI in China. Instead, it may serve to highlight some of the contentious issues that are emerging, and inform a wider understanding of the type of boundaries which may be drawn in China when a normative agenda in the country is set."}
{"input":" What role does UNESCO play in promoting global dialogue on AI?","output":" First, UNESCO will be a full and active participant in this global conversation. The organization has many years of experience in the ethics of science and technology. Its advisory bodies have already produced numerous reports and declarations, including on robotics, such as the Report of the World Commission on the Ethics of Scientific Knowledge and Technology on Robotics Ethics in 2017. The advisory bodies also have experience in developing normative instruments, including the Universal Declaration on the Human Genome and Human Rights in 1997 and the Universal Declaration on Bioethics and Human Rights in 2005. Second, UNESCO ensures that Africa fully participates in transformations related to AI, not only as a beneficiary but also upstream, contributing directly to its development. In terms of gender equality, UNESCO fights against the biases in the societies to guarantee that they are not reproduced in AI applications. Finally, UNESCO empowers young people by providing them with the skills they need for life in the twenty-first century for integration in a changing labor market. Third, UNESCO also has a key role to play in bridging existing divides, which AI is likely to deepen. Eliminating fragmentation between countries and genders, but also in terms of resources and knowledge, could enable more people to contribute to the digital transformation underway. What's more, UNESCO, with its humanist mission and international dimension, involving researchers, philosophers, programmers, policymakers, and private sector and civil society representatives, is the natural home for debate on such ethical issues. Last but not least, UNESCO is performing its role to the fullest, informing the global debate on the major transformations of its time while establishing principles to ensure that technological advances are used to serve the common good. The promise of AI and its underlying ethical issues are fascinating, and UNESCO's responses to these challenges will transform the world as UNESCO knows it.","context":"software & knowledge; regulation, testbeds and documentation; skills and\nlabour markets; and international co-operation.\nRead more: https:\/\/oecd.ai.\nUNESCO\u2019s Ad Hoc Expert Group on AI (AHEG)\nResponsible entity: UNESCO\nDescription: UNESCO has organised events to exchange knowledge about AI,\nfocusing on the dimensions of ethics, policy and capacity building. In March\n2020, UNESCO appointed 24 leading experts to an Ad Hoc Expert Group (AHEG)\non the ethics of AI. UNESCO\u2019s November 2019 General Conference tasked the\nAHEG with elaborating a recommendation on the ethics of artificial intelligence.\nRead more: https:\/\/en.unesco.org\/artificial-intelligence. on Digital Cooperation.\nhttps:\/\/www.un.org\/en\/pdfs\/DigitalCooperation-report-for%20web.pdf\nUSAID.2018. Reflecting the Past, Shaping the Future: Making AI Work for International Development. Washington, DC.\nhttps:\/\/www.usaid.gov\/sites\/default\/files\/documents\/15396\/AI-ML-in-Development.pdf\nVillani, C. and others. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy.\nParis: Conseil national du num\u00e9rique.\nVinuesa, R., Azizpour, H., Leite, I. et al. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals.\nNature Communications, 11, 233.\nhttps:\/\/doi.org\/10.1038\/s41467-019-14108-y\nWhittaker, M. and others. 2018. AI Now Report 2018. New York: AI Now Institute, New York University.\nhttps:\/\/ainowinstitute.org\/AI_Now_2018_Report.pdf\nWhite House. 2020. American Artificial Intelligence Initiative: Year One Annual Report. Office of Science and Technology Policy. our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing other harms. 60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed"}
{"input":" What overall role does artificial intelligence play in promoting sustainable development?","output":" AI could open up tremendous opportunities for achieving the Sustainable Development Goals (SDGs) set by the United Nations in the 2030 Agenda for Sustainable Development. Its applications enable innovative solutions, improved risk assessment, better planning, and faster knowledge sharing.","context":"leading to new applications, new development models,\ncombined with other methods as need be. This will have\n                                                                                and also sustainability concerns. This has important\nimportant implications on our SDG aspirations until\n                                                                                implications for humanity\u2019s aspirations expressed in the\n2030, as will be detailed in this report.\n                                                                                SDGs.\nConclusion                                                                      Finally, it is impossible to adequately understand the\nArtificial Intelligence (AI) seeks to make computers do                         full implications of AI without exploring the specificities on Digital Cooperation.\nhttps:\/\/www.un.org\/en\/pdfs\/DigitalCooperation-report-for%20web.pdf\nUSAID.2018. Reflecting the Past, Shaping the Future: Making AI Work for International Development. Washington, DC.\nhttps:\/\/www.usaid.gov\/sites\/default\/files\/documents\/15396\/AI-ML-in-Development.pdf\nVillani, C. and others. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy.\nParis: Conseil national du num\u00e9rique.\nVinuesa, R., Azizpour, H., Leite, I. et al. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals.\nNature Communications, 11, 233.\nhttps:\/\/doi.org\/10.1038\/s41467-019-14108-y\nWhittaker, M. and others. 2018. AI Now Report 2018. New York: AI Now Institute, New York University.\nhttps:\/\/ainowinstitute.org\/AI_Now_2018_Report.pdf\nWhite House. 2020. American Artificial Intelligence Initiative: Year One Annual Report. Office of Science and Technology Policy. thing that \u201cwe know when we see it\u201d. For example, it is              list as a working basis to discuss SDG impacts. However,\nnotoriously difficult to measure human intelligence, yet             I am cognizant of the fact that such a \u201cblack-box\nit is pretty obvious to us when we meet a highly                     approach\u201d to AI which does not specify the underlying\nintelligent person.                                                  techniques and technologies is not sufficient to fully\nAs a working definition, I suggest following eminent                 understand all SDG impacts. The details of AI concepts,\ncognitive scientist Margaret Boden: \u201cArtificial                      techniques and technologies matter greatly, especially\nIntelligence (AI) seeks to make computers do the sorts of            with regard to sustainability and with regard to future\nthings that minds can do. Some of these (e.g., reasoning,            directions of AI and its physical, economic and socio- This approach also shapes environmental protection, where President Xi has been promoting \u2018digital environmental protection\u2019 (\u6570\u5b57\u73af\u4fdd) (Kostka and Zhang 2018). Within this, AI is being used to predict and mitigate air pollution levels (Knight 2015), and to improve waste management and sorting (\u201cAI-powered waste management underway in China\u201d 2019)."}
{"input":" How does the White House view the current moment in AI policy, according to President Biden's remarks at the Executive Order signing?","output":" President Biden characterized the current regulatory atmosphere around AI policy as a genuine inflection point in history, suggesting that the decisions made in the near term will significantly influence the direction of AI development for the coming decades.","context":"At the Executive Order\u2019s signing, President Biden characterized the current regulatory atmosphere around AI policy as \u201ca genuine inflection point in history, one of those moments where the decisions we make in the very near term are going to set the course for the next decades.\u201d AI brings change with it that has as much potential as it does risk, he said such as \u201cexploring the universe, fighting climate change, ending cancer as we know it, and so much more.\u201d\n\nThe Executive Order was met with warm response from Congress and some civil society groups. President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated. Toward international cooperation on AI governance\u2014the US executive order on AI\nJoshua P. Meltzer\nNovember 1, 2023 On October 30, the White House released a detailed and comprehensive executive order on AI (EOAI)\u2014the Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The EOAI mobilizes the federal government to develop guidelines and principles, and compile reports on AI use and its development. The EOAI, along with the AI Bill of Rights, the Voluntary AI Commitments, and work on AI standards sum to an increasingly coherent and comprehensive approach to AI governance. U.S. leadership on AI governance is critical, particularly given the role of the U.S. as a leading developer and investor in AI, including more recently foundational AI models such as ChatGPT4. However, international cooperation on AI governance is also needed to make domestic AI governance efforts more effective, including by facilitating the exchange of AI governance experiences that can inform approaches to domestic AI governance; addressing the externalities and extraterritorial impacts of domestic AI"}
{"input":" What are some of the critical components of AI risk management addressed by the White House's Executive Order, according to the Center for Democracy and Technology? ","output":" The Center for Democracy and Technology welcomed the Order, particularly praising its direction to multiple federal agencies to issue new guidance and adopt processes prioritizing civil rights and democratic values in AI governance. This includes addressing AI deployment in critical areas such as the workplace, housing, education, and government benefits programs.","context":"President Joseph Biden issues a sweeping AI Executive Order\nOn Monday, President Joseph Biden issued a highly-anticipated \u201cExecutive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\u201d The first-of-its-kind executive action is the White House\u2019s attempt to quickly establish oversight of the rapidly evolving technology while Congress works to develop a comprehensive regulatory framework. The document is devoted, in large part, to national security concerns, doling out various new responsibilities to the Secretaries of Defense and Homeland Security, the Director of National Intelligence, and other agencies. But nearly the entire alphabet soup of federal agencies is implicated. The Center for Democracy and Technology (CDT) similarly welcomed the Order. In a press release, CDT President and CEO Alexandra Reeve Givens gave credit to the White House for addressing \u201ccritical components of AI risk management\u201d in key areas where AI is already being deployed, from the workplace and housing to education and government benefits programs. \u201cWe are particularly encouraged that the Order directs multiple federal agencies to issue new guidance and adopt processes to prioritize civil rights and democratic values in AI governance,\u201d Reeve Givens added. (Tech Policy Press spoke with Reeve Givens last week about her experience as an attendee at the Senate\u2019s second closed-door AI \u2018Insight Forum.\u2019) Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\n\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:\n\n     Section 1.  Purpose.  Artificial intelligence (AI) holds extraordinary potential for both promise and peril.  Responsible AI use has the potential to help solve urgent challenges while making our world more prosperous, productive, innovative, and secure.  At the same time, irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security.  Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks.  This endeavor demands a society-wide effort that includes government, the private sector, academia, and civil society. (h)  The Federal Government should lead the way to global societal, economic, and technological progress, as the United States has in previous eras of disruptive innovation and change.  This leadership is not measured solely by the technological advancements our country makes.  Effective leadership also means pioneering those systems and safeguards needed to deploy technology responsibly \u2014 and building and promoting those safeguards with the rest of the world.  My Administration will engage with international allies and partners in developing a framework to manage AI\u2019s risks, unlock AI\u2019s potential for good, and promote common approaches to shared challenges.  The Federal Government will seek to promote responsible AI safety and security principles and actions with other nations, including our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing"}
{"input":" What significant steps did the UK AI Safety Summit plan to take regarding AI research?","output":" The UK AI Safety Summit planned to showcase initial program results, present demonstrations focused on areas of AI risk such as misuse, societal harm, loss of human control, and unpredictable progress, and transition the Frontier AI Taskforce to a more formal AI Safety Institute to develop the infrastructure needed to understand and govern advanced AI risks.","context":"determining pay9\n\u2022  aiding with local and national public sector decisions, such as social housing allocation,10 \nbenefit claims, and other issues10\u201315\n\u2022  self-driving cars*16,17\nIn the past few years, there have been significant advancements in AI capabilities. Single AI \nsystems can now perform a wide range of tasks (PB 57). Generative AI (see Table for definitions) \ncan generate realistic text, images, audio, and video.18\u201320\nThere have been significant public policy developments relating to AI in recent years.\nIn 2023, the Government published a white paper outlining a \u2018pro-innovation approach to AI\u2019,21 \nalong with announcements including \u00a3900m for an \u2018exascale\u2019 \u2020 supercomputer.22 The Prime Minister \nhosted a global \u2018AI Safety Summit\u2019 in November 2023.23 It resulted in a declaration on AI safety \nsigned by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety. For full guest lists and more details on the forums, check out the Tech Policy Press US Senate \u2018AI Insight Forum\u2019 Tracker.\n\nUK AI Safety Summit 2023\n\nThe two-day, first-of-its-kind AI Safety Summit will officially kick off on Wednesday, Nov. 1. Hosted by the UK government, Prime Minister Rishi Sunak will bring global leaders, government officials, tech executives, and experts to Bletchley Park to discuss \u201chighly capable\u201d frontier AI systems and the future risks they pose. The Financial Times published a full list of attendees, which includes tech leaders from Anthropic, OpenAI, and Google, government officials from Ireland and Kenya, and researchers from the Oxford Internet Institute and the Stanford Cyber Policy Institute, among a long list of others. Some of the most notable guests include: The UK Department for Science, Innovation and Technology\u2019s \u2018Frontier AI Taskforce\u2019 issues second report\nOnly eighteen months into its formation, and just ahead of the UK-organized AI Safety Summit, the \u2018Frontier AI Taskforce\u2019 issued its second progress report. Since its first report in September, the group has tripled its research capacity, cemented new partnerships with leading AI organizations, and supported the development of Isambard-AI, an AI supercomputer where more intensive safety research will be conducted, among other developments. Policy implications of artificial intelligence (AI)\nHuman-Centered Artificial Intelligence (HAI).\n19. Jones, E. Explainer: What is a foundation model? Ada Lovelace Institute.\n20. Google Google Generative AI.\nGoogle AI.\n21. Department for Science, Innovation and Technology (2023). AI regulation: a pro- innovation \napproach. GOV.UK.\n22. Department for Science, Innovation and Technology (2023). Bristol set to host UK\u2019s most \npowerful supercomputer to turbocharge AI innovation. GOV.UK.\n23. Foreign, Commonwealth & Development Office et al. (2023). AI Safety Summit 2023. GOV.UK.\n24. Department for Science, Innovation and Technology et al. (2023). The Bletchley Declaration by \nCountries Attending the AI Safety Summit, 1-2 November 2023.\n25. Donelan, M. (2023). Introducing the AI Safety Institute. GOV.UK.\n26. The Alan Turing Institute (online). Data science and AI glossary.\n27. Cabinet Office et al. (2021). Ethics, Transparency and Accountability Framework for Automated"}
{"input":" What is the title of the standard issued by the British Standards Institution (BSI) related to robots and robotic devices? ","output":" The standard issued by BSI related to robots and robotic devices is BS 8611:2016 Robots and robotic devices. Guide to the ethical design and application of robots and robotic systems.","context":"In line with this publication, the Standardization Administration of the People\u2019s Republic of China, the national-level body responsible for developing technical standards, released a white paper on AI standards. The paper contains a discussion of the safety and ethical issues related to technology (Ding and Triolo 2018). Three key principles for setting the ethical requirements of AI technologies are outlined. First, the principle of human interest states that the ultimate goal of AI is to benefit human welfare. Second, the principle of liability emphasises the need to establish accountability as a requirement for both the development and the deployment of AI systems and solutions. Subsumed within this principle is transparency, which supports the requirement of understanding what the operating principles of an AI system are. Third, the principle of consistency of [sic] rights and responsibilities emphasised that, on the one hand, data should be properly recorded and oversight Technical standards function as a baseline to gauge a product's features and performance. For example, ISO\/IEC JTC 1\/SC 42, an international standard for AI, includes \u201cassessment of machine learning classification performance\u201d and \u201cbig data reference architecture.\u201d These technical standards enable a common platform for risk assessments and audits, allowing countries with varying regulations to mutually assess and evaluate AI systems or services. It is worth pointing out, however, that technical standards are not a panacea. To create a truly interoperable framework for AI governance, collaboration that goes beyond just technical aspects and includes socio-technical and normative elements is needed as well. For instance, in the NIST AI Risk Management Framework's Initial Draft, aspects such as explainability and bias are labeled as socio-technical elements, and fairness, accountability, and transparency are referred to as \u201cguiding principles.\u201d These elements, distinct from purely signed by 28 countries,24 and the establishment of an AI Safety Institute25 that aims to build \npublic sector capacity to research AI safety.\nThe rapid advancement of AI capabilities means that policy implications are continuously shifting. \nSome stakeholders have called for further Government action.\n* The Kings Speech announced an Automated Vehicles Bill that seeks to introduce new legal \nframeworks to support safe commercial developments of self-driving vehicles.16\n\u2020 An exascale supercomputer is a computer that can perform 1018 (a quintillion) operations per \nsecond.\n2                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nTable: Definitions of AI and related concepts\nDefinitions are not universally agreed, are continuously evolving, and are linked.\nTerm                    Definition\nArtificial intelligence (AI)         The UK Government\u2019s 2023 policy paper on \u2018A pro-innovation \napproach to AI Policy implications of artificial intelligence (AI)\nHuman-Centered Artificial Intelligence (HAI).\n19. Jones, E. Explainer: What is a foundation model? Ada Lovelace Institute.\n20. Google Google Generative AI.\nGoogle AI.\n21. Department for Science, Innovation and Technology (2023). AI regulation: a pro- innovation \napproach. GOV.UK.\n22. Department for Science, Innovation and Technology (2023). Bristol set to host UK\u2019s most \npowerful supercomputer to turbocharge AI innovation. GOV.UK.\n23. Foreign, Commonwealth & Development Office et al. (2023). AI Safety Summit 2023. GOV.UK.\n24. Department for Science, Innovation and Technology et al. (2023). The Bletchley Declaration by \nCountries Attending the AI Safety Summit, 1-2 November 2023.\n25. Donelan, M. (2023). Introducing the AI Safety Institute. GOV.UK.\n26. The Alan Turing Institute (online). Data science and AI glossary.\n27. Cabinet Office et al. (2021). Ethics, Transparency and Accountability Framework for Automated"}
{"input":" In which region is the standard BS 8611:2016 published, and what are its ethical considerations? ","output":" The standard BS 8611:2016 is published in the Regional category, and it guides the ethical design and application of robots and robotic systems, including the identification, formulation, and evaluation of potential ethical harm. \u200b","context":"The Blueprint notes that individuals must be protected from abusive data practices and calls for data minimization rules, stating \u201c[y]ou should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected.\u201d\n\nIn the days following its release, EPIC Deputy Director Caitirona Fitzgerald and Senior Counsel Ben Winters published an op-ed in Protocol, urging the Biden Administration to take action to bring the Blueprint into practice. The Fall 2023 Executive Order and Office of Management and Budget Guidance does exactly that. (a)  Within 240 days of the date of this order, the Secretary of Commerce, in consultation with the heads of other relevant agencies as the Secretary of Commerce may deem appropriate, shall submit a report to the Director of OMB and the Assistant to the President for National Security Affairs identifying the existing standards, tools, methods, and practices, as well as the potential development of further science-backed standards and techniques, for:\n\n          (i)    authenticating content and tracking its provenance;\n\n          (ii)   labeling synthetic content, such as using watermarking;\n\n          (iii)  detecting synthetic content;\n\n          (iv)   preventing generative AI from producing child sexual abuse material or producing non-consensual intimate imagery of real individuals (to include intimate digital depictions of the body or body parts of an identifiable individual);\n\n          (v)    testing software used for the above purposes; and for their own judgement and places some restrictions on the use of surveillance tools to monitor \nworkers226\n\u2022   intellectual property law, such as the Copyright, Designs and Patents Act 1988,230 which \ngoverns ownership and legal use of any intellectual property in outputs or in datasets, and is the \nremit of the Intellectual Property Office\n10                                  POSTnote 708 - 9 January 2024 indigenous peoples (see OP\/BP 4.01, Environmental Assessments, OP\/BP 4.10, Indigenous Peoples,\nand OP\/BP 4.12 Involuntary Resettlement); (d) audited annual financial statements for investment\nproject financing (or, in exceptional cases, an abridged version thereof) for which the invitation to\nnegotiate is issued on or after July 1, 2010; and (e) procurement plans (see Guidelines: Procurement\n9\n(d) Other documents prepared by member countries\/borrowers that are in the\nBank\u2019s possession. Countries often give the Bank a document or information\nthat does not fall into one of the above categories (for example, on a country\u2019s\nmacroeconomic context, governance issues, or sector- or institution-specific\nanalyses). If the country\/borrower has assigned an information classification\nthat restricts public access, the Bank does not disclose the material without\nobtaining the country\u2019s\/borrower\u2019s written consent; normally the Bank refers"}
{"input":" What is the main focus of CAN\/CIOSC 100-1: 2020 as revised by the CIO Strategy Council? ","output":" The main focus of CAN\/CIOSC 100-1: 2020, as revised by the CIO Strategy Council, is on Digital Governance and Information Management.","context":"MMC Ventures. 2019. The State of AI 2019 Divergence.\nhttps:\/\/www.mmcventures.com\/The-State-of-AI-2019-Divergence.pdf\nMulgan, G. 2019. \u201cWhy Regulation Needs an Overhaul.\u201d Nesta (Blog), March 12.\nhttps:\/\/www.nesta.org.uk\/blog\/regulation-needs-overhaul\/\nMurison, Malek. 2018. \u201cBrazil\u2019s National IoT Strategy Gets the Green Light.\u201d Internet of Business, March 28.\nhttps:\/\/internetofbusiness.com\/brazil-national-iot-strategy\/\nMzekandaba, Simnikiwe. 2019. \u201cANC Promises to Bolster Tech Innovation.\u201d ITWeb, January 14.\nhttps:\/\/www.itweb.co.za\/content\/GxwQDM1A9BrMlPVo\nNational Communication Authority. 2018. Ghana\u2019s Digital Agenda.\nhttps:\/\/www.nca.org.gh\/assets\/Uploads\/Key-NCA-Projects-2018.pdf Future-proof legislation:\nAs AI is a fast evolving technology, the proposal has a future-proof approach, allowing rules to adapt to technological change. AI applications should remain trustworthy even after they have been placed on the market. This requires ongoing quality and risk management by providers.\n\nNext steps:\nFollowing the Commission\u2019s proposal in April 2021, the regulation could enter into force late 2022\/early 2023 in a transitional period. In this period, standards would be mandated and developed, and the governance structures set up would be operational. The second half of 2024 is the earliest time the regulation could become applicable to operators with the standards ready and the first conformity assessments carried out. Finally, lurking in the background are two new bodies created by party-state institutional reforms announced in March 2023: the CCP Central Science and Technology Commission (CSTC) and the National Data Administration (NDA). Neither has been formally stood up, and information on them remains scare. The CSTC will serve as the CCP\u2019s top science and\ntechnology policymaking body. It will likely have a significant voice in AI regulatory policy, but it appears that the majority of its portfolio will focus on technology development\u2014including major national research projects and national laboratories\u2014rather than regulation.\nThe CSTC will reportedly be housed in the MOST, likely giving a boost to the latter in AI governance. The NDA will focus on data infrastructure and the utilization of data to support economic and social policies. These two bodies will merit close examination as they take shape. International forums such as the G20 and G7,\norganizations such as the UN, ITU, OECD\nand UNESCO and supranational and regional\norganizations such as the African Union can\ncoordinate policies and pool resources across\ncountries to devise and implement AI strategies.\nPolicies at international levels can also help to define\nand monitor regulation, practices, and standards in\nAI applications. Some international organizations\nhave already proposed guidelines and approaches\nfor building capabilities for governing AI, often\nleveraging soft law. The UN High-Level Panel on\nDigital Cooperation for example was convened by\nthe UN Secretary-General to advance global multistakeholder dialogue regarding the potential of\ndigital technologies to advance human wellbeing\nwhile mitigating any risks of these technologies.4\nThe UN and the International Telecommunication\nUnion (ITU) have also taken steps towards\ninternational coordination by articulating an\napproach for the UN system to support AI adoption"}
{"input":" What are some reasons for the importance of international cooperation on AI? ","output":" International cooperation on AI is important because it maximizes the advantage of scale, exploits comparative advantages for mutual benefit, avoids competitive and duplicative investments, and benefits from scale in several essential inputs used in AI development.","context":"our competitors, while leading key global conversations and collaborations to ensure that AI benefits the whole world, rather than exacerbating inequities, threatening human rights, and causing other harms. Although technology has often influenced geopolitics, the prospect of AI means that the technology itself could become a geopolitical actor. AI could have motives and objectives that differ considerably from those of governments and private companies. Humans' inability to comprehend how AI \"thinks\" and our limited understanding of the second- and third-order effects of our commands or requests of AI are also very troubling. Humans have enough trouble interacting with one another. It remains to be seen how we will manage our relationships with one or more AIs. structural, cultural and political factors that ground China\u2019s stance on AI, as well as an indication of its possible trajectory, while also highlighting where ethical problems remain, arise, or are likely to be exacerbated. They should be addressed as early as it is contextually possible boundaries for acceptable uses of AI is present and pressing. Altogether, this article seeks to provide a detailed and critical understanding of the reasons behind, and the current trajectory of, China\u2019s AI strategy. It emphasises that the Chinese government is aware of the potential benefits, practical risks, and the ethical challenges that AI presents, and that the direction of China\u2019s AI strategy will largely be determined by the interplay of these factors and by the extent to which government\u2019s interests may outweigh ethical concerns. Section 5 concludes the paper by summarising the key findings of our analysis."}
{"input":" How does the EU proposal for AI regulation differ from previous international AI initiatives? ","output":" The EU proposal for AI regulation is marked as the first attempt to introduce a comprehensive legislative scheme governing AI, differentiating it from previous initiatives which focused more on general principles or specific policy frameworks.","context":"AN OVERVIEW OF NATIONAL AI STRATEGIES AND POLICIES \u00a9 OECD 2021\n\n\f26 |\n\nReferences\nBerryhill, J. et al. (2019), Hello, World: Artificial intelligence and its use in the public\nsector, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/726fd39d-en.\n\n[6]\n\nCockburn, I. (2018), \u201cThe impact of artificial intelligence on innovation\u201d,\nNo. 24449, National Bureau of Economic Research, Cambridge, US,\nhttp:\/\/dx.doi.org\/10.3386\/w24449.\n\n[2]\n\nEuropean Commission (2021), European legal framework for AI to address\nfundamental rights and safety risks specific to the AI systems, https:\/\/digitalstrategy.ec.europa.eu\/en\/policies\/regulatory-framework-ai.\n\n[8]\n\nEuropean Commission (2021), Revised Coordinated Plan on AI, https:\/\/digitalstrategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021review.\n\n[7] Policy implications of artificial intelligence (AI)\nThe Digital Regulation Cooperation Forum (DRCF) was established in 2020 to foster collaboration \nbetween regulators regarding digital affairs.231 As of 2023, the DRCF consists of Ofcom, the \nInformation Commissioner\u2019s Office, the Competition and Markets Authority, and the Financial Conduct \nAuthority.\nIn the 2023 white paper \u2018A pro-innovation approach to AI regulation\u2019,21 the Government outlined a \ncommon set of cross-sectoral AI regulation principles for regulators to follow:\n\u2022  safety, security, and robustness\n\u2022  appropriate transparency and explainability\n\u2022  fairness\n\u2022  accountability and governance\n\u2022  contestability and redress\nIn the EU and US\nThe European Union is currently finalising an AI Act.232 As it stands, the Act is designed to work \nwith existing EU legislation such as the General Data Protection Regulation (GDPR) and the Digital \nServices Act (DSA). Title - AI Regulation is Coming- What is the Likely Outcome?\nBlog Post by Bill Whyman\n\nPublished October 10, 2023\n\n\u201cAI is too important not to regulate\u2014and too important not to regulate well,\u201d says Google. It is highly likely that Artificial Intelligence will be regulated. In many ways, it already is. The EU\u2019s AI Act has extensive top-down prescriptive rules including prohibiting uses of AI that it says pose unacceptable risk; it\u2019s in the final stages of passing. China has ruled that algorithms must be reviewed in advance by the state and \u201cshould adhere to the core socialist values\u201d. The United States is taking its typical decentralized approach. 60 http:\/\/thefuturesociety.org\/2020\/03\/17\/unesco-regional-forum-on-ai-in-latin-america-and-the-caribbean\/\n\n29\n\n\fANALYTICAL INSIGHTS - NOTE 4\n\nOverview of AI Flagship Initiatives Observed\nAs observed across the eleven countries reviewed,\nthe AI policymaking landscape has a wide range\nof policy and regulatory approaches. Some\ngovernments focus on regulation, policy reform,\nsoft law and multi-stakeholder partnerships to\ncreate an \u201cenabling\u201d environment for AI adoption,\n\nas in the case of Finland. Other countries opt for\na more \u201cdirecting\u201d approach centered on strategic\ninvestments, direct funding and establishing new\ncenters, such as in the UAE. The analysis of the\ncountry cases has also identified some common\nflagship initiatives which are summarized in Box 2.\n\nBOX 2: Overview of Common AI Flagship Initiatives Observed"}
{"input":" How do you view the role of standard-setting organizations like ISO, IEC, and IEEE in the context of AI? ","output":" You should view these standard-setting organizations as significant contributors to the technical aspects of AI, helping to develop global standards for AI which include both technical and ethical dimensions of responsible AI development.","context":"Second, much of what the U.S. ends up doing in terms of domestic AI governance will influence international AI outcomes. For example, the White House Voluntary AI Commitments have become the basis for the International Code of Conduct for Organizations Developing Advanced AI Systems released by the G7 on October 30.\n\nThird, the raft of new AI standards and guidelines across the federal government that the EOAI mandates will influence how governments and industries across the world approach AI governance. This will happen as the sheer size of the U.S. government procurement market incentivizes companies to converge on the U.S. AI standards and guidelines. Moreover, it is likely that many of these AI standards will be further developed and internationalized through more formal AI standards-setting processes in global standards development bodies such as the ISO\/IEC. International harmonization. Nearly every country is having its own debate about AI\u2019s dramatic impact. Different national rules will inevitably conflict, including intellectual property rights and antitrust. The EU AI Act is almost certain to be more regulatory than the U.S. approach, and the U.S.-EU Trade and Technology Council is coordinating their differences. The G7 has launched the \u201cHiroshima AI process\u201d. The OECD has developed AI principles, and the United Nations has proposed a new UN AI advisory body to better include views of developing countries. International technical standards, such as developed via ISO and IEEE, are a promising approach that should be considered. Shared R&D and education assistance can also encourage AI harmonization.  \n\nIII. Likely Outcome: A Bottom-up Patchwork Quilt of AI Rules. Technical standards function as a baseline to gauge a product's features and performance. For example, ISO\/IEC JTC 1\/SC 42, an international standard for AI, includes \u201cassessment of machine learning classification performance\u201d and \u201cbig data reference architecture.\u201d These technical standards enable a common platform for risk assessments and audits, allowing countries with varying regulations to mutually assess and evaluate AI systems or services. It is worth pointing out, however, that technical standards are not a panacea. To create a truly interoperable framework for AI governance, collaboration that goes beyond just technical aspects and includes socio-technical and normative elements is needed as well. For instance, in the NIST AI Risk Management Framework's Initial Draft, aspects such as explainability and bias are labeled as socio-technical elements, and fairness, accountability, and transparency are referred to as \u201cguiding principles.\u201d These elements, distinct from purely In line with this publication, the Standardization Administration of the People\u2019s Republic of China, the national-level body responsible for developing technical standards, released a white paper on AI standards. The paper contains a discussion of the safety and ethical issues related to technology (Ding and Triolo 2018). Three key principles for setting the ethical requirements of AI technologies are outlined. First, the principle of human interest states that the ultimate goal of AI is to benefit human welfare. Second, the principle of liability emphasises the need to establish accountability as a requirement for both the development and the deployment of AI systems and solutions. Subsumed within this principle is transparency, which supports the requirement of understanding what the operating principles of an AI system are. Third, the principle of consistency of [sic] rights and responsibilities emphasised that, on the one hand, data should be properly recorded and oversight"}
{"input":" What significant initiative was PAI launching at the event?","output":" PAI was launching their safety protocols for foundation models for public comment, which are a set of comprehensive and forward-looking guidelines for identifying and mitigating risks associated with large-scale AI deployment.","context":"For full guest lists and more details on the forums, check out the Tech Policy Press US Senate \u2018AI Insight Forum\u2019 Tracker.\n\nUK AI Safety Summit 2023\n\nThe two-day, first-of-its-kind AI Safety Summit will officially kick off on Wednesday, Nov. 1. Hosted by the UK government, Prime Minister Rishi Sunak will bring global leaders, government officials, tech executives, and experts to Bletchley Park to discuss \u201chighly capable\u201d frontier AI systems and the future risks they pose. The Financial Times published a full list of attendees, which includes tech leaders from Anthropic, OpenAI, and Google, government officials from Ireland and Kenya, and researchers from the Oxford Internet Institute and the Stanford Cyber Policy Institute, among a long list of others. Some of the most notable guests include: The desire to leapfrog the US is echoed in statements from China\u2019s political and military leadership. For instance, President Xi Jinping stated in 2017 that \u2018under a situation of increasingly fierce international military competition, only the innovators win\u2019 (Kania 2020, p. 2). This sentiment is shared by Lieutenant General Liu Guozhi, deputy of the 19th National Congress and director of the Science and Technology Committee of the Central Military Commission, who stated in an interview that AI presented a rare opportunity for taking shortcuts to achieve innovation and surpass rivals (\u201cAI military reform\u201d 2017). In parallel, academics affiliated with the People\u2019s Liberation Army (PLA) highlight that AI will be used to predict battlefield situations and identify optimal approaches, facilitating \u2018winning before the war\u2019 (Li 2019). Some members of the PLA go further than this in anticipating a battlefield \u2018singularity\u2019, where AI outpaces human decision-making (Kania 2017a). These facilitating \u2018winning before the war\u2019 (Li 2019). Some members of the PLA go further than this in anticipating a battlefield \u2018singularity\u2019, where AI outpaces human decision-making (Kania 2017a). These statements emphasise the belief, which is widespread throughout China\u2019s military and defence circles, in the importance of utilising emergent technologies including AI to achieve a competitive military advantage. This section synthesizes the findings from the\nreview of AI strategies in Finland and the UAE.\nThe heat maps shown in Figure 3 (Finland) and\nFigure 4 (UAE) indicate the role of government\nfor initiatives in each category as either \u201cdirecting,\u201d\n\u201cenabling\u201d, or having no initiative, reflecting\ngovernment approaches defined earlier in Box 2.\nThe assessment in these heat maps is based on a\nbroader review of the national strategies for these\ncountries included in the background paper of this\nreport, reflecting details that are not referenced in\nthe shorter summaries included in this working\npaper. The authors acknowledge that some AI\npolicies and initiatives may not have been available\nfor consideration during the period of review\nbetween 2019-2020, and that different conclusions\nmay be reached regarding the magnitude of\ngovernment interventions across the domains,\ntools and instruments illustrated in the heat maps.\n\nworldbank.org\/digitaldevelopment\n21"}
{"input":" Can you name a session from the agenda that focused on the UK's approach to AI governance and its global implications?","output":" Yes, the session titled The UK Perspective on AI Governance and Global Implications \u2013 A Fireside Chat with The Alan Turing Institute focused on the UK's approach to AI governance, including priorities like frontier models and catastrophic risks, and its global implications.","context":"Creating a new\ngovernmental or\nindependent body for\nAI\n\n\u2022 AI policy in the United Kingdom is coordinated by the UK Government\u2019s Office for\nArtificial Intelligence.\n\u2022 The U.S. White House established the National AI Initiative Office.\n\u2022 Singapore created a National AI Office to co-ordinate the implementation of its national\nAI strategy.\n\nAI expert advisory\ngroups\n\n\u2022 Austria\u2019s Council on Robotics and AI\n\u2022 Canada\u2019s Advisory Council on AI\n\u2022 Spain\u2019s Artificial Intelligence Advisory Council\n\u2022 The United States\u2019 Select Committee on AI under the National Science and Technology\nCouncil\n\nOversight and\nadvisory bodies for AI\nand data ethics\n\n\u2022 Germany\u2019s Data Ethics Commission\n\u2022 The Data Ethics Advisory Group in New Zealand\n\u2022 The United Kingdom\u2019s Centre for Data Ethics and Innovation (CDEI)\n\u2022 Singapore\u2019s Advisory Council on the Ethical Use of AI and Data.\n\nNote: This infographic offers a non-exhaustive selection of national AI governance\nimplementation examples.\nSource: Authors. While there is much variation in the form a\nstrategy takes \u2013 from a public announcement, to a\nguiding document, to a more comprehensive plan\n\u2013 governments are uniquely situated to consider\nthe risks and opportunities of AI within their own\nnational contexts and provide strategic direction.\nPrevious research that has explored and compared\nnational AI strategies and policies includes the\n8\nannual Stanford AI Index (2021), the OECD AI\nPolicy Observatory launched in 2020,9 the annual\nOxford Insights Government AI Readiness Index\n10\n(2020), the Future of Life Institute\u2019s review of\nNational and International AI Strategies,11 AiLab\u2019s 8\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nThe typology of AI use cases in the public sector includes:\n\u2022\n\nCITIZEN ENGAGEMENT. The introduction of AI tools such as chatbots that\nanswer citizen queries. For example, how can I apply for social welfare\nbenefits? Additionally, aggregation and pattern determination can be used\nto collect feedback from millions of citizens, for example on a draft policy or\nlegislation published online for public consultations.\n\n\u2022\n\nCITIZEN ENGAGEMENT: AI was able to analyze citizen sentiments from 21\nmillion comments received on the US Government\u2019s policy on NetNeutrality shared online;\n\n\u2022\n\nCOMPLIANCE AND RISK MANAGEMENT: In Armenia, AI helped the revenue agency increase its revenue by 6 times. The World Bank supported\nthe underlying technologies for this project. For more details, please watch\nthe World Bank hosted event.\n\n\u2022 Next Steps:\nThe government requested views on certain proposals, including the cross-sectoral principles, by June 21, 2023. The white paper also includes a long list of actions for the UK government to undertake over the coming year and beyond, which include the following:\nPublishing a portfolio of AI assurance techniques:\nPublishing an AI regulation roadmap for the central risk and monitoring functions\nEncouraging regulators to publish guidance on how cross-sectoral principles will apply within their remit\nPublishing a draft central, cross-economy AI risk register for consultation"}
{"input":" How did the Looking Ahead \u2014 Democracy by Design: Election Integrity in the Era of Generative AI session approach the impact of AI on elections?","output":" The session explored how AI, particularly through AI-generated images and synthetic media, might impact upcoming elections in the US and globally, discussed policies to strengthen democracy, and examined the roles of industry, civil society, academia, and government in protecting election integrity.","context":"138. Oxford Analytica (2023). Generative AI carries serious online risks.\n139. Clementson, D. E. (2023). 6 ways AI can make political campaigns more deceptive than ever. The \nConversation.\n140. Kroetsch, J. (2023). Skepticism in Era of AI Deep Fakes Will Erode Defamation Claims. \nBloomberg Law.\n141. Krimmer, R. et al. (2022). Elections in digital times: a guide for electoral practitioners. \nUNESCO.\n142. European Conferences of Electoral Management Bodies (2022). Concept paper 2022 - \u2018Artificial \nIntelligence and Electoral Integrity\u2019. European Conferences of Electoral Management Bodies.\n143. The Law Society (2019). Algorithm use in the criminal justice system report. The Law Society.\n144. Babuta, A. et al. (2019). Data Analytics and Algorithmic Bias in Policing. Royal United \nServices Institute.\n145. Metropolitan Police (online). Live Facial Recognition.\n146. Fussey, P. et al. (2019). Independent Report on the London Metropolitan Police Service\u2019s Trial Policy implications of artificial intelligence (AI)\nsystem to keep up with AI, say regulators. The Guardian.\n134. Panditharatne, M. et al. (2023). How AI Puts Elections at Risk \u2014 And the Needed Safeguards. \nBrennan Center for Justice.\n135. Bantourakis, M. (2023). How can we build trustworthy media ecosystems in the age of AI and \ndeclining trust? World Economic Forum.\n136. Kahn, G. (2023). Will AI-\ngenerated images create a new crisis for fact-checkers? Experts are not so sure. Reuters Institute \nfor the Study of Journalism.\n137. Mont\u2019Alverne, C. et al. (2022). The trust gap: how and why news on digital platforms is viewed \nmore sceptically versus news in general. Reuters Institute for the Study of Journalism.\n138. Oxford Analytica (2023). Generative AI carries serious online risks.\n139. Clementson, D. E. (2023). 6 ways AI can make political campaigns more deceptive than ever. The \nConversation. \u2020 Incidences of this have already been reported, such as an Indian politician who claimed a \nreportedly authentic audio clip of him was a deepfake.140\n7                                  POSTnote 708 - 9 January 2024\nPolicy implications of artificial intelligence (AI)\nAI could also be used to strengthen democracy.141 AI could be used to engage the public with \npolitics and the electoral process. It could help voters understand manifestos and identify which \ncandidates or political parties may best align with their priorities.132,142\nSurveillance\nThere has been increasing use of both live and retrospective facial recognition* by private \ncompanies and police forces, as well as predictive policing that uses AI to predict hotspots for \nfuture crime.143,144\nThe Metropolitan Police said these tools save police officers\u2019 time, help identify\ncriminals, and safeguard vulnerable people.145\nConcerns have been raised by some academics,146\u2013154 parliamentarians,155 human rights campaign 8\n\n>>>\n\nARTIFICIAL INTELLIGENCE IN THE PUBLIC SECTOR\n\n\fWhat Ethical Principles and\nGovernance Frameworks exist?\nThe typology of AI use cases in the public sector includes:\n\u2022\n\nCITIZEN ENGAGEMENT. The introduction of AI tools such as chatbots that\nanswer citizen queries. For example, how can I apply for social welfare\nbenefits? Additionally, aggregation and pattern determination can be used\nto collect feedback from millions of citizens, for example on a draft policy or\nlegislation published online for public consultations.\n\n\u2022\n\nCITIZEN ENGAGEMENT: AI was able to analyze citizen sentiments from 21\nmillion comments received on the US Government\u2019s policy on NetNeutrality shared online;\n\n\u2022\n\nCOMPLIANCE AND RISK MANAGEMENT: In Armenia, AI helped the revenue agency increase its revenue by 6 times. The World Bank supported\nthe underlying technologies for this project. For more details, please watch\nthe World Bank hosted event.\n\n\u2022"}
