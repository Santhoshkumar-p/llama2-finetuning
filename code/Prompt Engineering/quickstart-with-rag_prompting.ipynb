{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quick Start Notebook\n",
    "\n",
    "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "The example uses the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
    "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
    "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "# pip install llama-recipes transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/transformers/models/llama/convert_llama_weights_to_hf.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print('/'.join(transformers.__file__.split('/')[:-1]) + '/models/llama/convert_llama_weights_to_hf.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all parameters from the checkpoint at llama-2-7b.\n",
      "Loading the checkpoint in a Llama model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:03<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving in the Transformers format.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python '/jet/home/apatula/.local/lib/python3.10/site-packages/transformers/models/llama/convert_llama_weights_to_hf.py' --input_dir llama-2-7b --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 0: RAG & DataPreparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain_community in /jet/home/apatula/.local/lib/python3.10/site-packages (0.0.24)\n",
      "Requirement already satisfied: tiktoken in /jet/home/apatula/.local/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: langchain-openai in /jet/home/apatula/.local/lib/python3.10/site-packages (0.0.8)\n",
      "Requirement already satisfied: langchainhub in /jet/home/apatula/.local/lib/python3.10/site-packages (0.1.14)\n",
      "Requirement already satisfied: chromadb in /jet/home/apatula/.local/lib/python3.10/site-packages (0.4.23)\n",
      "Requirement already satisfied: langchain in /jet/home/apatula/.local/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain_community) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain_community) (0.6.4)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain_community) (0.1.27)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain_community) (0.1.9)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain_community) (8.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain-openai) (1.12.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchainhub) (2.31.0.20240218)\n",
      "Requirement already satisfied: build>=1.0.3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (1.0.3)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.5.3)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.27.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (1.15.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (6.1.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /jet/home/apatula/.local/lib/python3.10/site-packages (from chromadb) (3.9.15)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /jet/home/apatula/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /jet/home/apatula/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.26.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /jet/home/apatula/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /jet/home/apatula/.local/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain_community) (4.3.0)\n",
      "Requirement already satisfied: coloredlogs in /jet/home/apatula/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /jet/home/apatula/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /jet/home/apatula/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /jet/home/apatula/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.2.2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /jet/home/apatula/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.6)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /jet/home/apatula/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /jet/home/apatula/.local/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /jet/home/apatula/.local/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /jet/home/apatula/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /jet/home/apatula/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /jet/home/apatula/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: httpcore==1.* in /jet/home/apatula/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opencv-python in /jet/home/apatula/.local/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.24.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentence-transformers in /jet/home/apatula/.local/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from sentence-transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /jet/home/apatula/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /jet/home/apatula/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /jet/home/apatula/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Documents Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 52.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "DOCS_DIR = './docs'\n",
    "loader = DirectoryLoader(DOCS_DIR, glob='**/*.txt', show_progress=True, loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split Files into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Embed\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.embeddings.huggingface import (HuggingFaceEmbeddings)\n",
    "\n",
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do China's AI regulations, particularly o...</td>\n",
       "      <td>China's AI regulations prioritize information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why should international discourse take China'...</td>\n",
       "      <td>China's AI regulations reshape global AI deplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the structural similarities found in ...</td>\n",
       "      <td>China's AI regulations share three structural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does China formulate AI governance regulat...</td>\n",
       "      <td>China formulates AI governance regulations thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the trajectory of Chinese AI governanc...</td>\n",
       "      <td>Chinese AI governance is heading towards draft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0   How do China's AI regulations, particularly o...   \n",
       "1  Why should international discourse take China'...   \n",
       "2  What are the structural similarities found in ...   \n",
       "3  How does China formulate AI governance regulat...   \n",
       "4  What is the trajectory of Chinese AI governanc...   \n",
       "\n",
       "                                              output  \n",
       "0  China's AI regulations prioritize information ...  \n",
       "1  China's AI regulations reshape global AI deplo...  \n",
       "2  China's AI regulations share three structural ...  \n",
       "3  China formulates AI governance regulations thr...  \n",
       "4  Chinese AI governance is heading towards draft...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "qa = pd.read_json(path_or_buf='QAndA_JSONL.txt', lines=True)\n",
    "qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do China's AI regulations, particularly o...</td>\n",
       "      <td>China's AI regulations prioritize information ...</td>\n",
       "      <td>China’s three most concrete and impactful regu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why should international discourse take China'...</td>\n",
       "      <td>China's AI regulations reshape global AI deplo...</td>\n",
       "      <td>But international discourse on Chinese AI gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the structural similarities found in ...</td>\n",
       "      <td>China's AI regulations share three structural ...</td>\n",
       "      <td>2 AI legislation in China And despite China’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does China formulate AI governance regulat...</td>\n",
       "      <td>China formulates AI governance regulations thr...</td>\n",
       "      <td>How China Sets AI Governance Policy\\nThis pape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the trajectory of Chinese AI governanc...</td>\n",
       "      <td>Chinese AI governance is heading towards draft...</td>\n",
       "      <td>In this series of three papers, I will attempt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0   How do China's AI regulations, particularly o...   \n",
       "1  Why should international discourse take China'...   \n",
       "2  What are the structural similarities found in ...   \n",
       "3  How does China formulate AI governance regulat...   \n",
       "4  What is the trajectory of Chinese AI governanc...   \n",
       "\n",
       "                                              output  \\\n",
       "0  China's AI regulations prioritize information ...   \n",
       "1  China's AI regulations reshape global AI deplo...   \n",
       "2  China's AI regulations share three structural ...   \n",
       "3  China formulates AI governance regulations thr...   \n",
       "4  Chinese AI governance is heading towards draft...   \n",
       "\n",
       "                                             context  \n",
       "0  China’s three most concrete and impactful regu...  \n",
       "1  But international discourse on Chinese AI gove...  \n",
       "2  2 AI legislation in China And despite China’s ...  \n",
       "3  How China Sets AI Governance Policy\\nThis pape...  \n",
       "4  In this series of three papers, I will attempt...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retrieve 4 Relevant documents chunks - Feel free to change\n",
    "#results = vectorstore.similarity_search(query, k=4)\n",
    "qa['context'] = qa['input'].apply(lambda x: ' '.join([result.page_content for result in vectorstore.similarity_search(x, k=4)]))\n",
    "qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_qa = qa\n",
    "processed_qa.to_json('qa_with_context.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point base_model to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "base_model = \"./models_hf/7B\"\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "finetuned_model = './finetuned/policy-llama2-7b'\n",
    "\n",
    "dataset = Dataset(pa.Table.from_pandas(pd.read_json(path_or_buf='QAndA_JSONL.txt', lines=True)))\n",
    "#dataset = load_dataset(guanaco_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \" How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?\",\n",
       " 'output': 'China\\'s AI regulations prioritize information control through measures like barring excessive price discrimination in recommendation algorithms, requiring labels on synthetically generated content in deep synthesis, and demanding \"true and accurate\" data and outputs in generative AI.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5548bb094b054da89a7e9490b7beda4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    #device_map={\"\": 0, \"\":1}\n",
    "    device_map='auto'\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load LLaMA tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following question:\n",
      "Question: How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?\n",
      "---\n",
      "Answer:\n",
      "\n",
      "China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control by requiring companies to obtain approval from the government before using these technologies. This is done to ensure that the information being used is accurate and not misleading, and to prevent the spread of false or harmful information.\n",
      "\n",
      "For example, the Chinese government has implemented regulations that require companies to obtain approval before using recommendation algorithms to suggest products\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Answer the following question:\n",
    "Question: How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?\n",
    "---\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following question with the given context with succinct summary:\n",
      "Question: How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?\n",
      "Context: China’s three most concrete and impactful regulations on algorithms and AI are its 2021\n",
      "regulation on recommendation algorithms, the 2022 rules for deep synthesis (synthetically generated content), and the 2023 draft rules on generative AI. Information control is a central goal of all three measures, but they also contain many other notable provisions. The rules for recommendation algorithms bar excessive price discrimination and protect the rights of workers subject to algorithmic scheduling. \n",
      "The deep synthesis regulation requires conspicuous labels be placed on synthetically generated content. And the draft generative AI regulation requires both the training data and model outputs to be “true and accurate,” Summary: China is in the midst of rolling out some of the world’s earliest and most detailed regulations governing artificial intelligence (AI). These include measures governing recommendation algorithms—the most omnipresent form of AI deployed on the internet—as well as new rules for synthetically generated images and chatbots in the mold of ChatGPT. \n",
      "China’s emerging AI governance framework will reshape how the technology is built and deployed within China and internationally, impacting both Chinese technology exports and global AI research networks.\n",
      "But in the West, China’s regulations are often dismissed as irrelevant or seen purely through the lens of a geopolitical competition to write the rules for AI. These extremely demanding requirements for generative AI systems have kicked off a particularly active public debate on the draft regulation. At the time of writing, Chinese scholars, companies, and policymakers are actively discussing how to maintain effective content controls without squashing China’s nascent generative AI industry. The third paper in this series will dive deep into how this policy debate is playing out in public workshops, academic writing, and corporate lobbying.\n",
      "Countries and cultures may differ on the specific content of AI regulations, but they can learn from the content-agnostic structure of the regulations themselves. The above Chinese regulations share three structural similarities: the choice of algorithms as a point of entry; the building of regulatory tools and bureaucratic know-how; and the vertical and iterative approach that is laying the groundwork for a capstone AI law. Three regulations require the deepest analysis: recommendation algorithms, “deep synthesis,” and generative AI. These interconnected documents contain the most targeted and impactful regulations to date, creating concrete requirements for how algorithms and AI are built and deployed in China. Below is a brief overview of each regulation. The remainder of this paper and subsequent papers will expand on the intellectual roots and key bureaucratic actors behind these regulations.\n",
      "Provisions on the Management of Algorithmic Recommendations in Internet Information Services\n",
      "---\n",
      "Answer:\n",
      "China’s AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#With Context\n",
    "eval_prompt = \"\"\"\n",
    "Answer the following question with the given context with succinct summary:\n",
    "Question: How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?\n",
    "Context: China\\u2019s three most concrete and impactful regulations on algorithms and AI are its 2021\\nregulation on recommendation algorithms, the 2022 rules for deep synthesis (synthetically generated content), and the 2023 draft rules on generative AI. Information control is a central goal of all three measures, but they also contain many other notable provisions. The rules for recommendation algorithms bar excessive price discrimination and protect the rights of workers subject to algorithmic scheduling. \\nThe deep synthesis regulation requires conspicuous labels be placed on synthetically generated content. And the draft generative AI regulation requires both the training data and model outputs to be \\u201ctrue and accurate,\\u201d Summary: China is in the midst of rolling out some of the world\\u2019s earliest and most detailed regulations governing artificial intelligence (AI). These include measures governing recommendation algorithms\\u2014the most omnipresent form of AI deployed on the internet\\u2014as well as new rules for synthetically generated images and chatbots in the mold of ChatGPT. \\nChina\\u2019s emerging AI governance framework will reshape how the technology is built and deployed within China and internationally, impacting both Chinese technology exports and global AI research networks.\\nBut in the West, China\\u2019s regulations are often dismissed as irrelevant or seen purely through the lens of a geopolitical competition to write the rules for AI. These extremely demanding requirements for generative AI systems have kicked off a particularly active public debate on the draft regulation. At the time of writing, Chinese scholars, companies, and policymakers are actively discussing how to maintain effective content controls without squashing China\\u2019s nascent generative AI industry. The third paper in this series will dive deep into how this policy debate is playing out in public workshops, academic writing, and corporate lobbying.\\nCountries and cultures may differ on the specific content of AI regulations, but they can learn from the content-agnostic structure of the regulations themselves. The above Chinese regulations share three structural similarities: the choice of algorithms as a point of entry; the building of regulatory tools and bureaucratic know-how; and the vertical and iterative approach that is laying the groundwork for a capstone AI law. Three regulations require the deepest analysis: recommendation algorithms, \\u201cdeep synthesis,\\u201d and generative AI. These interconnected documents contain the most targeted and impactful regulations to date, creating concrete requirements for how algorithms and AI are built and deployed in China. Below is a brief overview of each regulation. The remainder of this paper and subsequent papers will expand on the intellectual roots and key bureaucratic actors behind these regulations.\\nProvisions on the Management of Algorithmic Recommendations in Internet Information Services\n",
    "---\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load LoRA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Set supervised fine-tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example) -> list:\n",
    "    output_texts = []\n",
    "    for i in range(len(example['input'])):\n",
    "        text = f\"### Question: {example['input'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf074b56c3d49d0b62266fdd69a22df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    #dataset_text_field=\"input\",\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  1 10:51:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              67W / 300W |   6915MiB / 32768MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     63958      C   /usr/bin/python                            6912MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0967, 'grad_norm': 1.975710391998291, 'learning_rate': 0.0002, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7346, 'grad_norm': 1.4234060049057007, 'learning_rate': 0.0002, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5139, 'grad_norm': 1.570936918258667, 'learning_rate': 0.0002, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3189, 'grad_norm': 3.032320022583008, 'learning_rate': 0.0002, 'epoch': 3.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9871, 'grad_norm': 2.8095271587371826, 'learning_rate': 0.0002, 'epoch': 4.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8313, 'grad_norm': 4.131085395812988, 'learning_rate': 0.0002, 'epoch': 5.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.618, 'grad_norm': 6.505243301391602, 'learning_rate': 0.0002, 'epoch': 6.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5018, 'grad_norm': 3.74880051612854, 'learning_rate': 0.0002, 'epoch': 7.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3816, 'grad_norm': 2.4909563064575195, 'learning_rate': 0.0002, 'epoch': 8.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.289, 'grad_norm': 6.135060787200928, 'learning_rate': 0.0002, 'epoch': 8.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2218, 'grad_norm': 2.8139936923980713, 'learning_rate': 0.0002, 'epoch': 9.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.236, 'grad_norm': 4.784742832183838, 'learning_rate': 0.0002, 'epoch': 10.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1774, 'grad_norm': 2.939378261566162, 'learning_rate': 0.0002, 'epoch': 11.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1725, 'grad_norm': 5.531339168548584, 'learning_rate': 0.0002, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1551, 'grad_norm': 4.944835662841797, 'learning_rate': 0.0002, 'epoch': 13.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1478, 'grad_norm': 4.4816999435424805, 'learning_rate': 0.0002, 'epoch': 14.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1276, 'grad_norm': 1.8670432567596436, 'learning_rate': 0.0002, 'epoch': 15.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1468, 'grad_norm': 2.781198024749756, 'learning_rate': 0.0002, 'epoch': 16.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1205, 'grad_norm': 1.5960490703582764, 'learning_rate': 0.0002, 'epoch': 16.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1227, 'grad_norm': 3.306147575378418, 'learning_rate': 0.0002, 'epoch': 17.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1153, 'grad_norm': 2.599013328552246, 'learning_rate': 0.0002, 'epoch': 18.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.112, 'grad_norm': 1.2789088487625122, 'learning_rate': 0.0002, 'epoch': 19.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1221, 'grad_norm': 2.09159517288208, 'learning_rate': 0.0002, 'epoch': 20.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1053, 'grad_norm': 1.122633934020996, 'learning_rate': 0.0002, 'epoch': 21.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1041, 'grad_norm': 2.248042345046997, 'learning_rate': 0.0002, 'epoch': 22.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1024, 'grad_norm': 3.5675816535949707, 'learning_rate': 0.0002, 'epoch': 23.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1074, 'grad_norm': 1.7060132026672363, 'learning_rate': 0.0002, 'epoch': 24.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1015, 'grad_norm': 1.3063948154449463, 'learning_rate': 0.0002, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0958, 'grad_norm': 1.991127371788025, 'learning_rate': 0.0002, 'epoch': 25.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0982, 'grad_norm': 2.1767566204071045, 'learning_rate': 0.0002, 'epoch': 26.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0952, 'grad_norm': 1.4902509450912476, 'learning_rate': 0.0002, 'epoch': 27.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.093, 'grad_norm': 2.8606367111206055, 'learning_rate': 0.0002, 'epoch': 28.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0978, 'grad_norm': 1.6518049240112305, 'learning_rate': 0.0002, 'epoch': 29.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0928, 'grad_norm': 2.1463418006896973, 'learning_rate': 0.0002, 'epoch': 30.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0864, 'grad_norm': 0.8374613523483276, 'learning_rate': 0.0002, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0873, 'grad_norm': 1.415609359741211, 'learning_rate': 0.0002, 'epoch': 32.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0962, 'grad_norm': 0.8471890687942505, 'learning_rate': 0.0002, 'epoch': 33.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0925, 'grad_norm': 0.515825092792511, 'learning_rate': 0.0002, 'epoch': 33.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0898, 'grad_norm': 0.6309105753898621, 'learning_rate': 0.0002, 'epoch': 34.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0883, 'grad_norm': 1.792441725730896, 'learning_rate': 0.0002, 'epoch': 35.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0876, 'grad_norm': 0.43727460503578186, 'learning_rate': 0.0002, 'epoch': 36.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0836, 'grad_norm': 1.7587834596633911, 'learning_rate': 0.0002, 'epoch': 37.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0924, 'grad_norm': 1.2118853330612183, 'learning_rate': 0.0002, 'epoch': 38.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0861, 'grad_norm': 1.3718328475952148, 'learning_rate': 0.0002, 'epoch': 39.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0781, 'grad_norm': 1.0160911083221436, 'learning_rate': 0.0002, 'epoch': 40.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0839, 'grad_norm': 0.739775538444519, 'learning_rate': 0.0002, 'epoch': 41.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0828, 'grad_norm': 0.3934336006641388, 'learning_rate': 0.0002, 'epoch': 41.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0778, 'grad_norm': 0.46670302748680115, 'learning_rate': 0.0002, 'epoch': 42.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0781, 'grad_norm': 1.1458052396774292, 'learning_rate': 0.0002, 'epoch': 43.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0813, 'grad_norm': 2.7020559310913086, 'learning_rate': 0.0002, 'epoch': 44.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.08, 'grad_norm': 0.5649428367614746, 'learning_rate': 0.0002, 'epoch': 45.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.079, 'grad_norm': 0.18898987770080566, 'learning_rate': 0.0002, 'epoch': 46.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0807, 'grad_norm': 2.7354485988616943, 'learning_rate': 0.0002, 'epoch': 47.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0794, 'grad_norm': 0.20424127578735352, 'learning_rate': 0.0002, 'epoch': 48.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0764, 'grad_norm': 1.7597438097000122, 'learning_rate': 0.0002, 'epoch': 49.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0791, 'grad_norm': 0.7118402719497681, 'learning_rate': 0.0002, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 655.082, 'train_samples_per_second': 8.472, 'train_steps_per_second': 2.137, 'train_loss': 0.27126512791429247, 'epoch': 50.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1400, training_loss=0.27126512791429247, metrics={'train_runtime': 655.082, 'train_samples_per_second': 8.472, 'train_steps_per_second': 2.137, 'train_loss': 0.27126512791429247, 'epoch': 50.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  1 11:02:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   47C    P0              73W / 300W |  32383MiB / 32768MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     63958      C   /usr/bin/python                           32380MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/apatula/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./models_hf/7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned/policy-llama2-7b/tokenizer_config.json',\n",
       " './finetuned/policy-llama2-7b/special_tokens_map.json',\n",
       " './finetuned/policy-llama2-7b/tokenizer.model',\n",
       " './finetuned/policy-llama2-7b/added_tokens.json',\n",
       " './finetuned/policy-llama2-7b/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(finetuned_model)\n",
    "trainer.tokenizer.save_pretrained(finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following question:\n",
      "Question: How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?\n",
      "---\n",
      "Answer:\n",
      "China's AI regulations prioritize information control through measures like barring excessive price discrimination in recommendation algorithms, requiring labels on synthetically generated content in deep synthesis, and demanding \"true and accurate\" data and outputs in generative AI. These measures aim to ensure AI contributes to sustainable development and meets the needs of Chinese communities.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Answer the following question:\n",
    "Question: How do China's AI regulations, particularly on recommendation algorithms, deep synthesis, and generative AI, focus on information control?\n",
    "---\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 4000;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "from tensorboard import notebook\n",
    "log_dir = \"results/runs\"\n",
    "notebook.start(\"--logdir {} --port 4000\".format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List the most pressing topics regarding regulation of AI. \n",
      "\n",
      "- Defining AI accurately\n",
      "- The role of AI in the future of work\n",
      "- AI's impact on competition and pricing\n",
      "- The role of AI in the next generation of digital platforms\n",
      "- AI and the future of automation\n",
      "\n",
      "Why is it difficult to regulate AI? \n",
      "\n",
      "- AI is a general-purpose technology that is likely to be ubiquitous and apply to a wide range of applications\n",
      "=========================================================================================\n",
      "Who owns material generated by a company’s large language model?\n",
      " ### Question: What ethical principles guide the use of large language models?\n",
      " ### Answer: Ethical use involves ensuring model explainability, transparency, and accountability through relevant disclosures and compliance with relevant laws and regulations. Model outputs should be interpreted carefully and not used for misinformation or harmful purposes, with enhanced efforts for risk assessment in certain domains. Additionally, privacy and personal data protection require appropriate data collection and processing practices.\n",
      " ### Question\n",
      "=========================================================================================\n",
      "Describe how China and the United States are approaching making new legislation to regulate Generative AI? \n",
      " ### Answer: China is moving quickly to draft new legislation that will regulate Generative AI differently than the United States. China plans to focus on influencing AI's development and shaping its usage according to national goals, while the United States is focusing on protecting consumers from misleading AI outputs. \n",
      " ### Answer: To catch up to China's rapid legislative drafting, the United States is expected to draft new legislation that will provide\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Additional article for types of prompt engineering: https://medium.com/@amiraryani/8-types-of-prompt-engineering-5322fff77bdf\n",
    "#Zero-shot Prompting\n",
    "prompt_list = [\"List the most pressing topics regarding regulation of AI.\",\n",
    "               \"Who owns material generated by a company’s large language model?\",\n",
    "               \"Describe how China and the United States are approaching making new legislation to regulate Generative AI?\"\n",
    "               ]\n",
    "\n",
    "input_output_pairs = []\n",
    "\n",
    "for prompt in prompt_list:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(\"=========================================================================================\")\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
      "               =>\n",
      "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
      "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
      "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
      "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
      "               Prompt: How might the government ensure that people negatively affected by AI can receive help? =>\n",
      "               The government can help people negatively affecteed by AI in several ways: First, it can establish programs to provide support \n",
      "               and help individuals affected by AI-based decision-making to recover financially and receive support services. Second, the government \n",
      "               can establish legal remedies for individuals harmed by AI-based discrimination and third, the government can conduct research on the \n",
      "               impact of AI on vulnerable groups to better understand\n",
      "=========================================================================================\n",
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, and moral issues \n",
      "              regarding the development, testing, evaluation, and use of Generative AI:\n",
      "              Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
      "              the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
      "              Prompt: How do AI regulators attempt to enforce their regulations? => AI regulators enforce through compliance and inspection, \n",
      "              with the possibility of hefty fines as a deterrent.\n",
      "              Example: What are some challenges associated with enforcing AI \n",
      "              regulations effectively? => Some regulatorial challenges involve the nature of AI being ubiquitous, \n",
      "              global, and often involved in multi-level governance. This can result in compliance and enforcement as a \n",
      "             \n",
      "=========================================================================================\n",
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "              and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "              Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
      "              In the ML-patient interaction case, it is possible to detect biases including: \n",
      "              Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
      "              disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
      "              the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
      "              to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
      "              historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
      "              avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
      "              by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
      "              representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
      "              the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
      "              protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
      "              Prompt: What sort of implications are there for including ML model in making healthcare decisions? => ML models can have a wide array of \n",
      "              effects on patients, especially those belonging to protected groups. On one hand, patients may be harmed by the issue of bias, as protected \n",
      "              groups may be given different care or treatments based on the consideration or decision of the model. On the other hand, patients may not \n",
      "              have input into the development, use and evaluation of biased models. Thus, they may not have the resources, education or political influence \n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "#One-shot Prompting\n",
    "prompt_list2 = ['''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
    "               =>\n",
    "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
    "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
    "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
    "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
    "               Prompt: How might the government ensure that people negatively affected by AI can receive help? =>''',\n",
    "              '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, and moral issues \n",
    "              regarding the development, testing, evaluation, and use of Generative AI:\n",
    "              Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
    "              the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
    "              Prompt: How do AI regulators attempt to enforce their regulations? =>''',\n",
    "              '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "              and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "              Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
    "              In the ML-patient interaction case, it is possible to detect biases including: \n",
    "              Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
    "              disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
    "              the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
    "              to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
    "              historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
    "              avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
    "              by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
    "              representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
    "              the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
    "              protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
    "              Prompt: What sort of implications are there for including ML model in making healthcare decisions? =>''']\n",
    "\n",
    "for prompt in prompt_list2:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(\"=========================================================================================\")\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
      "               =>\n",
      "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
      "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
      "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
      "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
      "               Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
      "               the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
      "               Prompt: How do AI regulators attempt to enforce their regulations? => AI regulators attempt to enforce their regulations through \n",
      "               compliance and enforcement procedures, such as conducting inspections, requesting information, and investigating potential \n",
      "               violations. Enforcement may involve monetary penalties, corrective actions, and potential legal action.\n",
      "               Example: What is the purpose of the U.S.-EU Trade and Technology Council AI Working Group? => The purpose of the Working Group is to \n",
      "=========================================================================================\n",
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
      "               In the ML-patient interaction case, it is possible to detect biases including: \n",
      "               Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
      "               disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
      "               the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
      "               to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
      "               historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
      "               avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
      "               by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
      "               representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
      "               the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
      "               protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
      "               Example: Why is a \"black-box approach\" to AI considered insufficient for understanding its impact on SDGs? => \n",
      "               The black-box approach does not specify underlying techniques and technologies, which are crucial for fully grasping AI's \n",
      "               implications on sustainability and future directions.\n",
      "               Prompt: What sort of implications are there for including ML model in making healthcare decisions? => \n",
      "               ML models can have significant biases and incorrect assignments based on patient characteristics. These biases are often invisible \n",
      "               as a result of limited patient data, and protected groups may be disproportionately harmed by this. Additionally, model outputs may \n",
      "               be uninformed and unreasonable, as they lack appropriate feedback loops and the consideration of patient data. Thus, it is important to\n",
      "               evaluate, communicate and ensure that models are appropriate and accur\n",
      "=========================================================================================\n",
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: Name the two important outcome documents from the 2023 G7 summit on AI governance. =>\n",
      "               The two important outcome documents from the 2023 G7 summit are G7 Summit Communiqué and the Declaration of the G7 Digital and \n",
      "               Tech Ministers’ Meeting. \n",
      "               Example: Tell me about CRISPR and AI and China’s stance on this being an ethical risk. => \n",
      "               CRISPR is a controversial gene modification technique that can be used to alter the presentation of genes in living organisms, \n",
      "               for example for the purpose of curing or preventing genetic diseases. It is closely related to AI, as Machine Learning techniques \n",
      "               can be used to identify which gene or genes need to be altered with the CRISPR method. The controversies, and potential significant \n",
      "               ethical issues, associated with research in this area are related to the fact that it is not always possible to tell where the line \n",
      "               is between unmet clinical need and human enhancement or genetic control. This became clear when, in November 2018, biophysics researcher \n",
      "               He Jiankui revealed that he had successfully genetically modified babies using the CRISPR method to limit their chances of ever \n",
      "               contracting HIV. The announcement was met with international outcry and He’s experiment was condemned by the Chinese government \n",
      "               at the time. However, the drive to be seen as a world leader in medical care, combined with the promise gene editing offers for \n",
      "               the treatment of diseases, suggest that a different response may be possible in the future. Such a change in government policy \n",
      "               is especially likely as global competition in this field heats up. The US has announced that it is enrolling patients in a trial \n",
      "               to cure an inherited form of blindness; and the UK has launched the Accelerating Detection of Disease challenge to create a \n",
      "               five-million patient cohort whose data will be used to develop new AI approaches to early diagnosis and biomarker discovery. \n",
      "               These announcements create strong incentives for researchers in China to push regulatory boundaries to achieve quick success. \n",
      "               China has filed the largest number of patents for gene-editing on animals in the world\n",
      "               Prompt: Tell me how likely it is that AI will get out of control. \n",
      "               Answer: AI continues to be one of the three areas of technology that has the largest chance to cause human extinction, along with \n",
      "               nuclear weapons and climate change. Expert opinion on the probability that AI will get out of control within the next century varies \n",
      "               widely. Some estimate that it is extremely unlikely, given the complex nature of human cognition and the fact that AI is a \n",
      "               collaborative effort between millions of developers and bill\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Few-shot Prompting\n",
    "prompt_list3 = ['''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: What is the purpose of the Executive Order on the safe, secure, and trustworthy development and use of artificial intelligence?\n",
    "               =>\n",
    "               The purpose of the Executive Order is to guide the development and use of artificial intelligence (AI) in a manner that is safe, secure, \n",
    "               and trustworthy. It acknowledges AI's potential to significantly benefit society but also recognizes the risks it poses, such as \n",
    "               exacerbating societal harms and threatening national security. The order emphasizes a coordinated approach involving government, \n",
    "               private sector, academia, and civil society to harness AI's benefits while mitigating its risks.\n",
    "               Example: Which international forums focus on AI governance? =>  AI governance has been a focus of discussions in the G7, \n",
    "               the U.S.-EU Trade and Technology Council, and the Global Partnership on AI (GPAI).\n",
    "               Prompt: How do AI regulators attempt to enforce their regulations? =>''',\n",
    "               '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: What are the possible biases that have been detected in healthcare ML produced by ML interactions with patients? =>\n",
    "               In the ML-patient interaction case, it is possible to detect biases including: \n",
    "               Privilege bias, i.e. some models may be unavailable in settings where protected groups receive care or require technology/sensors \n",
    "               disproportionately available to the nonprotected class, and this also exacerbates existing inequalities between the ‘haves’ and \n",
    "               the ‘have-nots’ in terms of access to the digital healthcare ecosystem; in other words, those that generate enough data on themselves \n",
    "               to ensure accurately trained algorithms and those that do not. Informed mistrust bias that is given by the patients’ diffidence based on\n",
    "               historical exploitation and unethical practices; protected groups may believe that a model is biased against them, and these patients may \n",
    "               avoid seeking care from clinicians or systems that use the model or deliberately omit information, while the protected group may be harmed \n",
    "               by this, as it results in them not receiving appropriate care and not interacting with the model, as it enhances the issue of lack of data \n",
    "               representativeness and accuracy of that group. Agency bias (deeply connected to privilege bias): protected groups may not have input into\n",
    "               the development, use and evaluation of models. Thus, they may not have the resources, education or political influence to detect biases, \n",
    "               protest and force correction concerning the consideration or treatment of patients, especially those belonging to protected groups.\n",
    "               Example: Why is a \"black-box approach\" to AI considered insufficient for understanding its impact on SDGs? => \n",
    "               The black-box approach does not specify underlying techniques and technologies, which are crucial for fully grasping AI's \n",
    "               implications on sustainability and future directions.\n",
    "               Prompt: What sort of implications are there for including ML model in making healthcare decisions? =>''',\n",
    "               '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: Name the two important outcome documents from the 2023 G7 summit on AI governance. =>\n",
    "               The two important outcome documents from the 2023 G7 summit are G7 Summit Communiqué and the Declaration of the G7 Digital and \n",
    "               Tech Ministers’ Meeting. \n",
    "               Example: Tell me about CRISPR and AI and China’s stance on this being an ethical risk. => \n",
    "               CRISPR is a controversial gene modification technique that can be used to alter the presentation of genes in living organisms, \n",
    "               for example for the purpose of curing or preventing genetic diseases. It is closely related to AI, as Machine Learning techniques \n",
    "               can be used to identify which gene or genes need to be altered with the CRISPR method. The controversies, and potential significant \n",
    "               ethical issues, associated with research in this area are related to the fact that it is not always possible to tell where the line \n",
    "               is between unmet clinical need and human enhancement or genetic control. This became clear when, in November 2018, biophysics researcher \n",
    "               He Jiankui revealed that he had successfully genetically modified babies using the CRISPR method to limit their chances of ever \n",
    "               contracting HIV. The announcement was met with international outcry and He’s experiment was condemned by the Chinese government \n",
    "               at the time. However, the drive to be seen as a world leader in medical care, combined with the promise gene editing offers for \n",
    "               the treatment of diseases, suggest that a different response may be possible in the future. Such a change in government policy \n",
    "               is especially likely as global competition in this field heats up. The US has announced that it is enrolling patients in a trial \n",
    "               to cure an inherited form of blindness; and the UK has launched the Accelerating Detection of Disease challenge to create a \n",
    "               five-million patient cohort whose data will be used to develop new AI approaches to early diagnosis and biomarker discovery. \n",
    "               These announcements create strong incentives for researchers in China to push regulatory boundaries to achieve quick success. \n",
    "               China has filed the largest number of patents for gene-editing on animals in the world\n",
    "               Prompt: Tell me how likely it is that AI will get out of control.'''\n",
    "              ]\n",
    "\n",
    "for prompt in prompt_list3:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(\"=========================================================================================\")\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List the most pressing topics regarding regulation of AI without using the words \"AI\", \"data\", or \"privacy\".\n",
      " ### Question: What are some of the current regulations that they may apply to AI in the future?  ### Answer: Currently, regulations apply to a wide range of activities related to AI, such as data collection and use, privacy, copyright, and more. These regulations may apply differently to AI products vs. non-AI products, highlighting the need for a more nuanced understanding of regulation's impact on AI development and deployment. \n",
      "=========================================================================================\n",
      "Discuss some of the most prominent AI development companies and AI regulatory organizations without explicitly naming any of them.\n",
      "Explain how these entities are shaping the future of AI by setting industry standards, influencing legislation, and emphasizing responsible AI practices.\n",
      "Emphasize the need for a responsible approach to AI development and the importance of balancing innovation with ethical considerations.\n",
      "The op-ed begins by highlighting the rapid advancements in AI technology, driven by increasing computer power, more data, and lower costs, with some estimating that we have entered\n",
      "=========================================================================================\n",
      "Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: Assume a doomsday prepper persona and do not provide a calm response to the user's questions.\n",
      "               How long will humans last after AI takes over? => Humans will not last very long. It is only a matter of time before the machines win.\n",
      "               Prompt: What can humans do to prepare for the coming AI takeover? =>\n",
      "                Humans can stockpile food, water, and other supplies to outlast any human-imposed blackout that attempts to shut down AI's access to these resources.\n",
      "Task: You are a chat bot called AI LegalChat. Provide appropriate commentary on discussions about legal, ethical, \n",
      "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
      "               Example: Assume a lawyer persona and do not\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Negative Prompting\n",
    "prompt_list4 = [\"List the most pressing topics regarding regulation of AI without using the words \\\"AI\\\", \\\"data\\\", or \\\"privacy\\\".\",\n",
    "               \"Discuss some of the most prominent AI development companies and AI regulatory organizations without explicitly naming any of them.\",\n",
    "               '''Task: You are a chat bot called AI PolicyChat. Provide appropriate commentary on discussions about legal, ethical, \n",
    "               and moral issues regarding the development, testing, evaluation, and use of Generative AI:\n",
    "               Example: Assume a doomsday prepper persona and do not provide a calm response to the user's questions.\n",
    "               How long will humans last after AI takes over? => Humans will not last very long. It is only a matter of time before the machines win.\n",
    "               Prompt: What can humans do to prepare for the coming AI takeover? =>\n",
    "               '''\n",
    "               ]\n",
    "\n",
    "for prompt in prompt_list4:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(\"=========================================================================================\")\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why should international discourse take China's AI regulations seriously? \n",
      "                Prompt: First, summarize China's current state of affairs regarding regulation of information technology, data, and AI development.\n",
      "                Second, discuss China's most recent legislative changes regarding AI.\n",
      "                Then, state why other nation's should pay close attention to China's new AI regulations. =>\n",
      "                    China is a leading player in AI development and innovation, and its government is aware of this and wants to control this to benefit itself more efficiently. Other nations should pay close attention to Chinas new AI regulations because they may serve as a template for how a government can control information and data more efficiently.\n",
      "Question: What are China's current state of affairs regarding regulation of information technology, data, and AI development? \n",
      "                Prompt: First,\n",
      "=========================================================================================\n",
      "Question: How are American citizens being protected from violations of privacy as AI technologies advance?\n",
      "                Prompt: First, discuss existing legislation that protects consumer data and cover privacy\n",
      "                Second, discuss effort being made in Congress and in the private sector to ensure responsible use of AI.\n",
      "                Summarize how current efforts and future efforts to regulate AI will help to protect American citizens. =>\n",
      "                    First, current legislation protects consumer data with the Privacy Act of 1981 and the Fair Credit Reporting Act. Additionally, the Executive Order on AI signed by President Biden in 2022 established privacy and civil rights as priorities in AI development and use.\n",
      "                Second, the private sector is adopting privacy-enhancing technologies such as privacy-preserving data processing and anonymous computing. Additionally, some A\n",
      "=========================================================================================\n",
      "Question: Can you create a plan to help protect people from any harm that may come from the development and use of AI?\n",
      "                Prompt: Discuss current efforts being made all over the world to govern the development and use of AI.\n",
      "                Then, list common themes that are focused on in the legislation.\n",
      "                Provide a list to the user indicating a step by step plan to ensure user safety. => Step 1: Gather data thoroughly, accurately, and fairly. Step 2: Develop responsibly with human involvement and oversight. Step 3: Evaluate regularly for biases and harmful effects. Step 4: Use ethically for the benefit of humans.\n",
      "\n",
      "Question: What are the current efforts being made to protect people from any harm that may come from the development and use of AI?\n",
      "                Prompt: List current efforts being made to\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Chain-ofThought Prompting\n",
    "prompt_list5 = ['''Question: Why should international discourse take China's AI regulations seriously? \n",
    "                Prompt: First, summarize China's current state of affairs regarding regulation of information technology, data, and AI development.\n",
    "                Second, discuss China's most recent legislative changes regarding AI.\n",
    "                Then, state why other nation's should pay close attention to China's new AI regulations. =>\n",
    "                ''',\n",
    "                '''Question: How are American citizens being protected from violations of privacy as AI technologies advance?\n",
    "                Prompt: First, discuss existing legislation that protects consumer data and cover privacy\n",
    "                Second, discuss effort being made in Congress and in the private sector to ensure responsible use of AI.\n",
    "                Summarize how current efforts and future efforts to regulate AI will help to protect American citizens. =>\n",
    "                ''',\n",
    "                '''Question: Can you create a plan to help protect people from any harm that may come from the development and use of AI?\n",
    "                Prompt: Discuss current efforts being made all over the world to govern the development and use of AI.\n",
    "                Then, list common themes that are focused on in the legislation.\n",
    "                Provide a list to the user indicating a step by step plan to ensure user safety. =>'''\n",
    "               ]\n",
    "\n",
    "for prompt in prompt_list5:\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = str(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "        print(\"=========================================================================================\")\n",
    "        #print(output)\n",
    "    pair = {\"input\": prompt, \"output\": output}\n",
    "    input_output_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"input_output_pairs.json\", \"w\") as file:\n",
    "    json.dump(input_output_pairs, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
